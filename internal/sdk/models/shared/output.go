// Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT.

package shared

import (
	"encoding/json"
	"errors"
	"fmt"
	"github.com/speakeasy/terraform-provider-criblio/internal/sdk/internal/utils"
)

type TypeDynatraceOtlp string

const (
	TypeDynatraceOtlpDynatraceOtlp TypeDynatraceOtlp = "dynatrace_otlp"
)

func (e TypeDynatraceOtlp) ToPointer() *TypeDynatraceOtlp {
	return &e
}
func (e *TypeDynatraceOtlp) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "dynatrace_otlp":
		*e = TypeDynatraceOtlp(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeDynatraceOtlp: %v", v)
	}
}

// ProtocolHTTP - Select a transport option for Dynatrace
type ProtocolHTTP string

const (
	ProtocolHTTPHTTP ProtocolHTTP = "http"
)

func (e ProtocolHTTP) ToPointer() *ProtocolHTTP {
	return &e
}
func (e *ProtocolHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "http":
		*e = ProtocolHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ProtocolHTTP: %v", v)
	}
}

// OTLPVersionHTTP - The version of OTLP Protobuf definitions to use when structuring data to send
type OTLPVersionHTTP string

const (
	OTLPVersionHTTPOneDot3Dot1 OTLPVersionHTTP = "1.3.1"
)

func (e OTLPVersionHTTP) ToPointer() *OTLPVersionHTTP {
	return &e
}
func (e *OTLPVersionHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "1.3.1":
		*e = OTLPVersionHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OTLPVersionHTTP: %v", v)
	}
}

// OutputCompressCompressionHTTP - Type of compression to apply to messages sent to the OpenTelemetry endpoint
type OutputCompressCompressionHTTP string

const (
	OutputCompressCompressionHTTPNone    OutputCompressCompressionHTTP = "none"
	OutputCompressCompressionHTTPDeflate OutputCompressCompressionHTTP = "deflate"
	OutputCompressCompressionHTTPGzip    OutputCompressCompressionHTTP = "gzip"
)

func (e OutputCompressCompressionHTTP) ToPointer() *OutputCompressCompressionHTTP {
	return &e
}
func (e *OutputCompressCompressionHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "deflate":
		fallthrough
	case "gzip":
		*e = OutputCompressCompressionHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputCompressCompressionHTTP: %v", v)
	}
}

// HTTPCompressCompressionHTTP - Type of compression to apply to messages sent to the OpenTelemetry endpoint
type HTTPCompressCompressionHTTP string

const (
	HTTPCompressCompressionHTTPNone HTTPCompressCompressionHTTP = "none"
	HTTPCompressCompressionHTTPGzip HTTPCompressCompressionHTTP = "gzip"
)

func (e HTTPCompressCompressionHTTP) ToPointer() *HTTPCompressCompressionHTTP {
	return &e
}
func (e *HTTPCompressCompressionHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = HTTPCompressCompressionHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for HTTPCompressCompressionHTTP: %v", v)
	}
}

type OutputMetadatumHTTP struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (o OutputMetadatumHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputMetadatumHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *OutputMetadatumHTTP) GetKey() *string {
	if o == nil {
		return nil
	}
	return o.Key
}

func (o *OutputMetadatumHTTP) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeHTTP - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeHTTP string

const (
	FailedRequestLoggingModeHTTPPayload           FailedRequestLoggingModeHTTP = "payload"
	FailedRequestLoggingModeHTTPPayloadAndHeaders FailedRequestLoggingModeHTTP = "payloadAndHeaders"
	FailedRequestLoggingModeHTTPNone              FailedRequestLoggingModeHTTP = "none"
)

func (e FailedRequestLoggingModeHTTP) ToPointer() *FailedRequestLoggingModeHTTP {
	return &e
}
func (e *FailedRequestLoggingModeHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeHTTP: %v", v)
	}
}

// EndpointType - Select the type of Dynatrace endpoint configured
type EndpointType string

const (
	EndpointTypeSaas EndpointType = "saas"
	EndpointTypeAg   EndpointType = "ag"
)

func (e EndpointType) ToPointer() *EndpointType {
	return &e
}
func (e *EndpointType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "saas":
		fallthrough
	case "ag":
		*e = EndpointType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for EndpointType: %v", v)
	}
}

// BackpressureBehaviorHTTP - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorHTTP string

const (
	BackpressureBehaviorHTTPBlock BackpressureBehaviorHTTP = "block"
	BackpressureBehaviorHTTPDrop  BackpressureBehaviorHTTP = "drop"
	BackpressureBehaviorHTTPQueue BackpressureBehaviorHTTP = "queue"
)

func (e BackpressureBehaviorHTTP) ToPointer() *BackpressureBehaviorHTTP {
	return &e
}
func (e *BackpressureBehaviorHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorHTTP: %v", v)
	}
}

type ExtraHTTPHeaderHTTP struct {
	// Field name
	Name *string `json:"name,omitempty"`
	// Field value
	Value string `json:"value"`
}

func (o *ExtraHTTPHeaderHTTP) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderHTTP) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type ResponseRetrySettingHTTP struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingHTTP) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingHTTP) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingHTTP) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingHTTP) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsHTTP struct {
	// Enable to retry on request timeout
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsHTTP) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsHTTP) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsHTTP) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsHTTP) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// PqCompressCompressionHTTP - Codec to use to compress the persisted data.
type PqCompressCompressionHTTP string

const (
	PqCompressCompressionHTTPNone PqCompressCompressionHTTP = "none"
	PqCompressCompressionHTTPGzip PqCompressCompressionHTTP = "gzip"
)

func (e PqCompressCompressionHTTP) ToPointer() *PqCompressCompressionHTTP {
	return &e
}
func (e *PqCompressCompressionHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = PqCompressCompressionHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PqCompressCompressionHTTP: %v", v)
	}
}

// QueueFullBehaviorHTTP - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorHTTP string

const (
	QueueFullBehaviorHTTPBlock QueueFullBehaviorHTTP = "block"
	QueueFullBehaviorHTTPDrop  QueueFullBehaviorHTTP = "drop"
)

func (e QueueFullBehaviorHTTP) ToPointer() *QueueFullBehaviorHTTP {
	return &e
}
func (e *QueueFullBehaviorHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorHTTP: %v", v)
	}
}

// OutputModeHTTP - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type OutputModeHTTP string

const (
	OutputModeHTTPError        OutputModeHTTP = "error"
	OutputModeHTTPBackpressure OutputModeHTTP = "backpressure"
	OutputModeHTTPAlways       OutputModeHTTP = "always"
)

func (e OutputModeHTTP) ToPointer() *OutputModeHTTP {
	return &e
}
func (e *OutputModeHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = OutputModeHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputModeHTTP: %v", v)
	}
}

type PqControlsHTTP struct {
}

type OutputDynatraceOtlp struct {
	// Unique ID for this output
	ID   *string            `json:"id,omitempty"`
	Type *TypeDynatraceOtlp `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Select a transport option for Dynatrace
	Protocol *ProtocolHTTP `default:"http" json:"protocol"`
	// The endpoint where Dynatrace events will be sent. Enter any valid URL or an IP address (IPv4 or IPv6; enclose IPv6 addresses in square brackets)
	Endpoint *string `default:"https://{your-environment-id}.live.dynatrace.com/api/v2/otlp" json:"endpoint"`
	// The version of OTLP Protobuf definitions to use when structuring data to send
	OtlpVersion *OTLPVersionHTTP `default:"1.3.1" json:"otlpVersion"`
	// Type of compression to apply to messages sent to the OpenTelemetry endpoint
	Compress *OutputCompressCompressionHTTP `default:"gzip" json:"compress"`
	// Type of compression to apply to messages sent to the OpenTelemetry endpoint
	HTTPCompress *HTTPCompressCompressionHTTP `default:"gzip" json:"httpCompress"`
	// If you want to send traces to the default `{endpoint}/v1/traces` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPTracesEndpointOverride *string `json:"httpTracesEndpointOverride,omitempty"`
	// If you want to send metrics to the default `{endpoint}/v1/metrics` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPMetricsEndpointOverride *string `json:"httpMetricsEndpointOverride,omitempty"`
	// If you want to send logs to the default `{endpoint}/v1/logs` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPLogsEndpointOverride *string `json:"httpLogsEndpointOverride,omitempty"`
	// List of key-value pairs to send with each gRPC request. Value supports JavaScript expressions that are evaluated just once, when the destination gets started. To pass credentials as metadata, use 'C.Secret'.
	Metadata []OutputMetadatumHTTP `json:"metadata,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size (in KB) of the request body. The maximum payload size is 4 MB. If this limit is exceeded, the entire OTLP message is dropped
	MaxPayloadSizeKB *float64 `default:"2048" json:"maxPayloadSizeKB"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max body size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeHTTP `default:"none" json:"failedRequestLoggingMode"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// How often the sender should ping the peer to keep the connection open
	KeepAliveTime *float64 `default:"30" json:"keepAliveTime"`
	// Disable to close the connection immediately after sending the outgoing request
	KeepAlive *bool `default:"true" json:"keepAlive"`
	// Select the type of Dynatrace endpoint configured
	EndpointType *EndpointType `default:"saas" json:"endpointType"`
	// Select or create a stored text secret
	TokenSecret   string  `json:"tokenSecret"`
	AuthTokenName *string `default:"Authorization" json:"authTokenName"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorHTTP `default:"block" json:"onBackpressure"`
	Description    *string                   `json:"description,omitempty"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Defaults to Yes. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Enables round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Headers to add to all events.
	ExtraHTTPHeaders []ExtraHTTPHeaderHTTP `json:"extraHttpHeaders,omitempty"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable).
	ResponseRetrySettings []ResponseRetrySettingHTTP `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsHTTP  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *PqCompressCompressionHTTP `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorHTTP `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *OutputModeHTTP `default:"error" json:"pqMode"`
	PqControls *PqControlsHTTP `json:"pqControls,omitempty"`
	Status     *TFStatus       `json:"status,omitempty"`
}

func (o OutputDynatraceOtlp) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputDynatraceOtlp) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputDynatraceOtlp) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputDynatraceOtlp) GetType() *TypeDynatraceOtlp {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputDynatraceOtlp) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDynatraceOtlp) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDynatraceOtlp) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDynatraceOtlp) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputDynatraceOtlp) GetProtocol() *ProtocolHTTP {
	if o == nil {
		return nil
	}
	return o.Protocol
}

func (o *OutputDynatraceOtlp) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputDynatraceOtlp) GetOtlpVersion() *OTLPVersionHTTP {
	if o == nil {
		return nil
	}
	return o.OtlpVersion
}

func (o *OutputDynatraceOtlp) GetCompress() *OutputCompressCompressionHTTP {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputDynatraceOtlp) GetHTTPCompress() *HTTPCompressCompressionHTTP {
	if o == nil {
		return nil
	}
	return o.HTTPCompress
}

func (o *OutputDynatraceOtlp) GetHTTPTracesEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPTracesEndpointOverride
}

func (o *OutputDynatraceOtlp) GetHTTPMetricsEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPMetricsEndpointOverride
}

func (o *OutputDynatraceOtlp) GetHTTPLogsEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPLogsEndpointOverride
}

func (o *OutputDynatraceOtlp) GetMetadata() []OutputMetadatumHTTP {
	if o == nil {
		return nil
	}
	return o.Metadata
}

func (o *OutputDynatraceOtlp) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputDynatraceOtlp) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputDynatraceOtlp) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputDynatraceOtlp) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputDynatraceOtlp) GetFailedRequestLoggingMode() *FailedRequestLoggingModeHTTP {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputDynatraceOtlp) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputDynatraceOtlp) GetKeepAliveTime() *float64 {
	if o == nil {
		return nil
	}
	return o.KeepAliveTime
}

func (o *OutputDynatraceOtlp) GetKeepAlive() *bool {
	if o == nil {
		return nil
	}
	return o.KeepAlive
}

func (o *OutputDynatraceOtlp) GetEndpointType() *EndpointType {
	if o == nil {
		return nil
	}
	return o.EndpointType
}

func (o *OutputDynatraceOtlp) GetTokenSecret() string {
	if o == nil {
		return ""
	}
	return o.TokenSecret
}

func (o *OutputDynatraceOtlp) GetAuthTokenName() *string {
	if o == nil {
		return nil
	}
	return o.AuthTokenName
}

func (o *OutputDynatraceOtlp) GetOnBackpressure() *BackpressureBehaviorHTTP {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputDynatraceOtlp) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputDynatraceOtlp) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputDynatraceOtlp) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputDynatraceOtlp) GetExtraHTTPHeaders() []ExtraHTTPHeaderHTTP {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputDynatraceOtlp) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputDynatraceOtlp) GetResponseRetrySettings() []ResponseRetrySettingHTTP {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputDynatraceOtlp) GetTimeoutRetrySettings() *TimeoutRetrySettingsHTTP {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputDynatraceOtlp) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputDynatraceOtlp) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputDynatraceOtlp) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputDynatraceOtlp) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputDynatraceOtlp) GetPqCompress() *PqCompressCompressionHTTP {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputDynatraceOtlp) GetPqOnBackpressure() *QueueFullBehaviorHTTP {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputDynatraceOtlp) GetPqMode() *OutputModeHTTP {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputDynatraceOtlp) GetPqControls() *PqControlsHTTP {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputDynatraceOtlp) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeDynatraceHTTP string

const (
	TypeDynatraceHTTPDynatraceHTTP TypeDynatraceHTTP = "dynatrace_http"
)

func (e TypeDynatraceHTTP) ToPointer() *TypeDynatraceHTTP {
	return &e
}
func (e *TypeDynatraceHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "dynatrace_http":
		*e = TypeDynatraceHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeDynatraceHTTP: %v", v)
	}
}

// MethodDynatraceHTTP - The method to use when sending events. Defaults to POST.
type MethodDynatraceHTTP string

const (
	MethodDynatraceHTTPPost  MethodDynatraceHTTP = "POST"
	MethodDynatraceHTTPPut   MethodDynatraceHTTP = "PUT"
	MethodDynatraceHTTPPatch MethodDynatraceHTTP = "PATCH"
)

func (e MethodDynatraceHTTP) ToPointer() *MethodDynatraceHTTP {
	return &e
}
func (e *MethodDynatraceHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "POST":
		fallthrough
	case "PUT":
		fallthrough
	case "PATCH":
		*e = MethodDynatraceHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for MethodDynatraceHTTP: %v", v)
	}
}

type ExtraHTTPHeaderDynatraceHTTP struct {
	// Field name
	Name *string `json:"name,omitempty"`
	// Field value
	Value string `json:"value"`
}

func (o *ExtraHTTPHeaderDynatraceHTTP) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderDynatraceHTTP) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeDynatraceHTTP - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeDynatraceHTTP string

const (
	FailedRequestLoggingModeDynatraceHTTPPayload           FailedRequestLoggingModeDynatraceHTTP = "payload"
	FailedRequestLoggingModeDynatraceHTTPPayloadAndHeaders FailedRequestLoggingModeDynatraceHTTP = "payloadAndHeaders"
	FailedRequestLoggingModeDynatraceHTTPNone              FailedRequestLoggingModeDynatraceHTTP = "none"
)

func (e FailedRequestLoggingModeDynatraceHTTP) ToPointer() *FailedRequestLoggingModeDynatraceHTTP {
	return &e
}
func (e *FailedRequestLoggingModeDynatraceHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeDynatraceHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeDynatraceHTTP: %v", v)
	}
}

type ResponseRetrySettingDynatraceHTTP struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingDynatraceHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingDynatraceHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingDynatraceHTTP) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingDynatraceHTTP) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingDynatraceHTTP) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingDynatraceHTTP) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsDynatraceHTTP struct {
	// Enable to retry on request timeout
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsDynatraceHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsDynatraceHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsDynatraceHTTP) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsDynatraceHTTP) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsDynatraceHTTP) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsDynatraceHTTP) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorDynatraceHTTP - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorDynatraceHTTP string

const (
	BackpressureBehaviorDynatraceHTTPBlock BackpressureBehaviorDynatraceHTTP = "block"
	BackpressureBehaviorDynatraceHTTPDrop  BackpressureBehaviorDynatraceHTTP = "drop"
	BackpressureBehaviorDynatraceHTTPQueue BackpressureBehaviorDynatraceHTTP = "queue"
)

func (e BackpressureBehaviorDynatraceHTTP) ToPointer() *BackpressureBehaviorDynatraceHTTP {
	return &e
}
func (e *BackpressureBehaviorDynatraceHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorDynatraceHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorDynatraceHTTP: %v", v)
	}
}

type AuthenticationTypeDynatraceHTTP string

const (
	AuthenticationTypeDynatraceHTTPToken      AuthenticationTypeDynatraceHTTP = "token"
	AuthenticationTypeDynatraceHTTPTextSecret AuthenticationTypeDynatraceHTTP = "textSecret"
)

func (e AuthenticationTypeDynatraceHTTP) ToPointer() *AuthenticationTypeDynatraceHTTP {
	return &e
}
func (e *AuthenticationTypeDynatraceHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "token":
		fallthrough
	case "textSecret":
		*e = AuthenticationTypeDynatraceHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationTypeDynatraceHTTP: %v", v)
	}
}

// FormatDynatraceHTTP - How to format events before sending. Defaults to JSON. Plaintext is not currently supported.
type FormatDynatraceHTTP string

const (
	FormatDynatraceHTTPJSONArray FormatDynatraceHTTP = "json_array"
	FormatDynatraceHTTPPlaintext FormatDynatraceHTTP = "plaintext"
)

func (e FormatDynatraceHTTP) ToPointer() *FormatDynatraceHTTP {
	return &e
}
func (e *FormatDynatraceHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "json_array":
		fallthrough
	case "plaintext":
		*e = FormatDynatraceHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FormatDynatraceHTTP: %v", v)
	}
}

type Endpoint string

const (
	EndpointCloud      Endpoint = "cloud"
	EndpointActiveGate Endpoint = "activeGate"
	EndpointManual     Endpoint = "manual"
)

func (e Endpoint) ToPointer() *Endpoint {
	return &e
}
func (e *Endpoint) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "cloud":
		fallthrough
	case "activeGate":
		fallthrough
	case "manual":
		*e = Endpoint(v)
		return nil
	default:
		return fmt.Errorf("invalid value for Endpoint: %v", v)
	}
}

type TelemetryType string

const (
	TelemetryTypeLogs    TelemetryType = "logs"
	TelemetryTypeMetrics TelemetryType = "metrics"
)

func (e TelemetryType) ToPointer() *TelemetryType {
	return &e
}
func (e *TelemetryType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "logs":
		fallthrough
	case "metrics":
		*e = TelemetryType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TelemetryType: %v", v)
	}
}

// CompressionDynatraceHTTP - Codec to use to compress the persisted data.
type CompressionDynatraceHTTP string

const (
	CompressionDynatraceHTTPNone CompressionDynatraceHTTP = "none"
	CompressionDynatraceHTTPGzip CompressionDynatraceHTTP = "gzip"
)

func (e CompressionDynatraceHTTP) ToPointer() *CompressionDynatraceHTTP {
	return &e
}
func (e *CompressionDynatraceHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionDynatraceHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionDynatraceHTTP: %v", v)
	}
}

// QueueFullBehaviorDynatraceHTTP - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorDynatraceHTTP string

const (
	QueueFullBehaviorDynatraceHTTPBlock QueueFullBehaviorDynatraceHTTP = "block"
	QueueFullBehaviorDynatraceHTTPDrop  QueueFullBehaviorDynatraceHTTP = "drop"
)

func (e QueueFullBehaviorDynatraceHTTP) ToPointer() *QueueFullBehaviorDynatraceHTTP {
	return &e
}
func (e *QueueFullBehaviorDynatraceHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorDynatraceHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorDynatraceHTTP: %v", v)
	}
}

// ModeDynatraceHTTP - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type ModeDynatraceHTTP string

const (
	ModeDynatraceHTTPError        ModeDynatraceHTTP = "error"
	ModeDynatraceHTTPBackpressure ModeDynatraceHTTP = "backpressure"
	ModeDynatraceHTTPAlways       ModeDynatraceHTTP = "always"
)

func (e ModeDynatraceHTTP) ToPointer() *ModeDynatraceHTTP {
	return &e
}
func (e *ModeDynatraceHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeDynatraceHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeDynatraceHTTP: %v", v)
	}
}

type PqControlsDynatraceHTTP struct {
}

type OutputDynatraceHTTP struct {
	// Unique ID for this output
	ID   *string            `json:"id,omitempty"`
	Type *TypeDynatraceHTTP `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The method to use when sending events. Defaults to POST.
	Method *MethodDynatraceHTTP `default:"POST" json:"method"`
	// Disable to close the connection immediately after sending the outgoing request
	KeepAlive *bool `default:"true" json:"keepAlive"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Defaults to Yes. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max body size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events. You can also add headers dynamically on a per-event basis in the __headers field, as explained [here](https://docs.cribl.io/stream/destinations-webhook/#internal-fields).
	ExtraHTTPHeaders []ExtraHTTPHeaderDynatraceHTTP `json:"extraHttpHeaders,omitempty"`
	// Enables round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeDynatraceHTTP `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable).
	ResponseRetrySettings []ResponseRetrySettingDynatraceHTTP `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsDynatraceHTTP  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorDynatraceHTTP `default:"block" json:"onBackpressure"`
	AuthType       *AuthenticationTypeDynatraceHTTP   `default:"token" json:"authType"`
	// How to format events before sending. Defaults to JSON. Plaintext is not currently supported.
	Format        *FormatDynatraceHTTP `default:"json_array" json:"format"`
	Endpoint      *Endpoint            `default:"cloud" json:"endpoint"`
	TelemetryType *TelemetryType       `default:"logs" json:"telemetryType"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	Description        *string  `json:"description,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *CompressionDynatraceHTTP `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorDynatraceHTTP `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeDynatraceHTTP       `default:"error" json:"pqMode"`
	PqControls *PqControlsDynatraceHTTP `json:"pqControls,omitempty"`
	// Bearer token to include in the authorization header
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// ID of the environment to send to
	EnvironmentID *string `json:"environmentId,omitempty"`
	// ActiveGate domain with Log analytics collector module enabled. For example https://{activeGate-domain}:9999/e/{environment-id}/api/v2/logs/ingest.
	ActiveGateDomain *string `json:"activeGateDomain,omitempty"`
	// URL to send events to. Can be overwritten by an event's __url field.
	URL    *string   `json:"url,omitempty"`
	Status *TFStatus `json:"status,omitempty"`
}

func (o OutputDynatraceHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputDynatraceHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputDynatraceHTTP) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputDynatraceHTTP) GetType() *TypeDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputDynatraceHTTP) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDynatraceHTTP) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDynatraceHTTP) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDynatraceHTTP) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputDynatraceHTTP) GetMethod() *MethodDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.Method
}

func (o *OutputDynatraceHTTP) GetKeepAlive() *bool {
	if o == nil {
		return nil
	}
	return o.KeepAlive
}

func (o *OutputDynatraceHTTP) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputDynatraceHTTP) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputDynatraceHTTP) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputDynatraceHTTP) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputDynatraceHTTP) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputDynatraceHTTP) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputDynatraceHTTP) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputDynatraceHTTP) GetExtraHTTPHeaders() []ExtraHTTPHeaderDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputDynatraceHTTP) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputDynatraceHTTP) GetFailedRequestLoggingMode() *FailedRequestLoggingModeDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputDynatraceHTTP) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputDynatraceHTTP) GetResponseRetrySettings() []ResponseRetrySettingDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputDynatraceHTTP) GetTimeoutRetrySettings() *TimeoutRetrySettingsDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputDynatraceHTTP) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputDynatraceHTTP) GetOnBackpressure() *BackpressureBehaviorDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputDynatraceHTTP) GetAuthType() *AuthenticationTypeDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputDynatraceHTTP) GetFormat() *FormatDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputDynatraceHTTP) GetEndpoint() *Endpoint {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputDynatraceHTTP) GetTelemetryType() *TelemetryType {
	if o == nil {
		return nil
	}
	return o.TelemetryType
}

func (o *OutputDynatraceHTTP) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputDynatraceHTTP) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputDynatraceHTTP) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputDynatraceHTTP) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputDynatraceHTTP) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputDynatraceHTTP) GetPqCompress() *CompressionDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputDynatraceHTTP) GetPqOnBackpressure() *QueueFullBehaviorDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputDynatraceHTTP) GetPqMode() *ModeDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputDynatraceHTTP) GetPqControls() *PqControlsDynatraceHTTP {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputDynatraceHTTP) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputDynatraceHTTP) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputDynatraceHTTP) GetEnvironmentID() *string {
	if o == nil {
		return nil
	}
	return o.EnvironmentID
}

func (o *OutputDynatraceHTTP) GetActiveGateDomain() *string {
	if o == nil {
		return nil
	}
	return o.ActiveGateDomain
}

func (o *OutputDynatraceHTTP) GetURL() *string {
	if o == nil {
		return nil
	}
	return o.URL
}

func (o *OutputDynatraceHTTP) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type OutputTypeNetflow string

const (
	OutputTypeNetflowNetflow OutputTypeNetflow = "netflow"
)

func (e OutputTypeNetflow) ToPointer() *OutputTypeNetflow {
	return &e
}
func (e *OutputTypeNetflow) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "netflow":
		*e = OutputTypeNetflow(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeNetflow: %v", v)
	}
}

type HostNetflow struct {
	// Destination host
	Host string `json:"host"`
	// Destination port, default is 2055
	Port *float64 `default:"2055" json:"port"`
}

func (h HostNetflow) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(h, "", false)
}

func (h *HostNetflow) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &h, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *HostNetflow) GetHost() string {
	if o == nil {
		return ""
	}
	return o.Host
}

func (o *HostNetflow) GetPort() *float64 {
	if o == nil {
		return nil
	}
	return o.Port
}

type OutputNetflow struct {
	// Unique ID for this output
	ID   *string           `json:"id,omitempty"`
	Type OutputTypeNetflow `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// One or more NetFlow destinations to forward events to
	Hosts []HostNetflow `json:"hosts"`
	// How often to resolve the destination hostname to an IP address. Ignored if all destinations are IP addresses. A value of 0 means every datagram sent will incur a DNS lookup.
	DNSResolvePeriodSec *float64  `default:"0" json:"dnsResolvePeriodSec"`
	Description         *string   `json:"description,omitempty"`
	Status              *TFStatus `json:"status,omitempty"`
}

func (o OutputNetflow) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputNetflow) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputNetflow) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputNetflow) GetType() OutputTypeNetflow {
	if o == nil {
		return OutputTypeNetflow("")
	}
	return o.Type
}

func (o *OutputNetflow) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputNetflow) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputNetflow) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputNetflow) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputNetflow) GetHosts() []HostNetflow {
	if o == nil {
		return []HostNetflow{}
	}
	return o.Hosts
}

func (o *OutputNetflow) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputNetflow) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputNetflow) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeXsiam string

const (
	TypeXsiamXsiam TypeXsiam = "xsiam"
)

func (e TypeXsiam) ToPointer() *TypeXsiam {
	return &e
}
func (e *TypeXsiam) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "xsiam":
		*e = TypeXsiam(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeXsiam: %v", v)
	}
}

type ExtraHTTPHeaderXsiam struct {
	// Field name
	Name *string `json:"name,omitempty"`
	// Field value
	Value string `json:"value"`
}

func (o *ExtraHTTPHeaderXsiam) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderXsiam) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeXsiam - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeXsiam string

const (
	FailedRequestLoggingModeXsiamPayload           FailedRequestLoggingModeXsiam = "payload"
	FailedRequestLoggingModeXsiamPayloadAndHeaders FailedRequestLoggingModeXsiam = "payloadAndHeaders"
	FailedRequestLoggingModeXsiamNone              FailedRequestLoggingModeXsiam = "none"
)

func (e FailedRequestLoggingModeXsiam) ToPointer() *FailedRequestLoggingModeXsiam {
	return &e
}
func (e *FailedRequestLoggingModeXsiam) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeXsiam(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeXsiam: %v", v)
	}
}

// AuthenticationMethodXsiam - Enter a token directly, or provide a secret referencing a token
type AuthenticationMethodXsiam string

const (
	AuthenticationMethodXsiamToken  AuthenticationMethodXsiam = "token"
	AuthenticationMethodXsiamSecret AuthenticationMethodXsiam = "secret"
)

func (e AuthenticationMethodXsiam) ToPointer() *AuthenticationMethodXsiam {
	return &e
}
func (e *AuthenticationMethodXsiam) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "token":
		fallthrough
	case "secret":
		*e = AuthenticationMethodXsiam(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodXsiam: %v", v)
	}
}

type ResponseRetrySettingXsiam struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingXsiam) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingXsiam) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingXsiam) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingXsiam) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingXsiam) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingXsiam) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsXsiam struct {
	// Enable to retry on request timeout
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsXsiam) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsXsiam) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsXsiam) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsXsiam) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsXsiam) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsXsiam) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorXsiam - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorXsiam string

const (
	BackpressureBehaviorXsiamBlock BackpressureBehaviorXsiam = "block"
	BackpressureBehaviorXsiamDrop  BackpressureBehaviorXsiam = "drop"
	BackpressureBehaviorXsiamQueue BackpressureBehaviorXsiam = "queue"
)

func (e BackpressureBehaviorXsiam) ToPointer() *BackpressureBehaviorXsiam {
	return &e
}
func (e *BackpressureBehaviorXsiam) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorXsiam(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorXsiam: %v", v)
	}
}

type URLXsiam struct {
	URL any `json:"url"`
	// Assign a weight (>0) to each endpoint to indicate its traffic-handling capability
	Weight *float64 `default:"1" json:"weight"`
}

func (u URLXsiam) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(u, "", false)
}

func (u *URLXsiam) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &u, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *URLXsiam) GetURL() any {
	if o == nil {
		return nil
	}
	return o.URL
}

func (o *URLXsiam) GetWeight() *float64 {
	if o == nil {
		return nil
	}
	return o.Weight
}

// CompressionXsiam - Codec to use to compress the persisted data.
type CompressionXsiam string

const (
	CompressionXsiamNone CompressionXsiam = "none"
	CompressionXsiamGzip CompressionXsiam = "gzip"
)

func (e CompressionXsiam) ToPointer() *CompressionXsiam {
	return &e
}
func (e *CompressionXsiam) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionXsiam(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionXsiam: %v", v)
	}
}

// QueueFullBehaviorXsiam - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorXsiam string

const (
	QueueFullBehaviorXsiamBlock QueueFullBehaviorXsiam = "block"
	QueueFullBehaviorXsiamDrop  QueueFullBehaviorXsiam = "drop"
)

func (e QueueFullBehaviorXsiam) ToPointer() *QueueFullBehaviorXsiam {
	return &e
}
func (e *QueueFullBehaviorXsiam) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorXsiam(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorXsiam: %v", v)
	}
}

// ModeXsiam - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type ModeXsiam string

const (
	ModeXsiamError        ModeXsiam = "error"
	ModeXsiamBackpressure ModeXsiam = "backpressure"
	ModeXsiamAlways       ModeXsiam = "always"
)

func (e ModeXsiam) ToPointer() *ModeXsiam {
	return &e
}
func (e *ModeXsiam) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeXsiam(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeXsiam: %v", v)
	}
}

type PqControlsXsiam struct {
}

type OutputXsiam struct {
	// Unique ID for this output
	ID   string    `json:"id"`
	Type TypeXsiam `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.
	LoadBalanced *bool `default:"false" json:"loadBalanced"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Defaults to Yes. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max body size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events.
	ExtraHTTPHeaders []ExtraHTTPHeaderXsiam `json:"extraHttpHeaders,omitempty"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeXsiam `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Enter a token directly, or provide a secret referencing a token
	AuthType *AuthenticationMethodXsiam `default:"token" json:"authType"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable).
	ResponseRetrySettings []ResponseRetrySettingXsiam `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsXsiam  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// Maximum number of requests to limit to per second
	ThrottleRateReqPerSec *int64 `default:"400" json:"throttleRateReqPerSec"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorXsiam `default:"block" json:"onBackpressure"`
	Description    *string                    `json:"description,omitempty"`
	// XSIAM endpoint URL to send events to, such as https://api-{tenant external URL}/logs/v1/event
	URL *string `default:"http://localhost:8088/logs/v1/event" json:"url"`
	// Enables round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Exclude all IPs of the current host from the list of any resolved hostnames.
	ExcludeSelf *bool      `default:"false" json:"excludeSelf"`
	Urls        []URLXsiam `json:"urls,omitempty"`
	// Re-resolve any hostnames every this many seconds and pick up destinations from A records.
	DNSResolvePeriodSec *float64 `default:"600" json:"dnsResolvePeriodSec"`
	// How far back in time to keep traffic stats for load balancing purposes.
	LoadBalanceStatsPeriodSec *float64 `default:"300" json:"loadBalanceStatsPeriodSec"`
	// XSIAM authentication token
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *CompressionXsiam `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorXsiam `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeXsiam       `default:"error" json:"pqMode"`
	PqControls *PqControlsXsiam `json:"pqControls,omitempty"`
	Status     *TFStatus        `json:"status,omitempty"`
}

func (o OutputXsiam) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputXsiam) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputXsiam) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputXsiam) GetType() TypeXsiam {
	if o == nil {
		return TypeXsiam("")
	}
	return o.Type
}

func (o *OutputXsiam) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputXsiam) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputXsiam) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputXsiam) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputXsiam) GetLoadBalanced() *bool {
	if o == nil {
		return nil
	}
	return o.LoadBalanced
}

func (o *OutputXsiam) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputXsiam) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputXsiam) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputXsiam) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputXsiam) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputXsiam) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputXsiam) GetExtraHTTPHeaders() []ExtraHTTPHeaderXsiam {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputXsiam) GetFailedRequestLoggingMode() *FailedRequestLoggingModeXsiam {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputXsiam) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputXsiam) GetAuthType() *AuthenticationMethodXsiam {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputXsiam) GetResponseRetrySettings() []ResponseRetrySettingXsiam {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputXsiam) GetTimeoutRetrySettings() *TimeoutRetrySettingsXsiam {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputXsiam) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputXsiam) GetThrottleRateReqPerSec() *int64 {
	if o == nil {
		return nil
	}
	return o.ThrottleRateReqPerSec
}

func (o *OutputXsiam) GetOnBackpressure() *BackpressureBehaviorXsiam {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputXsiam) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputXsiam) GetURL() *string {
	if o == nil {
		return nil
	}
	return o.URL
}

func (o *OutputXsiam) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputXsiam) GetExcludeSelf() *bool {
	if o == nil {
		return nil
	}
	return o.ExcludeSelf
}

func (o *OutputXsiam) GetUrls() []URLXsiam {
	if o == nil {
		return nil
	}
	return o.Urls
}

func (o *OutputXsiam) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputXsiam) GetLoadBalanceStatsPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.LoadBalanceStatsPeriodSec
}

func (o *OutputXsiam) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputXsiam) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputXsiam) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputXsiam) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputXsiam) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputXsiam) GetPqCompress() *CompressionXsiam {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputXsiam) GetPqOnBackpressure() *QueueFullBehaviorXsiam {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputXsiam) GetPqMode() *ModeXsiam {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputXsiam) GetPqControls() *PqControlsXsiam {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputXsiam) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeClickHouse string

const (
	TypeClickHouseClickHouse TypeClickHouse = "click_house"
)

func (e TypeClickHouse) ToPointer() *TypeClickHouse {
	return &e
}
func (e *TypeClickHouse) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "click_house":
		*e = TypeClickHouse(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeClickHouse: %v", v)
	}
}

type AuthenticationTypeClickHouse string

const (
	AuthenticationTypeClickHouseNone               AuthenticationTypeClickHouse = "none"
	AuthenticationTypeClickHouseBasic              AuthenticationTypeClickHouse = "basic"
	AuthenticationTypeClickHouseCredentialsSecret  AuthenticationTypeClickHouse = "credentialsSecret"
	AuthenticationTypeClickHouseSslUserCertificate AuthenticationTypeClickHouse = "sslUserCertificate"
	AuthenticationTypeClickHouseToken              AuthenticationTypeClickHouse = "token"
	AuthenticationTypeClickHouseTextSecret         AuthenticationTypeClickHouse = "textSecret"
	AuthenticationTypeClickHouseOauth              AuthenticationTypeClickHouse = "oauth"
)

func (e AuthenticationTypeClickHouse) ToPointer() *AuthenticationTypeClickHouse {
	return &e
}
func (e *AuthenticationTypeClickHouse) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "basic":
		fallthrough
	case "credentialsSecret":
		fallthrough
	case "sslUserCertificate":
		fallthrough
	case "token":
		fallthrough
	case "textSecret":
		fallthrough
	case "oauth":
		*e = AuthenticationTypeClickHouse(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationTypeClickHouse: %v", v)
	}
}

// FormatClickHouse - Data format to use when sending data to ClickHouse. Defaults to JSON Compact.
type FormatClickHouse string

const (
	FormatClickHouseJSONCompactEachRowWithNames FormatClickHouse = "json-compact-each-row-with-names"
	FormatClickHouseJSONEachRow                 FormatClickHouse = "json-each-row"
)

func (e FormatClickHouse) ToPointer() *FormatClickHouse {
	return &e
}
func (e *FormatClickHouse) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "json-compact-each-row-with-names":
		fallthrough
	case "json-each-row":
		*e = FormatClickHouse(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FormatClickHouse: %v", v)
	}
}

// OutputMappingType - How event fields are mapped to ClickHouse columns.
type OutputMappingType string

const (
	OutputMappingTypeAutomatic OutputMappingType = "automatic"
	OutputMappingTypeCustom    OutputMappingType = "custom"
)

func (e OutputMappingType) ToPointer() *OutputMappingType {
	return &e
}
func (e *OutputMappingType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "automatic":
		fallthrough
	case "custom":
		*e = OutputMappingType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMappingType: %v", v)
	}
}

// MinimumTLSVersionClickHouse - Minimum TLS version to use when connecting
type MinimumTLSVersionClickHouse string

const (
	MinimumTLSVersionClickHouseTlSv1  MinimumTLSVersionClickHouse = "TLSv1"
	MinimumTLSVersionClickHouseTlSv11 MinimumTLSVersionClickHouse = "TLSv1.1"
	MinimumTLSVersionClickHouseTlSv12 MinimumTLSVersionClickHouse = "TLSv1.2"
	MinimumTLSVersionClickHouseTlSv13 MinimumTLSVersionClickHouse = "TLSv1.3"
)

func (e MinimumTLSVersionClickHouse) ToPointer() *MinimumTLSVersionClickHouse {
	return &e
}
func (e *MinimumTLSVersionClickHouse) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = MinimumTLSVersionClickHouse(v)
		return nil
	default:
		return fmt.Errorf("invalid value for MinimumTLSVersionClickHouse: %v", v)
	}
}

// MaximumTLSVersionClickHouse - Maximum TLS version to use when connecting
type MaximumTLSVersionClickHouse string

const (
	MaximumTLSVersionClickHouseTlSv1  MaximumTLSVersionClickHouse = "TLSv1"
	MaximumTLSVersionClickHouseTlSv11 MaximumTLSVersionClickHouse = "TLSv1.1"
	MaximumTLSVersionClickHouseTlSv12 MaximumTLSVersionClickHouse = "TLSv1.2"
	MaximumTLSVersionClickHouseTlSv13 MaximumTLSVersionClickHouse = "TLSv1.3"
)

func (e MaximumTLSVersionClickHouse) ToPointer() *MaximumTLSVersionClickHouse {
	return &e
}
func (e *MaximumTLSVersionClickHouse) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = MaximumTLSVersionClickHouse(v)
		return nil
	default:
		return fmt.Errorf("invalid value for MaximumTLSVersionClickHouse: %v", v)
	}
}

type TLSSettingsClientSideClickHouse struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate.
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key.
	Passphrase *string `json:"passphrase,omitempty"`
	// Minimum TLS version to use when connecting
	MinVersion *MinimumTLSVersionClickHouse `json:"minVersion,omitempty"`
	// Maximum TLS version to use when connecting
	MaxVersion *MaximumTLSVersionClickHouse `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideClickHouse) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideClickHouse) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TLSSettingsClientSideClickHouse) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *TLSSettingsClientSideClickHouse) GetServername() *string {
	if o == nil {
		return nil
	}
	return o.Servername
}

func (o *TLSSettingsClientSideClickHouse) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

func (o *TLSSettingsClientSideClickHouse) GetCaPath() *string {
	if o == nil {
		return nil
	}
	return o.CaPath
}

func (o *TLSSettingsClientSideClickHouse) GetPrivKeyPath() *string {
	if o == nil {
		return nil
	}
	return o.PrivKeyPath
}

func (o *TLSSettingsClientSideClickHouse) GetCertPath() *string {
	if o == nil {
		return nil
	}
	return o.CertPath
}

func (o *TLSSettingsClientSideClickHouse) GetPassphrase() *string {
	if o == nil {
		return nil
	}
	return o.Passphrase
}

func (o *TLSSettingsClientSideClickHouse) GetMinVersion() *MinimumTLSVersionClickHouse {
	if o == nil {
		return nil
	}
	return o.MinVersion
}

func (o *TLSSettingsClientSideClickHouse) GetMaxVersion() *MaximumTLSVersionClickHouse {
	if o == nil {
		return nil
	}
	return o.MaxVersion
}

type ExtraHTTPHeaderClickHouse struct {
	// Field name
	Name *string `json:"name,omitempty"`
	// Field value
	Value string `json:"value"`
}

func (o *ExtraHTTPHeaderClickHouse) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderClickHouse) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeClickHouse - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeClickHouse string

const (
	FailedRequestLoggingModeClickHousePayload           FailedRequestLoggingModeClickHouse = "payload"
	FailedRequestLoggingModeClickHousePayloadAndHeaders FailedRequestLoggingModeClickHouse = "payloadAndHeaders"
	FailedRequestLoggingModeClickHouseNone              FailedRequestLoggingModeClickHouse = "none"
)

func (e FailedRequestLoggingModeClickHouse) ToPointer() *FailedRequestLoggingModeClickHouse {
	return &e
}
func (e *FailedRequestLoggingModeClickHouse) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeClickHouse(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeClickHouse: %v", v)
	}
}

type ResponseRetrySettingClickHouse struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingClickHouse) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingClickHouse) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingClickHouse) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingClickHouse) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingClickHouse) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingClickHouse) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsClickHouse struct {
	// Enable to retry on request timeout
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsClickHouse) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsClickHouse) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsClickHouse) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsClickHouse) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsClickHouse) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsClickHouse) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorClickHouse - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorClickHouse string

const (
	BackpressureBehaviorClickHouseBlock BackpressureBehaviorClickHouse = "block"
	BackpressureBehaviorClickHouseDrop  BackpressureBehaviorClickHouse = "drop"
	BackpressureBehaviorClickHouseQueue BackpressureBehaviorClickHouse = "queue"
)

func (e BackpressureBehaviorClickHouse) ToPointer() *BackpressureBehaviorClickHouse {
	return &e
}
func (e *BackpressureBehaviorClickHouse) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorClickHouse(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorClickHouse: %v", v)
	}
}

type OauthParamClickHouse struct {
	// OAuth parameter name
	Name string `json:"name"`
	// OAuth parameter value
	Value string `json:"value"`
}

func (o *OauthParamClickHouse) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OauthParamClickHouse) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OauthHeaderClickHouse struct {
	// OAuth header name
	Name string `json:"name"`
	// OAuth header value
	Value string `json:"value"`
}

func (o *OauthHeaderClickHouse) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OauthHeaderClickHouse) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OutputColumnMapping struct {
	// Name of the column in ClickHouse that will store field value
	ColumnName string `json:"columnName"`
	// Type of the column in the ClickHouse database
	ColumnType *string `json:"columnType,omitempty"`
	// JavaScript expression to compute value to be inserted into ClickHouse table
	ColumnValueExpression string `json:"columnValueExpression"`
}

func (o *OutputColumnMapping) GetColumnName() string {
	if o == nil {
		return ""
	}
	return o.ColumnName
}

func (o *OutputColumnMapping) GetColumnType() *string {
	if o == nil {
		return nil
	}
	return o.ColumnType
}

func (o *OutputColumnMapping) GetColumnValueExpression() string {
	if o == nil {
		return ""
	}
	return o.ColumnValueExpression
}

// CompressionClickHouse - Codec to use to compress the persisted data.
type CompressionClickHouse string

const (
	CompressionClickHouseNone CompressionClickHouse = "none"
	CompressionClickHouseGzip CompressionClickHouse = "gzip"
)

func (e CompressionClickHouse) ToPointer() *CompressionClickHouse {
	return &e
}
func (e *CompressionClickHouse) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionClickHouse(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionClickHouse: %v", v)
	}
}

// QueueFullBehaviorClickHouse - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorClickHouse string

const (
	QueueFullBehaviorClickHouseBlock QueueFullBehaviorClickHouse = "block"
	QueueFullBehaviorClickHouseDrop  QueueFullBehaviorClickHouse = "drop"
)

func (e QueueFullBehaviorClickHouse) ToPointer() *QueueFullBehaviorClickHouse {
	return &e
}
func (e *QueueFullBehaviorClickHouse) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorClickHouse(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorClickHouse: %v", v)
	}
}

// ModeClickHouse - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type ModeClickHouse string

const (
	ModeClickHouseError        ModeClickHouse = "error"
	ModeClickHouseBackpressure ModeClickHouse = "backpressure"
	ModeClickHouseAlways       ModeClickHouse = "always"
)

func (e ModeClickHouse) ToPointer() *ModeClickHouse {
	return &e
}
func (e *ModeClickHouse) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeClickHouse(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeClickHouse: %v", v)
	}
}

type PqControlsClickHouse struct {
}

type OutputClickHouse struct {
	// Unique ID for this output
	ID   *string         `json:"id,omitempty"`
	Type *TypeClickHouse `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// URL of the ClickHouse instance. Example: http://localhost:8123/
	URL      string                        `json:"url"`
	AuthType *AuthenticationTypeClickHouse `default:"none" json:"authType"`
	Database string                        `json:"database"`
	// Name of the ClickHouse table where data will be inserted. Name can contain letters (A-Z, a-z), numbers (0-9), and the character "_", and must start with either a letter or the character "_".
	TableName string `json:"tableName"`
	// Data format to use when sending data to ClickHouse. Defaults to JSON Compact.
	Format *FormatClickHouse `default:"json-compact-each-row-with-names" json:"format"`
	// How event fields are mapped to ClickHouse columns.
	MappingType *OutputMappingType `default:"automatic" json:"mappingType"`
	// Collect data into batches for later processing. Disable to write to a ClickHouse table immediately.
	AsyncInserts *bool                            `default:"false" json:"asyncInserts"`
	TLS          *TLSSettingsClientSideClickHouse `json:"tls,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Defaults to Yes. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max body size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events.
	ExtraHTTPHeaders []ExtraHTTPHeaderClickHouse `json:"extraHttpHeaders,omitempty"`
	// Enables round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeClickHouse `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable).
	ResponseRetrySettings []ResponseRetrySettingClickHouse `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsClickHouse  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// Log the most recent event that fails to match the table schema
	DumpFormatErrorsToDisk *bool `default:"false" json:"dumpFormatErrorsToDisk"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorClickHouse `default:"block" json:"onBackpressure"`
	Description    *string                         `json:"description,omitempty"`
	Username       *string                         `json:"username,omitempty"`
	Password       *string                         `json:"password,omitempty"`
	// Bearer token to include in the authorization header
	Token *string `json:"token,omitempty"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// URL for OAuth
	LoginURL *string `json:"loginUrl,omitempty"`
	// Secret parameter name to pass in request body
	SecretParamName *string `json:"secretParamName,omitempty"`
	// Secret parameter value to pass in request body
	Secret *string `json:"secret,omitempty"`
	// Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
	TokenAttributeName *string `json:"tokenAttributeName,omitempty"`
	// JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
	AuthHeaderExpr *string `default:"Bearer \\${token}" json:"authHeaderExpr"`
	// How often the OAuth token should be refreshed.
	TokenTimeoutSecs *float64 `default:"3600" json:"tokenTimeoutSecs"`
	// Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthParams []OauthParamClickHouse `json:"oauthParams,omitempty"`
	// Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthHeaders []OauthHeaderClickHouse `json:"oauthHeaders,omitempty"`
	// Username for certificate authentication
	SQLUsername *string `json:"sqlUsername,omitempty"`
	// Cribl will wait for confirmation that data has been fully inserted into the ClickHouse database before proceeding. Disabling this option can increase throughput, but Cribl wont be able to verify data has been completely inserted.
	WaitForAsyncInserts *bool `default:"true" json:"waitForAsyncInserts"`
	// Fields to exclude from sending to ClickHouse
	ExcludeMappingFields []string `json:"excludeMappingFields,omitempty"`
	// Retrieves the table schema from ClickHouse and populates the Column Mapping table
	DescribeTable  *string               `json:"describeTable,omitempty"`
	ColumnMappings []OutputColumnMapping `json:"columnMappings,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *CompressionClickHouse `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorClickHouse `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeClickHouse       `default:"error" json:"pqMode"`
	PqControls *PqControlsClickHouse `json:"pqControls,omitempty"`
	Status     *TFStatus             `json:"status,omitempty"`
}

func (o OutputClickHouse) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputClickHouse) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputClickHouse) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputClickHouse) GetType() *TypeClickHouse {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputClickHouse) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputClickHouse) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputClickHouse) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputClickHouse) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputClickHouse) GetURL() string {
	if o == nil {
		return ""
	}
	return o.URL
}

func (o *OutputClickHouse) GetAuthType() *AuthenticationTypeClickHouse {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputClickHouse) GetDatabase() string {
	if o == nil {
		return ""
	}
	return o.Database
}

func (o *OutputClickHouse) GetTableName() string {
	if o == nil {
		return ""
	}
	return o.TableName
}

func (o *OutputClickHouse) GetFormat() *FormatClickHouse {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputClickHouse) GetMappingType() *OutputMappingType {
	if o == nil {
		return nil
	}
	return o.MappingType
}

func (o *OutputClickHouse) GetAsyncInserts() *bool {
	if o == nil {
		return nil
	}
	return o.AsyncInserts
}

func (o *OutputClickHouse) GetTLS() *TLSSettingsClientSideClickHouse {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputClickHouse) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputClickHouse) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputClickHouse) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputClickHouse) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputClickHouse) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputClickHouse) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputClickHouse) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputClickHouse) GetExtraHTTPHeaders() []ExtraHTTPHeaderClickHouse {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputClickHouse) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputClickHouse) GetFailedRequestLoggingMode() *FailedRequestLoggingModeClickHouse {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputClickHouse) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputClickHouse) GetResponseRetrySettings() []ResponseRetrySettingClickHouse {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputClickHouse) GetTimeoutRetrySettings() *TimeoutRetrySettingsClickHouse {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputClickHouse) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputClickHouse) GetDumpFormatErrorsToDisk() *bool {
	if o == nil {
		return nil
	}
	return o.DumpFormatErrorsToDisk
}

func (o *OutputClickHouse) GetOnBackpressure() *BackpressureBehaviorClickHouse {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputClickHouse) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputClickHouse) GetUsername() *string {
	if o == nil {
		return nil
	}
	return o.Username
}

func (o *OutputClickHouse) GetPassword() *string {
	if o == nil {
		return nil
	}
	return o.Password
}

func (o *OutputClickHouse) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputClickHouse) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

func (o *OutputClickHouse) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputClickHouse) GetLoginURL() *string {
	if o == nil {
		return nil
	}
	return o.LoginURL
}

func (o *OutputClickHouse) GetSecretParamName() *string {
	if o == nil {
		return nil
	}
	return o.SecretParamName
}

func (o *OutputClickHouse) GetSecret() *string {
	if o == nil {
		return nil
	}
	return o.Secret
}

func (o *OutputClickHouse) GetTokenAttributeName() *string {
	if o == nil {
		return nil
	}
	return o.TokenAttributeName
}

func (o *OutputClickHouse) GetAuthHeaderExpr() *string {
	if o == nil {
		return nil
	}
	return o.AuthHeaderExpr
}

func (o *OutputClickHouse) GetTokenTimeoutSecs() *float64 {
	if o == nil {
		return nil
	}
	return o.TokenTimeoutSecs
}

func (o *OutputClickHouse) GetOauthParams() []OauthParamClickHouse {
	if o == nil {
		return nil
	}
	return o.OauthParams
}

func (o *OutputClickHouse) GetOauthHeaders() []OauthHeaderClickHouse {
	if o == nil {
		return nil
	}
	return o.OauthHeaders
}

func (o *OutputClickHouse) GetSQLUsername() *string {
	if o == nil {
		return nil
	}
	return o.SQLUsername
}

func (o *OutputClickHouse) GetWaitForAsyncInserts() *bool {
	if o == nil {
		return nil
	}
	return o.WaitForAsyncInserts
}

func (o *OutputClickHouse) GetExcludeMappingFields() []string {
	if o == nil {
		return nil
	}
	return o.ExcludeMappingFields
}

func (o *OutputClickHouse) GetDescribeTable() *string {
	if o == nil {
		return nil
	}
	return o.DescribeTable
}

func (o *OutputClickHouse) GetColumnMappings() []OutputColumnMapping {
	if o == nil {
		return nil
	}
	return o.ColumnMappings
}

func (o *OutputClickHouse) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputClickHouse) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputClickHouse) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputClickHouse) GetPqCompress() *CompressionClickHouse {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputClickHouse) GetPqOnBackpressure() *QueueFullBehaviorClickHouse {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputClickHouse) GetPqMode() *ModeClickHouse {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputClickHouse) GetPqControls() *PqControlsClickHouse {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputClickHouse) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeDiskSpool string

const (
	TypeDiskSpoolDiskSpool TypeDiskSpool = "disk_spool"
)

func (e TypeDiskSpool) ToPointer() *TypeDiskSpool {
	return &e
}
func (e *TypeDiskSpool) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "disk_spool":
		*e = TypeDiskSpool(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeDiskSpool: %v", v)
	}
}

// CompressionDiskSpool - Data compression format. Default is gzip.
type CompressionDiskSpool string

const (
	CompressionDiskSpoolNone CompressionDiskSpool = "none"
	CompressionDiskSpoolGzip CompressionDiskSpool = "gzip"
)

func (e CompressionDiskSpool) ToPointer() *CompressionDiskSpool {
	return &e
}
func (e *CompressionDiskSpool) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionDiskSpool(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionDiskSpool: %v", v)
	}
}

type OutputDiskSpool struct {
	// Unique ID for this output
	ID   string        `json:"id"`
	Type TypeDiskSpool `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Time period for grouping spooled events. Default is 10m.
	TimeWindow *string `default:"10m" json:"timeWindow"`
	// Maximum disk space that can be consumed before older buckets are deleted. Examples: 420MB, 4GB. Default is 1GB.
	MaxDataSize *string `default:"1GB" json:"maxDataSize"`
	// Maximum amount of time to retain data before older buckets are deleted. Examples: 2h, 4d. Default is 24h.
	MaxDataTime *string `default:"24h" json:"maxDataTime"`
	// Data compression format. Default is gzip.
	Compress *CompressionDiskSpool `default:"gzip" json:"compress"`
	// JavaScript expression defining how files are partitioned and organized within the time-buckets. If blank, the event's __partition property is used and otherwise, events go directly into the time-bucket directory.
	PartitionExpr *string   `json:"partitionExpr,omitempty"`
	Description   *string   `json:"description,omitempty"`
	Status        *TFStatus `json:"status,omitempty"`
}

func (o OutputDiskSpool) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputDiskSpool) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputDiskSpool) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputDiskSpool) GetType() TypeDiskSpool {
	if o == nil {
		return TypeDiskSpool("")
	}
	return o.Type
}

func (o *OutputDiskSpool) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDiskSpool) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDiskSpool) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDiskSpool) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputDiskSpool) GetTimeWindow() *string {
	if o == nil {
		return nil
	}
	return o.TimeWindow
}

func (o *OutputDiskSpool) GetMaxDataSize() *string {
	if o == nil {
		return nil
	}
	return o.MaxDataSize
}

func (o *OutputDiskSpool) GetMaxDataTime() *string {
	if o == nil {
		return nil
	}
	return o.MaxDataTime
}

func (o *OutputDiskSpool) GetCompress() *CompressionDiskSpool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputDiskSpool) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputDiskSpool) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputDiskSpool) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeCriblLake string

const (
	TypeCriblLakeCriblLake TypeCriblLake = "cribl_lake"
)

func (e TypeCriblLake) ToPointer() *TypeCriblLake {
	return &e
}
func (e *TypeCriblLake) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "cribl_lake":
		*e = TypeCriblLake(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeCriblLake: %v", v)
	}
}

// SignatureVersionCriblLake - Signature version to use for signing S3 requests
type SignatureVersionCriblLake string

const (
	SignatureVersionCriblLakeV2 SignatureVersionCriblLake = "v2"
	SignatureVersionCriblLakeV4 SignatureVersionCriblLake = "v4"
)

func (e SignatureVersionCriblLake) ToPointer() *SignatureVersionCriblLake {
	return &e
}
func (e *SignatureVersionCriblLake) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "v2":
		fallthrough
	case "v4":
		*e = SignatureVersionCriblLake(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SignatureVersionCriblLake: %v", v)
	}
}

// ObjectACLCriblLake - Object ACL to assign to uploaded objects.
type ObjectACLCriblLake string

const (
	ObjectACLCriblLakePrivate                ObjectACLCriblLake = "private"
	ObjectACLCriblLakePublicRead             ObjectACLCriblLake = "public-read"
	ObjectACLCriblLakePublicReadWrite        ObjectACLCriblLake = "public-read-write"
	ObjectACLCriblLakeAuthenticatedRead      ObjectACLCriblLake = "authenticated-read"
	ObjectACLCriblLakeAwsExecRead            ObjectACLCriblLake = "aws-exec-read"
	ObjectACLCriblLakeBucketOwnerRead        ObjectACLCriblLake = "bucket-owner-read"
	ObjectACLCriblLakeBucketOwnerFullControl ObjectACLCriblLake = "bucket-owner-full-control"
)

func (e ObjectACLCriblLake) ToPointer() *ObjectACLCriblLake {
	return &e
}
func (e *ObjectACLCriblLake) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "private":
		fallthrough
	case "public-read":
		fallthrough
	case "public-read-write":
		fallthrough
	case "authenticated-read":
		fallthrough
	case "aws-exec-read":
		fallthrough
	case "bucket-owner-read":
		fallthrough
	case "bucket-owner-full-control":
		*e = ObjectACLCriblLake(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ObjectACLCriblLake: %v", v)
	}
}

// StorageClassCriblLake - Storage class to select for uploaded objects.
type StorageClassCriblLake string

const (
	StorageClassCriblLakeStandard           StorageClassCriblLake = "STANDARD"
	StorageClassCriblLakeReducedRedundancy  StorageClassCriblLake = "REDUCED_REDUNDANCY"
	StorageClassCriblLakeStandardIa         StorageClassCriblLake = "STANDARD_IA"
	StorageClassCriblLakeOnezoneIa          StorageClassCriblLake = "ONEZONE_IA"
	StorageClassCriblLakeIntelligentTiering StorageClassCriblLake = "INTELLIGENT_TIERING"
	StorageClassCriblLakeGlacier            StorageClassCriblLake = "GLACIER"
	StorageClassCriblLakeGlacierIr          StorageClassCriblLake = "GLACIER_IR"
	StorageClassCriblLakeDeepArchive        StorageClassCriblLake = "DEEP_ARCHIVE"
)

func (e StorageClassCriblLake) ToPointer() *StorageClassCriblLake {
	return &e
}
func (e *StorageClassCriblLake) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "STANDARD":
		fallthrough
	case "REDUCED_REDUNDANCY":
		fallthrough
	case "STANDARD_IA":
		fallthrough
	case "ONEZONE_IA":
		fallthrough
	case "INTELLIGENT_TIERING":
		fallthrough
	case "GLACIER":
		fallthrough
	case "GLACIER_IR":
		fallthrough
	case "DEEP_ARCHIVE":
		*e = StorageClassCriblLake(v)
		return nil
	default:
		return fmt.Errorf("invalid value for StorageClassCriblLake: %v", v)
	}
}

// ServerSideEncryptionCriblLake - Server-side encryption for uploaded objects.
type ServerSideEncryptionCriblLake string

const (
	ServerSideEncryptionCriblLakeAes256 ServerSideEncryptionCriblLake = "AES256"
	ServerSideEncryptionCriblLakeAwsKms ServerSideEncryptionCriblLake = "aws:kms"
)

func (e ServerSideEncryptionCriblLake) ToPointer() *ServerSideEncryptionCriblLake {
	return &e
}
func (e *ServerSideEncryptionCriblLake) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "AES256":
		fallthrough
	case "aws:kms":
		*e = ServerSideEncryptionCriblLake(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ServerSideEncryptionCriblLake: %v", v)
	}
}

// BackpressureBehaviorCriblLake - Whether to block or drop events when all receivers are exerting backpressure
type BackpressureBehaviorCriblLake string

const (
	BackpressureBehaviorCriblLakeBlock BackpressureBehaviorCriblLake = "block"
	BackpressureBehaviorCriblLakeDrop  BackpressureBehaviorCriblLake = "drop"
)

func (e BackpressureBehaviorCriblLake) ToPointer() *BackpressureBehaviorCriblLake {
	return &e
}
func (e *BackpressureBehaviorCriblLake) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = BackpressureBehaviorCriblLake(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorCriblLake: %v", v)
	}
}

// DiskSpaceProtectionCriblLake - Whether to block or drop events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionCriblLake string

const (
	DiskSpaceProtectionCriblLakeBlock DiskSpaceProtectionCriblLake = "block"
	DiskSpaceProtectionCriblLakeDrop  DiskSpaceProtectionCriblLake = "drop"
)

func (e DiskSpaceProtectionCriblLake) ToPointer() *DiskSpaceProtectionCriblLake {
	return &e
}
func (e *DiskSpaceProtectionCriblLake) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = DiskSpaceProtectionCriblLake(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DiskSpaceProtectionCriblLake: %v", v)
	}
}

type AwsAuthenticationMethod string

const (
	AwsAuthenticationMethodAuto    AwsAuthenticationMethod = "auto"
	AwsAuthenticationMethodAutoRPC AwsAuthenticationMethod = "auto_rpc"
	AwsAuthenticationMethodManual  AwsAuthenticationMethod = "manual"
)

func (e AwsAuthenticationMethod) ToPointer() *AwsAuthenticationMethod {
	return &e
}
func (e *AwsAuthenticationMethod) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "auto":
		fallthrough
	case "auto_rpc":
		fallthrough
	case "manual":
		*e = AwsAuthenticationMethod(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AwsAuthenticationMethod: %v", v)
	}
}

type FormatCriblLake string

const (
	FormatCriblLakeJSON    FormatCriblLake = "json"
	FormatCriblLakeParquet FormatCriblLake = "parquet"
	FormatCriblLakeDdss    FormatCriblLake = "ddss"
)

func (e FormatCriblLake) ToPointer() *FormatCriblLake {
	return &e
}
func (e *FormatCriblLake) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "json":
		fallthrough
	case "parquet":
		fallthrough
	case "ddss":
		*e = FormatCriblLake(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FormatCriblLake: %v", v)
	}
}

type OutputCriblLake struct {
	// Unique ID for this output
	ID   *string       `json:"id,omitempty"`
	Type TypeCriblLake `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. E.g., referencing a Global Variable: `myBucket-${C.vars.myVar}`.
	Bucket *string `json:"bucket,omitempty"`
	// Region where the S3 bucket is located.
	Region *string `json:"region,omitempty"`
	// Secret key. This value can be a constant or a JavaScript expression(e.g., `${C.env.SOME_SECRET}`).
	AwsSecretKey *string `json:"awsSecretKey,omitempty"`
	// S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.
	Endpoint *string `json:"endpoint,omitempty"`
	// Signature version to use for signing S3 requests
	SignatureVersion *SignatureVersionCriblLake `default:"v4" json:"signatureVersion"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Use Assume Role credentials to access S3
	EnableAssumeRole *bool `default:"false" json:"enableAssumeRole"`
	// Amazon Resource Name (ARN) of the role to assume
	AssumeRoleArn *string `json:"assumeRoleArn,omitempty"`
	// External ID to use when assuming role
	AssumeRoleExternalID *string `json:"assumeRoleExternalId,omitempty"`
	// Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
	DurationSeconds *float64 `default:"3600" json:"durationSeconds"`
	// Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant stable storage.
	StagePath *string `default:"\\$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Append output's ID to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Lake dataset to send the data to.
	DestPath string `json:"destPath"`
	// Object ACL to assign to uploaded objects.
	ObjectACL *ObjectACLCriblLake `default:"private" json:"objectACL"`
	// Storage class to select for uploaded objects.
	StorageClass *StorageClassCriblLake `json:"storageClass,omitempty"`
	// Server-side encryption for uploaded objects.
	ServerSideEncryption *ServerSideEncryptionCriblLake `json:"serverSideEncryption,omitempty"`
	// ID or ARN of the KMS customer-managed key to use for encryption
	KmsKeyID *string `json:"kmsKeyId,omitempty"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// Whether to block or drop events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorCriblLake `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, dead-letter it to prevent further errors.
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// Whether to block or drop events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionCriblLake `default:"block" json:"onDiskFullBackpressure"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Disable if you can access files within the bucket but not the bucket itself.
	VerifyPermissions *bool `default:"true" json:"verifyPermissions"`
	// Maximum number of files that can be waiting for upload before backpressure is applied
	MaxClosingFilesToBackpressure *float64                 `default:"100" json:"maxClosingFilesToBackpressure"`
	AwsAuthenticationMethod       *AwsAuthenticationMethod `default:"auto" json:"awsAuthenticationMethod"`
	Format                        *FormatCriblLake         `json:"format,omitempty"`
	// Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.
	MaxConcurrentFileParts *float64 `default:"1" json:"maxConcurrentFileParts"`
	Description            *string  `json:"description,omitempty"`
	// How frequently, in seconds, to clean up empty directories when 'Remove empty staging dirs' is enabled
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"\\$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum          *float64  `default:"20" json:"maxRetryNum"`
	Status               *TFStatus `json:"status,omitempty"`
	AdditionalProperties any       `additionalProperties:"true" json:"-"`
}

func (o OutputCriblLake) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputCriblLake) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputCriblLake) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputCriblLake) GetType() TypeCriblLake {
	if o == nil {
		return TypeCriblLake("")
	}
	return o.Type
}

func (o *OutputCriblLake) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputCriblLake) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputCriblLake) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputCriblLake) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputCriblLake) GetBucket() *string {
	if o == nil {
		return nil
	}
	return o.Bucket
}

func (o *OutputCriblLake) GetRegion() *string {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputCriblLake) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputCriblLake) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputCriblLake) GetSignatureVersion() *SignatureVersionCriblLake {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputCriblLake) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputCriblLake) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputCriblLake) GetEnableAssumeRole() *bool {
	if o == nil {
		return nil
	}
	return o.EnableAssumeRole
}

func (o *OutputCriblLake) GetAssumeRoleArn() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleArn
}

func (o *OutputCriblLake) GetAssumeRoleExternalID() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleExternalID
}

func (o *OutputCriblLake) GetDurationSeconds() *float64 {
	if o == nil {
		return nil
	}
	return o.DurationSeconds
}

func (o *OutputCriblLake) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputCriblLake) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputCriblLake) GetDestPath() string {
	if o == nil {
		return ""
	}
	return o.DestPath
}

func (o *OutputCriblLake) GetObjectACL() *ObjectACLCriblLake {
	if o == nil {
		return nil
	}
	return o.ObjectACL
}

func (o *OutputCriblLake) GetStorageClass() *StorageClassCriblLake {
	if o == nil {
		return nil
	}
	return o.StorageClass
}

func (o *OutputCriblLake) GetServerSideEncryption() *ServerSideEncryptionCriblLake {
	if o == nil {
		return nil
	}
	return o.ServerSideEncryption
}

func (o *OutputCriblLake) GetKmsKeyID() *string {
	if o == nil {
		return nil
	}
	return o.KmsKeyID
}

func (o *OutputCriblLake) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputCriblLake) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputCriblLake) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputCriblLake) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputCriblLake) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputCriblLake) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputCriblLake) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputCriblLake) GetOnBackpressure() *BackpressureBehaviorCriblLake {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputCriblLake) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputCriblLake) GetOnDiskFullBackpressure() *DiskSpaceProtectionCriblLake {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputCriblLake) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputCriblLake) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputCriblLake) GetVerifyPermissions() *bool {
	if o == nil {
		return nil
	}
	return o.VerifyPermissions
}

func (o *OutputCriblLake) GetMaxClosingFilesToBackpressure() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxClosingFilesToBackpressure
}

func (o *OutputCriblLake) GetAwsAuthenticationMethod() *AwsAuthenticationMethod {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputCriblLake) GetFormat() *FormatCriblLake {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputCriblLake) GetMaxConcurrentFileParts() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentFileParts
}

func (o *OutputCriblLake) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputCriblLake) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputCriblLake) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputCriblLake) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

func (o *OutputCriblLake) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

func (o *OutputCriblLake) GetAdditionalProperties() any {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

type OutputTypeSecurityLake string

const (
	OutputTypeSecurityLakeSecurityLake OutputTypeSecurityLake = "security_lake"
)

func (e OutputTypeSecurityLake) ToPointer() *OutputTypeSecurityLake {
	return &e
}
func (e *OutputTypeSecurityLake) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "security_lake":
		*e = OutputTypeSecurityLake(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeSecurityLake: %v", v)
	}
}

// OutputAuthenticationMethodSecurityLake - AWS authentication method. Choose Auto to use IAM roles.
type OutputAuthenticationMethodSecurityLake string

const (
	OutputAuthenticationMethodSecurityLakeAuto   OutputAuthenticationMethodSecurityLake = "auto"
	OutputAuthenticationMethodSecurityLakeManual OutputAuthenticationMethodSecurityLake = "manual"
	OutputAuthenticationMethodSecurityLakeSecret OutputAuthenticationMethodSecurityLake = "secret"
)

func (e OutputAuthenticationMethodSecurityLake) ToPointer() *OutputAuthenticationMethodSecurityLake {
	return &e
}
func (e *OutputAuthenticationMethodSecurityLake) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "auto":
		fallthrough
	case "manual":
		fallthrough
	case "secret":
		*e = OutputAuthenticationMethodSecurityLake(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputAuthenticationMethodSecurityLake: %v", v)
	}
}

// OutputSignatureVersionSecurityLake - Signature version to use for signing Amazon Security Lake requests
type OutputSignatureVersionSecurityLake string

const (
	OutputSignatureVersionSecurityLakeV2 OutputSignatureVersionSecurityLake = "v2"
	OutputSignatureVersionSecurityLakeV4 OutputSignatureVersionSecurityLake = "v4"
)

func (e OutputSignatureVersionSecurityLake) ToPointer() *OutputSignatureVersionSecurityLake {
	return &e
}
func (e *OutputSignatureVersionSecurityLake) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "v2":
		fallthrough
	case "v4":
		*e = OutputSignatureVersionSecurityLake(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputSignatureVersionSecurityLake: %v", v)
	}
}

// ObjectACLSecurityLake - Object ACL to assign to uploaded objects.
type ObjectACLSecurityLake string

const (
	ObjectACLSecurityLakePrivate                ObjectACLSecurityLake = "private"
	ObjectACLSecurityLakePublicRead             ObjectACLSecurityLake = "public-read"
	ObjectACLSecurityLakePublicReadWrite        ObjectACLSecurityLake = "public-read-write"
	ObjectACLSecurityLakeAuthenticatedRead      ObjectACLSecurityLake = "authenticated-read"
	ObjectACLSecurityLakeAwsExecRead            ObjectACLSecurityLake = "aws-exec-read"
	ObjectACLSecurityLakeBucketOwnerRead        ObjectACLSecurityLake = "bucket-owner-read"
	ObjectACLSecurityLakeBucketOwnerFullControl ObjectACLSecurityLake = "bucket-owner-full-control"
)

func (e ObjectACLSecurityLake) ToPointer() *ObjectACLSecurityLake {
	return &e
}
func (e *ObjectACLSecurityLake) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "private":
		fallthrough
	case "public-read":
		fallthrough
	case "public-read-write":
		fallthrough
	case "authenticated-read":
		fallthrough
	case "aws-exec-read":
		fallthrough
	case "bucket-owner-read":
		fallthrough
	case "bucket-owner-full-control":
		*e = ObjectACLSecurityLake(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ObjectACLSecurityLake: %v", v)
	}
}

// StorageClassSecurityLake - Storage class to select for uploaded objects.
type StorageClassSecurityLake string

const (
	StorageClassSecurityLakeStandard           StorageClassSecurityLake = "STANDARD"
	StorageClassSecurityLakeReducedRedundancy  StorageClassSecurityLake = "REDUCED_REDUNDANCY"
	StorageClassSecurityLakeStandardIa         StorageClassSecurityLake = "STANDARD_IA"
	StorageClassSecurityLakeOnezoneIa          StorageClassSecurityLake = "ONEZONE_IA"
	StorageClassSecurityLakeIntelligentTiering StorageClassSecurityLake = "INTELLIGENT_TIERING"
	StorageClassSecurityLakeGlacier            StorageClassSecurityLake = "GLACIER"
	StorageClassSecurityLakeGlacierIr          StorageClassSecurityLake = "GLACIER_IR"
	StorageClassSecurityLakeDeepArchive        StorageClassSecurityLake = "DEEP_ARCHIVE"
)

func (e StorageClassSecurityLake) ToPointer() *StorageClassSecurityLake {
	return &e
}
func (e *StorageClassSecurityLake) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "STANDARD":
		fallthrough
	case "REDUCED_REDUNDANCY":
		fallthrough
	case "STANDARD_IA":
		fallthrough
	case "ONEZONE_IA":
		fallthrough
	case "INTELLIGENT_TIERING":
		fallthrough
	case "GLACIER":
		fallthrough
	case "GLACIER_IR":
		fallthrough
	case "DEEP_ARCHIVE":
		*e = StorageClassSecurityLake(v)
		return nil
	default:
		return fmt.Errorf("invalid value for StorageClassSecurityLake: %v", v)
	}
}

// ServerSideEncryptionSecurityLake - Server-side encryption for uploaded objects.
type ServerSideEncryptionSecurityLake string

const (
	ServerSideEncryptionSecurityLakeAes256 ServerSideEncryptionSecurityLake = "AES256"
	ServerSideEncryptionSecurityLakeAwsKms ServerSideEncryptionSecurityLake = "aws:kms"
)

func (e ServerSideEncryptionSecurityLake) ToPointer() *ServerSideEncryptionSecurityLake {
	return &e
}
func (e *ServerSideEncryptionSecurityLake) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "AES256":
		fallthrough
	case "aws:kms":
		*e = ServerSideEncryptionSecurityLake(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ServerSideEncryptionSecurityLake: %v", v)
	}
}

// BackpressureBehaviorSecurityLake - Whether to block or drop events when all receivers are exerting backpressure
type BackpressureBehaviorSecurityLake string

const (
	BackpressureBehaviorSecurityLakeBlock BackpressureBehaviorSecurityLake = "block"
	BackpressureBehaviorSecurityLakeDrop  BackpressureBehaviorSecurityLake = "drop"
)

func (e BackpressureBehaviorSecurityLake) ToPointer() *BackpressureBehaviorSecurityLake {
	return &e
}
func (e *BackpressureBehaviorSecurityLake) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = BackpressureBehaviorSecurityLake(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorSecurityLake: %v", v)
	}
}

// DiskSpaceProtectionSecurityLake - Whether to block or drop events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionSecurityLake string

const (
	DiskSpaceProtectionSecurityLakeBlock DiskSpaceProtectionSecurityLake = "block"
	DiskSpaceProtectionSecurityLakeDrop  DiskSpaceProtectionSecurityLake = "drop"
)

func (e DiskSpaceProtectionSecurityLake) ToPointer() *DiskSpaceProtectionSecurityLake {
	return &e
}
func (e *DiskSpaceProtectionSecurityLake) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = DiskSpaceProtectionSecurityLake(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DiskSpaceProtectionSecurityLake: %v", v)
	}
}

// ParquetVersionSecurityLake - Determines which data types are supported and how they are represented
type ParquetVersionSecurityLake string

const (
	ParquetVersionSecurityLakeParquet10 ParquetVersionSecurityLake = "PARQUET_1_0"
	ParquetVersionSecurityLakeParquet24 ParquetVersionSecurityLake = "PARQUET_2_4"
	ParquetVersionSecurityLakeParquet26 ParquetVersionSecurityLake = "PARQUET_2_6"
)

func (e ParquetVersionSecurityLake) ToPointer() *ParquetVersionSecurityLake {
	return &e
}
func (e *ParquetVersionSecurityLake) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "PARQUET_1_0":
		fallthrough
	case "PARQUET_2_4":
		fallthrough
	case "PARQUET_2_6":
		*e = ParquetVersionSecurityLake(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ParquetVersionSecurityLake: %v", v)
	}
}

// DataPageVersionSecurityLake - Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
type DataPageVersionSecurityLake string

const (
	DataPageVersionSecurityLakeDataPageV1 DataPageVersionSecurityLake = "DATA_PAGE_V1"
	DataPageVersionSecurityLakeDataPageV2 DataPageVersionSecurityLake = "DATA_PAGE_V2"
)

func (e DataPageVersionSecurityLake) ToPointer() *DataPageVersionSecurityLake {
	return &e
}
func (e *DataPageVersionSecurityLake) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "DATA_PAGE_V1":
		fallthrough
	case "DATA_PAGE_V2":
		*e = DataPageVersionSecurityLake(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DataPageVersionSecurityLake: %v", v)
	}
}

type KeyValueMetadatumSecurityLake struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (k KeyValueMetadatumSecurityLake) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(k, "", false)
}

func (k *KeyValueMetadatumSecurityLake) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &k, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *KeyValueMetadatumSecurityLake) GetKey() *string {
	if o == nil {
		return nil
	}
	return o.Key
}

func (o *KeyValueMetadatumSecurityLake) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OutputSecurityLake struct {
	// Unique ID for this output
	ID   *string                 `json:"id,omitempty"`
	Type *OutputTypeSecurityLake `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions and labels to generated metrics and logs, respectively.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. E.g., referencing a Global Variable: `myBucket-${C.vars.myVar}`.
	Bucket string `json:"bucket"`
	// Region where the Amazon Security Lake is located.
	Region       string  `json:"region"`
	AwsSecretKey *string `json:"awsSecretKey,omitempty"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *OutputAuthenticationMethodSecurityLake `default:"auto" json:"awsAuthenticationMethod"`
	// Amazon Security Lake service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Amazon Security Lake-compatible endpoint.
	Endpoint *string `json:"endpoint,omitempty"`
	// Signature version to use for signing Amazon Security Lake requests
	SignatureVersion *OutputSignatureVersionSecurityLake `default:"v4" json:"signatureVersion"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Use Assume Role credentials to access S3
	EnableAssumeRole *bool `default:"false" json:"enableAssumeRole"`
	// Amazon Resource Name (ARN) of the role to assume
	AssumeRoleArn string `json:"assumeRoleArn"`
	// External ID to use when assuming role
	AssumeRoleExternalID *string `json:"assumeRoleExternalId,omitempty"`
	// Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
	DurationSeconds *float64 `default:"3600" json:"durationSeconds"`
	// Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant stable storage.
	StagePath *string `default:"\\$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Append output's ID to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Object ACL to assign to uploaded objects.
	ObjectACL *ObjectACLSecurityLake `default:"private" json:"objectACL"`
	// Storage class to select for uploaded objects.
	StorageClass *StorageClassSecurityLake `json:"storageClass,omitempty"`
	// Server-side encryption for uploaded objects.
	ServerSideEncryption *ServerSideEncryptionSecurityLake `json:"serverSideEncryption,omitempty"`
	// ID or ARN of the KMS customer-managed key to use for encryption
	KmsKeyID *string `json:"kmsKeyId,omitempty"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// Whether to block or drop events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorSecurityLake `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, dead-letter it to prevent further errors.
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// Whether to block or drop events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionSecurityLake `default:"block" json:"onDiskFullBackpressure"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.
	MaxConcurrentFileParts *float64 `default:"4" json:"maxConcurrentFileParts"`
	// Disable if you can access files within the bucket but not the bucket itself.
	VerifyPermissions *bool `default:"true" json:"verifyPermissions"`
	// Maximum number of files that can be waiting for upload before backpressure is applied
	MaxClosingFilesToBackpressure *float64 `default:"100" json:"maxClosingFilesToBackpressure"`
	// ID of the AWS account whose data the Destination will write to Security Lake. This should have been configured when creating the Amazon Security Lake custom source.
	AccountID string `json:"accountId"`
	// Name of the custom source configured in Amazon Security Lake
	CustomSource string `json:"customSource"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionSecurityLake `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *DataPageVersionSecurityLake `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging, e.g., "key":"OCSF Event Class", "value":"9001".
	KeyValueMetadata []KeyValueMetadatumSecurityLake `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool   `default:"false" json:"enablePageChecksum"`
	Description        *string `json:"description,omitempty"`
	// This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)
	AwsAPIKey *string `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// How frequently, in seconds, to clean up empty directories when 'Remove empty staging dirs' is enabled
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
	ParquetSchema *string `json:"parquetSchema,omitempty"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"\\$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64  `default:"20" json:"maxRetryNum"`
	Status      *TFStatus `json:"status,omitempty"`
}

func (o OutputSecurityLake) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSecurityLake) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputSecurityLake) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputSecurityLake) GetType() *OutputTypeSecurityLake {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputSecurityLake) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSecurityLake) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSecurityLake) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSecurityLake) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSecurityLake) GetBucket() string {
	if o == nil {
		return ""
	}
	return o.Bucket
}

func (o *OutputSecurityLake) GetRegion() string {
	if o == nil {
		return ""
	}
	return o.Region
}

func (o *OutputSecurityLake) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputSecurityLake) GetAwsAuthenticationMethod() *OutputAuthenticationMethodSecurityLake {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputSecurityLake) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputSecurityLake) GetSignatureVersion() *OutputSignatureVersionSecurityLake {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputSecurityLake) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputSecurityLake) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputSecurityLake) GetEnableAssumeRole() *bool {
	if o == nil {
		return nil
	}
	return o.EnableAssumeRole
}

func (o *OutputSecurityLake) GetAssumeRoleArn() string {
	if o == nil {
		return ""
	}
	return o.AssumeRoleArn
}

func (o *OutputSecurityLake) GetAssumeRoleExternalID() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleExternalID
}

func (o *OutputSecurityLake) GetDurationSeconds() *float64 {
	if o == nil {
		return nil
	}
	return o.DurationSeconds
}

func (o *OutputSecurityLake) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputSecurityLake) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputSecurityLake) GetObjectACL() *ObjectACLSecurityLake {
	if o == nil {
		return nil
	}
	return o.ObjectACL
}

func (o *OutputSecurityLake) GetStorageClass() *StorageClassSecurityLake {
	if o == nil {
		return nil
	}
	return o.StorageClass
}

func (o *OutputSecurityLake) GetServerSideEncryption() *ServerSideEncryptionSecurityLake {
	if o == nil {
		return nil
	}
	return o.ServerSideEncryption
}

func (o *OutputSecurityLake) GetKmsKeyID() *string {
	if o == nil {
		return nil
	}
	return o.KmsKeyID
}

func (o *OutputSecurityLake) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputSecurityLake) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputSecurityLake) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputSecurityLake) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputSecurityLake) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputSecurityLake) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputSecurityLake) GetOnBackpressure() *BackpressureBehaviorSecurityLake {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSecurityLake) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputSecurityLake) GetOnDiskFullBackpressure() *DiskSpaceProtectionSecurityLake {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputSecurityLake) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputSecurityLake) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputSecurityLake) GetMaxConcurrentFileParts() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentFileParts
}

func (o *OutputSecurityLake) GetVerifyPermissions() *bool {
	if o == nil {
		return nil
	}
	return o.VerifyPermissions
}

func (o *OutputSecurityLake) GetMaxClosingFilesToBackpressure() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxClosingFilesToBackpressure
}

func (o *OutputSecurityLake) GetAccountID() string {
	if o == nil {
		return ""
	}
	return o.AccountID
}

func (o *OutputSecurityLake) GetCustomSource() string {
	if o == nil {
		return ""
	}
	return o.CustomSource
}

func (o *OutputSecurityLake) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputSecurityLake) GetParquetVersion() *ParquetVersionSecurityLake {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputSecurityLake) GetParquetDataPageVersion() *DataPageVersionSecurityLake {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputSecurityLake) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputSecurityLake) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputSecurityLake) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputSecurityLake) GetKeyValueMetadata() []KeyValueMetadatumSecurityLake {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputSecurityLake) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputSecurityLake) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputSecurityLake) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputSecurityLake) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSecurityLake) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputSecurityLake) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputSecurityLake) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputSecurityLake) GetParquetSchema() *string {
	if o == nil {
		return nil
	}
	return o.ParquetSchema
}

func (o *OutputSecurityLake) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputSecurityLake) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

func (o *OutputSecurityLake) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeDlS3 string

const (
	TypeDlS3DlS3 TypeDlS3 = "dl_s3"
)

func (e TypeDlS3) ToPointer() *TypeDlS3 {
	return &e
}
func (e *TypeDlS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "dl_s3":
		*e = TypeDlS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeDlS3: %v", v)
	}
}

// AuthenticationMethodDlS3 - AWS authentication method. Choose Auto to use IAM roles.
type AuthenticationMethodDlS3 string

const (
	AuthenticationMethodDlS3Auto   AuthenticationMethodDlS3 = "auto"
	AuthenticationMethodDlS3Manual AuthenticationMethodDlS3 = "manual"
	AuthenticationMethodDlS3Secret AuthenticationMethodDlS3 = "secret"
)

func (e AuthenticationMethodDlS3) ToPointer() *AuthenticationMethodDlS3 {
	return &e
}
func (e *AuthenticationMethodDlS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "auto":
		fallthrough
	case "manual":
		fallthrough
	case "secret":
		*e = AuthenticationMethodDlS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodDlS3: %v", v)
	}
}

// SignatureVersionDlS3 - Signature version to use for signing S3 requests
type SignatureVersionDlS3 string

const (
	SignatureVersionDlS3V2 SignatureVersionDlS3 = "v2"
	SignatureVersionDlS3V4 SignatureVersionDlS3 = "v4"
)

func (e SignatureVersionDlS3) ToPointer() *SignatureVersionDlS3 {
	return &e
}
func (e *SignatureVersionDlS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "v2":
		fallthrough
	case "v4":
		*e = SignatureVersionDlS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SignatureVersionDlS3: %v", v)
	}
}

// ObjectACLDlS3 - Object ACL to assign to uploaded objects.
type ObjectACLDlS3 string

const (
	ObjectACLDlS3Private                ObjectACLDlS3 = "private"
	ObjectACLDlS3PublicRead             ObjectACLDlS3 = "public-read"
	ObjectACLDlS3PublicReadWrite        ObjectACLDlS3 = "public-read-write"
	ObjectACLDlS3AuthenticatedRead      ObjectACLDlS3 = "authenticated-read"
	ObjectACLDlS3AwsExecRead            ObjectACLDlS3 = "aws-exec-read"
	ObjectACLDlS3BucketOwnerRead        ObjectACLDlS3 = "bucket-owner-read"
	ObjectACLDlS3BucketOwnerFullControl ObjectACLDlS3 = "bucket-owner-full-control"
)

func (e ObjectACLDlS3) ToPointer() *ObjectACLDlS3 {
	return &e
}
func (e *ObjectACLDlS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "private":
		fallthrough
	case "public-read":
		fallthrough
	case "public-read-write":
		fallthrough
	case "authenticated-read":
		fallthrough
	case "aws-exec-read":
		fallthrough
	case "bucket-owner-read":
		fallthrough
	case "bucket-owner-full-control":
		*e = ObjectACLDlS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ObjectACLDlS3: %v", v)
	}
}

// StorageClassDlS3 - Storage class to select for uploaded objects.
type StorageClassDlS3 string

const (
	StorageClassDlS3Standard           StorageClassDlS3 = "STANDARD"
	StorageClassDlS3ReducedRedundancy  StorageClassDlS3 = "REDUCED_REDUNDANCY"
	StorageClassDlS3StandardIa         StorageClassDlS3 = "STANDARD_IA"
	StorageClassDlS3OnezoneIa          StorageClassDlS3 = "ONEZONE_IA"
	StorageClassDlS3IntelligentTiering StorageClassDlS3 = "INTELLIGENT_TIERING"
	StorageClassDlS3Glacier            StorageClassDlS3 = "GLACIER"
	StorageClassDlS3GlacierIr          StorageClassDlS3 = "GLACIER_IR"
	StorageClassDlS3DeepArchive        StorageClassDlS3 = "DEEP_ARCHIVE"
)

func (e StorageClassDlS3) ToPointer() *StorageClassDlS3 {
	return &e
}
func (e *StorageClassDlS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "STANDARD":
		fallthrough
	case "REDUCED_REDUNDANCY":
		fallthrough
	case "STANDARD_IA":
		fallthrough
	case "ONEZONE_IA":
		fallthrough
	case "INTELLIGENT_TIERING":
		fallthrough
	case "GLACIER":
		fallthrough
	case "GLACIER_IR":
		fallthrough
	case "DEEP_ARCHIVE":
		*e = StorageClassDlS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for StorageClassDlS3: %v", v)
	}
}

// ServerSideEncryptionDlS3 - Server-side encryption for uploaded objects.
type ServerSideEncryptionDlS3 string

const (
	ServerSideEncryptionDlS3Aes256 ServerSideEncryptionDlS3 = "AES256"
	ServerSideEncryptionDlS3AwsKms ServerSideEncryptionDlS3 = "aws:kms"
)

func (e ServerSideEncryptionDlS3) ToPointer() *ServerSideEncryptionDlS3 {
	return &e
}
func (e *ServerSideEncryptionDlS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "AES256":
		fallthrough
	case "aws:kms":
		*e = ServerSideEncryptionDlS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ServerSideEncryptionDlS3: %v", v)
	}
}

// DataFormatDlS3 - Format of the output data
type DataFormatDlS3 string

const (
	DataFormatDlS3JSON    DataFormatDlS3 = "json"
	DataFormatDlS3Raw     DataFormatDlS3 = "raw"
	DataFormatDlS3Parquet DataFormatDlS3 = "parquet"
)

func (e DataFormatDlS3) ToPointer() *DataFormatDlS3 {
	return &e
}
func (e *DataFormatDlS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "json":
		fallthrough
	case "raw":
		fallthrough
	case "parquet":
		*e = DataFormatDlS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DataFormatDlS3: %v", v)
	}
}

// BackpressureBehaviorDlS3 - Whether to block or drop events when all receivers are exerting backpressure
type BackpressureBehaviorDlS3 string

const (
	BackpressureBehaviorDlS3Block BackpressureBehaviorDlS3 = "block"
	BackpressureBehaviorDlS3Drop  BackpressureBehaviorDlS3 = "drop"
)

func (e BackpressureBehaviorDlS3) ToPointer() *BackpressureBehaviorDlS3 {
	return &e
}
func (e *BackpressureBehaviorDlS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = BackpressureBehaviorDlS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorDlS3: %v", v)
	}
}

// DiskSpaceProtectionDlS3 - Whether to block or drop events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionDlS3 string

const (
	DiskSpaceProtectionDlS3Block DiskSpaceProtectionDlS3 = "block"
	DiskSpaceProtectionDlS3Drop  DiskSpaceProtectionDlS3 = "drop"
)

func (e DiskSpaceProtectionDlS3) ToPointer() *DiskSpaceProtectionDlS3 {
	return &e
}
func (e *DiskSpaceProtectionDlS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = DiskSpaceProtectionDlS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DiskSpaceProtectionDlS3: %v", v)
	}
}

// CompressDlS3 - Choose data compression format to apply before moving files to final destination
type CompressDlS3 string

const (
	CompressDlS3None CompressDlS3 = "none"
	CompressDlS3Gzip CompressDlS3 = "gzip"
)

func (e CompressDlS3) ToPointer() *CompressDlS3 {
	return &e
}
func (e *CompressDlS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressDlS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressDlS3: %v", v)
	}
}

// CompressionLevelDlS3 - Compression level to apply before moving files to final destination
type CompressionLevelDlS3 string

const (
	CompressionLevelDlS3BestSpeed       CompressionLevelDlS3 = "best_speed"
	CompressionLevelDlS3Normal          CompressionLevelDlS3 = "normal"
	CompressionLevelDlS3BestCompression CompressionLevelDlS3 = "best_compression"
)

func (e CompressionLevelDlS3) ToPointer() *CompressionLevelDlS3 {
	return &e
}
func (e *CompressionLevelDlS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "best_speed":
		fallthrough
	case "normal":
		fallthrough
	case "best_compression":
		*e = CompressionLevelDlS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionLevelDlS3: %v", v)
	}
}

// ParquetVersionDlS3 - Determines which data types are supported and how they are represented
type ParquetVersionDlS3 string

const (
	ParquetVersionDlS3Parquet10 ParquetVersionDlS3 = "PARQUET_1_0"
	ParquetVersionDlS3Parquet24 ParquetVersionDlS3 = "PARQUET_2_4"
	ParquetVersionDlS3Parquet26 ParquetVersionDlS3 = "PARQUET_2_6"
)

func (e ParquetVersionDlS3) ToPointer() *ParquetVersionDlS3 {
	return &e
}
func (e *ParquetVersionDlS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "PARQUET_1_0":
		fallthrough
	case "PARQUET_2_4":
		fallthrough
	case "PARQUET_2_6":
		*e = ParquetVersionDlS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ParquetVersionDlS3: %v", v)
	}
}

// DataPageVersionDlS3 - Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
type DataPageVersionDlS3 string

const (
	DataPageVersionDlS3DataPageV1 DataPageVersionDlS3 = "DATA_PAGE_V1"
	DataPageVersionDlS3DataPageV2 DataPageVersionDlS3 = "DATA_PAGE_V2"
)

func (e DataPageVersionDlS3) ToPointer() *DataPageVersionDlS3 {
	return &e
}
func (e *DataPageVersionDlS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "DATA_PAGE_V1":
		fallthrough
	case "DATA_PAGE_V2":
		*e = DataPageVersionDlS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DataPageVersionDlS3: %v", v)
	}
}

type KeyValueMetadatumDlS3 struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (k KeyValueMetadatumDlS3) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(k, "", false)
}

func (k *KeyValueMetadatumDlS3) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &k, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *KeyValueMetadatumDlS3) GetKey() *string {
	if o == nil {
		return nil
	}
	return o.Key
}

func (o *KeyValueMetadatumDlS3) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OutputDlS3 struct {
	// Unique ID for this output
	ID   *string   `json:"id,omitempty"`
	Type *TypeDlS3 `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. E.g., referencing a Global Variable: `myBucket-${C.vars.myVar}`.
	Bucket string `json:"bucket"`
	// Region where the S3 bucket is located.
	Region *string `json:"region,omitempty"`
	// Secret key. This value can be a constant or a JavaScript expression(e.g., `${C.env.SOME_SECRET}`).
	AwsSecretKey *string `json:"awsSecretKey,omitempty"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *AuthenticationMethodDlS3 `default:"auto" json:"awsAuthenticationMethod"`
	// S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.
	Endpoint *string `json:"endpoint,omitempty"`
	// Signature version to use for signing S3 requests
	SignatureVersion *SignatureVersionDlS3 `default:"v4" json:"signatureVersion"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Use Assume Role credentials to access S3
	EnableAssumeRole *bool `default:"false" json:"enableAssumeRole"`
	// Amazon Resource Name (ARN) of the role to assume
	AssumeRoleArn *string `json:"assumeRoleArn,omitempty"`
	// External ID to use when assuming role
	AssumeRoleExternalID *string `json:"assumeRoleExternalId,omitempty"`
	// Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
	DurationSeconds *float64 `default:"3600" json:"durationSeconds"`
	// Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant stable storage.
	StagePath *string `default:"\\$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Append output's ID to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Prefix to append to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. E.g., referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`.
	DestPath *string `default:"" json:"destPath"`
	// Object ACL to assign to uploaded objects.
	ObjectACL *ObjectACLDlS3 `default:"private" json:"objectACL"`
	// Storage class to select for uploaded objects.
	StorageClass *StorageClassDlS3 `json:"storageClass,omitempty"`
	// Server-side encryption for uploaded objects.
	ServerSideEncryption *ServerSideEncryptionDlS3 `json:"serverSideEncryption,omitempty"`
	// ID or ARN of the KMS customer-managed key to use for encryption
	KmsKeyID *string `json:"kmsKeyId,omitempty"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// Format of the output data
	Format *DataFormatDlS3 `default:"json" json:"format"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// Whether to block or drop events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorDlS3 `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, dead-letter it to prevent further errors.
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// Whether to block or drop events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionDlS3 `default:"block" json:"onDiskFullBackpressure"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.
	MaxConcurrentFileParts *float64 `default:"4" json:"maxConcurrentFileParts"`
	// Disable if you can access files within the bucket but not the bucket itself.
	VerifyPermissions *bool `default:"true" json:"verifyPermissions"`
	// Maximum number of files that can be waiting for upload before backpressure is applied
	MaxClosingFilesToBackpressure *float64 `default:"100" json:"maxClosingFilesToBackpressure"`
	// List of fields to partition the path by, in addition to time, which is included automatically. The effective partition will be YYYY/MM/DD/HH/<list/of/fields>.
	PartitioningFields []string `json:"partitioningFields,omitempty"`
	Description        *string  `json:"description,omitempty"`
	// This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)
	AwsAPIKey *string `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// Choose data compression format to apply before moving files to final destination
	Compress *CompressDlS3 `default:"gzip" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *CompressionLevelDlS3 `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionDlS3 `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *DataPageVersionDlS3 `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging, e.g., "key":"OCSF Event Class", "value":"9001".
	KeyValueMetadata []KeyValueMetadatumDlS3 `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// How frequently, in seconds, to clean up empty directories when 'Remove empty staging dirs' is enabled
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"\\$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64  `default:"20" json:"maxRetryNum"`
	Status      *TFStatus `json:"status,omitempty"`
}

func (o OutputDlS3) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputDlS3) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputDlS3) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputDlS3) GetType() *TypeDlS3 {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputDlS3) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDlS3) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDlS3) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDlS3) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputDlS3) GetBucket() string {
	if o == nil {
		return ""
	}
	return o.Bucket
}

func (o *OutputDlS3) GetRegion() *string {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputDlS3) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputDlS3) GetAwsAuthenticationMethod() *AuthenticationMethodDlS3 {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputDlS3) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputDlS3) GetSignatureVersion() *SignatureVersionDlS3 {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputDlS3) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputDlS3) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputDlS3) GetEnableAssumeRole() *bool {
	if o == nil {
		return nil
	}
	return o.EnableAssumeRole
}

func (o *OutputDlS3) GetAssumeRoleArn() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleArn
}

func (o *OutputDlS3) GetAssumeRoleExternalID() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleExternalID
}

func (o *OutputDlS3) GetDurationSeconds() *float64 {
	if o == nil {
		return nil
	}
	return o.DurationSeconds
}

func (o *OutputDlS3) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputDlS3) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputDlS3) GetDestPath() *string {
	if o == nil {
		return nil
	}
	return o.DestPath
}

func (o *OutputDlS3) GetObjectACL() *ObjectACLDlS3 {
	if o == nil {
		return nil
	}
	return o.ObjectACL
}

func (o *OutputDlS3) GetStorageClass() *StorageClassDlS3 {
	if o == nil {
		return nil
	}
	return o.StorageClass
}

func (o *OutputDlS3) GetServerSideEncryption() *ServerSideEncryptionDlS3 {
	if o == nil {
		return nil
	}
	return o.ServerSideEncryption
}

func (o *OutputDlS3) GetKmsKeyID() *string {
	if o == nil {
		return nil
	}
	return o.KmsKeyID
}

func (o *OutputDlS3) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputDlS3) GetFormat() *DataFormatDlS3 {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputDlS3) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputDlS3) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputDlS3) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputDlS3) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputDlS3) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputDlS3) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputDlS3) GetOnBackpressure() *BackpressureBehaviorDlS3 {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputDlS3) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputDlS3) GetOnDiskFullBackpressure() *DiskSpaceProtectionDlS3 {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputDlS3) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputDlS3) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputDlS3) GetMaxConcurrentFileParts() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentFileParts
}

func (o *OutputDlS3) GetVerifyPermissions() *bool {
	if o == nil {
		return nil
	}
	return o.VerifyPermissions
}

func (o *OutputDlS3) GetMaxClosingFilesToBackpressure() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxClosingFilesToBackpressure
}

func (o *OutputDlS3) GetPartitioningFields() []string {
	if o == nil {
		return nil
	}
	return o.PartitioningFields
}

func (o *OutputDlS3) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputDlS3) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputDlS3) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputDlS3) GetCompress() *CompressDlS3 {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputDlS3) GetCompressionLevel() *CompressionLevelDlS3 {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputDlS3) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputDlS3) GetParquetVersion() *ParquetVersionDlS3 {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputDlS3) GetParquetDataPageVersion() *DataPageVersionDlS3 {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputDlS3) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputDlS3) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputDlS3) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputDlS3) GetKeyValueMetadata() []KeyValueMetadatumDlS3 {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputDlS3) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputDlS3) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputDlS3) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputDlS3) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputDlS3) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputDlS3) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

func (o *OutputDlS3) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeCrowdstrikeNextGenSiem string

const (
	TypeCrowdstrikeNextGenSiemCrowdstrikeNextGenSiem TypeCrowdstrikeNextGenSiem = "crowdstrike_next_gen_siem"
)

func (e TypeCrowdstrikeNextGenSiem) ToPointer() *TypeCrowdstrikeNextGenSiem {
	return &e
}
func (e *TypeCrowdstrikeNextGenSiem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "crowdstrike_next_gen_siem":
		*e = TypeCrowdstrikeNextGenSiem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeCrowdstrikeNextGenSiem: %v", v)
	}
}

type ExtraHTTPHeaderCrowdstrikeNextGenSiem struct {
	// Field name
	Name *string `json:"name,omitempty"`
	// Field value
	Value string `json:"value"`
}

func (o *ExtraHTTPHeaderCrowdstrikeNextGenSiem) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderCrowdstrikeNextGenSiem) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeCrowdstrikeNextGenSiem - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeCrowdstrikeNextGenSiem string

const (
	FailedRequestLoggingModeCrowdstrikeNextGenSiemPayload           FailedRequestLoggingModeCrowdstrikeNextGenSiem = "payload"
	FailedRequestLoggingModeCrowdstrikeNextGenSiemPayloadAndHeaders FailedRequestLoggingModeCrowdstrikeNextGenSiem = "payloadAndHeaders"
	FailedRequestLoggingModeCrowdstrikeNextGenSiemNone              FailedRequestLoggingModeCrowdstrikeNextGenSiem = "none"
)

func (e FailedRequestLoggingModeCrowdstrikeNextGenSiem) ToPointer() *FailedRequestLoggingModeCrowdstrikeNextGenSiem {
	return &e
}
func (e *FailedRequestLoggingModeCrowdstrikeNextGenSiem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeCrowdstrikeNextGenSiem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeCrowdstrikeNextGenSiem: %v", v)
	}
}

// RequestFormatCrowdstrikeNextGenSiem - When set to JSON, the event is automatically formatted with required fields before sending. When set to Raw, only the event's `_raw` value is sent.
type RequestFormatCrowdstrikeNextGenSiem string

const (
	RequestFormatCrowdstrikeNextGenSiemJSON RequestFormatCrowdstrikeNextGenSiem = "JSON"
	RequestFormatCrowdstrikeNextGenSiemRaw  RequestFormatCrowdstrikeNextGenSiem = "raw"
)

func (e RequestFormatCrowdstrikeNextGenSiem) ToPointer() *RequestFormatCrowdstrikeNextGenSiem {
	return &e
}
func (e *RequestFormatCrowdstrikeNextGenSiem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "JSON":
		fallthrough
	case "raw":
		*e = RequestFormatCrowdstrikeNextGenSiem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for RequestFormatCrowdstrikeNextGenSiem: %v", v)
	}
}

// AuthenticationMethodCrowdstrikeNextGenSiem - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type AuthenticationMethodCrowdstrikeNextGenSiem string

const (
	AuthenticationMethodCrowdstrikeNextGenSiemManual AuthenticationMethodCrowdstrikeNextGenSiem = "manual"
	AuthenticationMethodCrowdstrikeNextGenSiemSecret AuthenticationMethodCrowdstrikeNextGenSiem = "secret"
)

func (e AuthenticationMethodCrowdstrikeNextGenSiem) ToPointer() *AuthenticationMethodCrowdstrikeNextGenSiem {
	return &e
}
func (e *AuthenticationMethodCrowdstrikeNextGenSiem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		*e = AuthenticationMethodCrowdstrikeNextGenSiem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodCrowdstrikeNextGenSiem: %v", v)
	}
}

type ResponseRetrySettingCrowdstrikeNextGenSiem struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingCrowdstrikeNextGenSiem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingCrowdstrikeNextGenSiem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingCrowdstrikeNextGenSiem) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingCrowdstrikeNextGenSiem) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingCrowdstrikeNextGenSiem) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingCrowdstrikeNextGenSiem) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsCrowdstrikeNextGenSiem struct {
	// Enable to retry on request timeout
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsCrowdstrikeNextGenSiem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsCrowdstrikeNextGenSiem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsCrowdstrikeNextGenSiem) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsCrowdstrikeNextGenSiem) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsCrowdstrikeNextGenSiem) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsCrowdstrikeNextGenSiem) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorCrowdstrikeNextGenSiem - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorCrowdstrikeNextGenSiem string

const (
	BackpressureBehaviorCrowdstrikeNextGenSiemBlock BackpressureBehaviorCrowdstrikeNextGenSiem = "block"
	BackpressureBehaviorCrowdstrikeNextGenSiemDrop  BackpressureBehaviorCrowdstrikeNextGenSiem = "drop"
	BackpressureBehaviorCrowdstrikeNextGenSiemQueue BackpressureBehaviorCrowdstrikeNextGenSiem = "queue"
)

func (e BackpressureBehaviorCrowdstrikeNextGenSiem) ToPointer() *BackpressureBehaviorCrowdstrikeNextGenSiem {
	return &e
}
func (e *BackpressureBehaviorCrowdstrikeNextGenSiem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorCrowdstrikeNextGenSiem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorCrowdstrikeNextGenSiem: %v", v)
	}
}

// CompressionCrowdstrikeNextGenSiem - Codec to use to compress the persisted data.
type CompressionCrowdstrikeNextGenSiem string

const (
	CompressionCrowdstrikeNextGenSiemNone CompressionCrowdstrikeNextGenSiem = "none"
	CompressionCrowdstrikeNextGenSiemGzip CompressionCrowdstrikeNextGenSiem = "gzip"
)

func (e CompressionCrowdstrikeNextGenSiem) ToPointer() *CompressionCrowdstrikeNextGenSiem {
	return &e
}
func (e *CompressionCrowdstrikeNextGenSiem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionCrowdstrikeNextGenSiem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionCrowdstrikeNextGenSiem: %v", v)
	}
}

// QueueFullBehaviorCrowdstrikeNextGenSiem - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorCrowdstrikeNextGenSiem string

const (
	QueueFullBehaviorCrowdstrikeNextGenSiemBlock QueueFullBehaviorCrowdstrikeNextGenSiem = "block"
	QueueFullBehaviorCrowdstrikeNextGenSiemDrop  QueueFullBehaviorCrowdstrikeNextGenSiem = "drop"
)

func (e QueueFullBehaviorCrowdstrikeNextGenSiem) ToPointer() *QueueFullBehaviorCrowdstrikeNextGenSiem {
	return &e
}
func (e *QueueFullBehaviorCrowdstrikeNextGenSiem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorCrowdstrikeNextGenSiem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorCrowdstrikeNextGenSiem: %v", v)
	}
}

// ModeCrowdstrikeNextGenSiem - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type ModeCrowdstrikeNextGenSiem string

const (
	ModeCrowdstrikeNextGenSiemError        ModeCrowdstrikeNextGenSiem = "error"
	ModeCrowdstrikeNextGenSiemBackpressure ModeCrowdstrikeNextGenSiem = "backpressure"
	ModeCrowdstrikeNextGenSiemAlways       ModeCrowdstrikeNextGenSiem = "always"
)

func (e ModeCrowdstrikeNextGenSiem) ToPointer() *ModeCrowdstrikeNextGenSiem {
	return &e
}
func (e *ModeCrowdstrikeNextGenSiem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeCrowdstrikeNextGenSiem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeCrowdstrikeNextGenSiem: %v", v)
	}
}

type PqControlsCrowdstrikeNextGenSiem struct {
}

type OutputCrowdstrikeNextGenSiem struct {
	// Unique ID for this output
	ID   *string                     `json:"id,omitempty"`
	Type *TypeCrowdstrikeNextGenSiem `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// URL provided from a CrowdStrike data connector, e.g. https://<your-api-key>.ingest.<your-region>.crowdstrike.com/services/collector
	URL string `json:"url"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Defaults to Yes. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max body size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events.
	ExtraHTTPHeaders []ExtraHTTPHeaderCrowdstrikeNextGenSiem `json:"extraHttpHeaders,omitempty"`
	// Enables round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"true" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeCrowdstrikeNextGenSiem `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// When set to JSON, the event is automatically formatted with required fields before sending. When set to Raw, only the event's `_raw` value is sent.
	Format *RequestFormatCrowdstrikeNextGenSiem `default:"raw" json:"format"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType *AuthenticationMethodCrowdstrikeNextGenSiem `default:"manual" json:"authType"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable).
	ResponseRetrySettings []ResponseRetrySettingCrowdstrikeNextGenSiem `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsCrowdstrikeNextGenSiem  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorCrowdstrikeNextGenSiem `default:"block" json:"onBackpressure"`
	Description    *string                                     `json:"description,omitempty"`
	// CrowdStrike Next-Gen SIEM authentication token
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *CompressionCrowdstrikeNextGenSiem `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorCrowdstrikeNextGenSiem `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeCrowdstrikeNextGenSiem       `default:"error" json:"pqMode"`
	PqControls *PqControlsCrowdstrikeNextGenSiem `json:"pqControls,omitempty"`
	Status     *TFStatus                         `json:"status,omitempty"`
}

func (o OutputCrowdstrikeNextGenSiem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputCrowdstrikeNextGenSiem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputCrowdstrikeNextGenSiem) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputCrowdstrikeNextGenSiem) GetType() *TypeCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputCrowdstrikeNextGenSiem) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputCrowdstrikeNextGenSiem) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputCrowdstrikeNextGenSiem) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputCrowdstrikeNextGenSiem) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputCrowdstrikeNextGenSiem) GetURL() string {
	if o == nil {
		return ""
	}
	return o.URL
}

func (o *OutputCrowdstrikeNextGenSiem) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputCrowdstrikeNextGenSiem) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputCrowdstrikeNextGenSiem) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputCrowdstrikeNextGenSiem) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputCrowdstrikeNextGenSiem) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputCrowdstrikeNextGenSiem) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputCrowdstrikeNextGenSiem) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputCrowdstrikeNextGenSiem) GetExtraHTTPHeaders() []ExtraHTTPHeaderCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputCrowdstrikeNextGenSiem) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputCrowdstrikeNextGenSiem) GetFailedRequestLoggingMode() *FailedRequestLoggingModeCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputCrowdstrikeNextGenSiem) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputCrowdstrikeNextGenSiem) GetFormat() *RequestFormatCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputCrowdstrikeNextGenSiem) GetAuthType() *AuthenticationMethodCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputCrowdstrikeNextGenSiem) GetResponseRetrySettings() []ResponseRetrySettingCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputCrowdstrikeNextGenSiem) GetTimeoutRetrySettings() *TimeoutRetrySettingsCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputCrowdstrikeNextGenSiem) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputCrowdstrikeNextGenSiem) GetOnBackpressure() *BackpressureBehaviorCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputCrowdstrikeNextGenSiem) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputCrowdstrikeNextGenSiem) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputCrowdstrikeNextGenSiem) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqCompress() *CompressionCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqOnBackpressure() *QueueFullBehaviorCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqMode() *ModeCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputCrowdstrikeNextGenSiem) GetPqControls() *PqControlsCrowdstrikeNextGenSiem {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputCrowdstrikeNextGenSiem) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeHumioHec string

const (
	TypeHumioHecHumioHec TypeHumioHec = "humio_hec"
)

func (e TypeHumioHec) ToPointer() *TypeHumioHec {
	return &e
}
func (e *TypeHumioHec) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "humio_hec":
		*e = TypeHumioHec(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeHumioHec: %v", v)
	}
}

type ExtraHTTPHeaderHumioHec struct {
	// Field name
	Name *string `json:"name,omitempty"`
	// Field value
	Value string `json:"value"`
}

func (o *ExtraHTTPHeaderHumioHec) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderHumioHec) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeHumioHec - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeHumioHec string

const (
	FailedRequestLoggingModeHumioHecPayload           FailedRequestLoggingModeHumioHec = "payload"
	FailedRequestLoggingModeHumioHecPayloadAndHeaders FailedRequestLoggingModeHumioHec = "payloadAndHeaders"
	FailedRequestLoggingModeHumioHecNone              FailedRequestLoggingModeHumioHec = "none"
)

func (e FailedRequestLoggingModeHumioHec) ToPointer() *FailedRequestLoggingModeHumioHec {
	return &e
}
func (e *FailedRequestLoggingModeHumioHec) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeHumioHec(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeHumioHec: %v", v)
	}
}

// RequestFormatHumioHec - When set to JSON, the event is automatically formatted with required fields before sending. When set to Raw, only the event's `_raw` value is sent.
type RequestFormatHumioHec string

const (
	RequestFormatHumioHecJSON RequestFormatHumioHec = "JSON"
	RequestFormatHumioHecRaw  RequestFormatHumioHec = "raw"
)

func (e RequestFormatHumioHec) ToPointer() *RequestFormatHumioHec {
	return &e
}
func (e *RequestFormatHumioHec) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "JSON":
		fallthrough
	case "raw":
		*e = RequestFormatHumioHec(v)
		return nil
	default:
		return fmt.Errorf("invalid value for RequestFormatHumioHec: %v", v)
	}
}

// AuthenticationMethodHumioHec - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type AuthenticationMethodHumioHec string

const (
	AuthenticationMethodHumioHecManual AuthenticationMethodHumioHec = "manual"
	AuthenticationMethodHumioHecSecret AuthenticationMethodHumioHec = "secret"
)

func (e AuthenticationMethodHumioHec) ToPointer() *AuthenticationMethodHumioHec {
	return &e
}
func (e *AuthenticationMethodHumioHec) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		*e = AuthenticationMethodHumioHec(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodHumioHec: %v", v)
	}
}

type ResponseRetrySettingHumioHec struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingHumioHec) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingHumioHec) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingHumioHec) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingHumioHec) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingHumioHec) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingHumioHec) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsHumioHec struct {
	// Enable to retry on request timeout
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsHumioHec) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsHumioHec) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsHumioHec) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsHumioHec) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsHumioHec) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsHumioHec) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorHumioHec - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorHumioHec string

const (
	BackpressureBehaviorHumioHecBlock BackpressureBehaviorHumioHec = "block"
	BackpressureBehaviorHumioHecDrop  BackpressureBehaviorHumioHec = "drop"
	BackpressureBehaviorHumioHecQueue BackpressureBehaviorHumioHec = "queue"
)

func (e BackpressureBehaviorHumioHec) ToPointer() *BackpressureBehaviorHumioHec {
	return &e
}
func (e *BackpressureBehaviorHumioHec) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorHumioHec(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorHumioHec: %v", v)
	}
}

// CompressionHumioHec - Codec to use to compress the persisted data.
type CompressionHumioHec string

const (
	CompressionHumioHecNone CompressionHumioHec = "none"
	CompressionHumioHecGzip CompressionHumioHec = "gzip"
)

func (e CompressionHumioHec) ToPointer() *CompressionHumioHec {
	return &e
}
func (e *CompressionHumioHec) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionHumioHec(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionHumioHec: %v", v)
	}
}

// QueueFullBehaviorHumioHec - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorHumioHec string

const (
	QueueFullBehaviorHumioHecBlock QueueFullBehaviorHumioHec = "block"
	QueueFullBehaviorHumioHecDrop  QueueFullBehaviorHumioHec = "drop"
)

func (e QueueFullBehaviorHumioHec) ToPointer() *QueueFullBehaviorHumioHec {
	return &e
}
func (e *QueueFullBehaviorHumioHec) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorHumioHec(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorHumioHec: %v", v)
	}
}

// ModeHumioHec - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type ModeHumioHec string

const (
	ModeHumioHecError        ModeHumioHec = "error"
	ModeHumioHecBackpressure ModeHumioHec = "backpressure"
	ModeHumioHecAlways       ModeHumioHec = "always"
)

func (e ModeHumioHec) ToPointer() *ModeHumioHec {
	return &e
}
func (e *ModeHumioHec) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeHumioHec(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeHumioHec: %v", v)
	}
}

type PqControlsHumioHec struct {
}

type OutputHumioHec struct {
	// Unique ID for this output
	ID   *string       `json:"id,omitempty"`
	Type *TypeHumioHec `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// URL to a CrowdStrike Falcon LogScale endpoint to send events to, e.g., https://cloud.us.humio.com/api/v1/ingest/hec for JSON and https://cloud.us.humio.com/api/v1/ingest/hec/raw for raw
	URL *string `default:"https://cloud.us.humio.com/api/v1/ingest/hec" json:"url"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Defaults to Yes. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max body size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events.
	ExtraHTTPHeaders []ExtraHTTPHeaderHumioHec `json:"extraHttpHeaders,omitempty"`
	// Enables round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"true" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeHumioHec `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// When set to JSON, the event is automatically formatted with required fields before sending. When set to Raw, only the event's `_raw` value is sent.
	Format *RequestFormatHumioHec `default:"JSON" json:"format"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType *AuthenticationMethodHumioHec `default:"manual" json:"authType"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable).
	ResponseRetrySettings []ResponseRetrySettingHumioHec `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsHumioHec  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorHumioHec `default:"block" json:"onBackpressure"`
	Description    *string                       `json:"description,omitempty"`
	// CrowdStrike Falcon LogScale authentication token
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *CompressionHumioHec `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorHumioHec `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeHumioHec       `default:"error" json:"pqMode"`
	PqControls *PqControlsHumioHec `json:"pqControls,omitempty"`
	Status     *TFStatus           `json:"status,omitempty"`
}

func (o OutputHumioHec) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputHumioHec) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputHumioHec) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputHumioHec) GetType() *TypeHumioHec {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputHumioHec) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputHumioHec) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputHumioHec) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputHumioHec) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputHumioHec) GetURL() *string {
	if o == nil {
		return nil
	}
	return o.URL
}

func (o *OutputHumioHec) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputHumioHec) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputHumioHec) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputHumioHec) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputHumioHec) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputHumioHec) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputHumioHec) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputHumioHec) GetExtraHTTPHeaders() []ExtraHTTPHeaderHumioHec {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputHumioHec) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputHumioHec) GetFailedRequestLoggingMode() *FailedRequestLoggingModeHumioHec {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputHumioHec) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputHumioHec) GetFormat() *RequestFormatHumioHec {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputHumioHec) GetAuthType() *AuthenticationMethodHumioHec {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputHumioHec) GetResponseRetrySettings() []ResponseRetrySettingHumioHec {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputHumioHec) GetTimeoutRetrySettings() *TimeoutRetrySettingsHumioHec {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputHumioHec) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputHumioHec) GetOnBackpressure() *BackpressureBehaviorHumioHec {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputHumioHec) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputHumioHec) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputHumioHec) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputHumioHec) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputHumioHec) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputHumioHec) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputHumioHec) GetPqCompress() *CompressionHumioHec {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputHumioHec) GetPqOnBackpressure() *QueueFullBehaviorHumioHec {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputHumioHec) GetPqMode() *ModeHumioHec {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputHumioHec) GetPqControls() *PqControlsHumioHec {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputHumioHec) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type OutputTypeCriblHTTP string

const (
	OutputTypeCriblHTTPCriblHTTP OutputTypeCriblHTTP = "cribl_http"
)

func (e OutputTypeCriblHTTP) ToPointer() *OutputTypeCriblHTTP {
	return &e
}
func (e *OutputTypeCriblHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "cribl_http":
		*e = OutputTypeCriblHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeCriblHTTP: %v", v)
	}
}

// OutputMinimumTLSVersionCriblHTTP - Minimum TLS version to use when connecting
type OutputMinimumTLSVersionCriblHTTP string

const (
	OutputMinimumTLSVersionCriblHTTPTlSv1  OutputMinimumTLSVersionCriblHTTP = "TLSv1"
	OutputMinimumTLSVersionCriblHTTPTlSv11 OutputMinimumTLSVersionCriblHTTP = "TLSv1.1"
	OutputMinimumTLSVersionCriblHTTPTlSv12 OutputMinimumTLSVersionCriblHTTP = "TLSv1.2"
	OutputMinimumTLSVersionCriblHTTPTlSv13 OutputMinimumTLSVersionCriblHTTP = "TLSv1.3"
)

func (e OutputMinimumTLSVersionCriblHTTP) ToPointer() *OutputMinimumTLSVersionCriblHTTP {
	return &e
}
func (e *OutputMinimumTLSVersionCriblHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = OutputMinimumTLSVersionCriblHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMinimumTLSVersionCriblHTTP: %v", v)
	}
}

// OutputMaximumTLSVersionCriblHTTP - Maximum TLS version to use when connecting
type OutputMaximumTLSVersionCriblHTTP string

const (
	OutputMaximumTLSVersionCriblHTTPTlSv1  OutputMaximumTLSVersionCriblHTTP = "TLSv1"
	OutputMaximumTLSVersionCriblHTTPTlSv11 OutputMaximumTLSVersionCriblHTTP = "TLSv1.1"
	OutputMaximumTLSVersionCriblHTTPTlSv12 OutputMaximumTLSVersionCriblHTTP = "TLSv1.2"
	OutputMaximumTLSVersionCriblHTTPTlSv13 OutputMaximumTLSVersionCriblHTTP = "TLSv1.3"
)

func (e OutputMaximumTLSVersionCriblHTTP) ToPointer() *OutputMaximumTLSVersionCriblHTTP {
	return &e
}
func (e *OutputMaximumTLSVersionCriblHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = OutputMaximumTLSVersionCriblHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMaximumTLSVersionCriblHTTP: %v", v)
	}
}

type TLSSettingsClientSideCriblHTTP struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certs that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (e.g., the system's CA). Defaults to Yes. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate.
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key.
	Passphrase *string `json:"passphrase,omitempty"`
	// Minimum TLS version to use when connecting
	MinVersion *OutputMinimumTLSVersionCriblHTTP `json:"minVersion,omitempty"`
	// Maximum TLS version to use when connecting
	MaxVersion *OutputMaximumTLSVersionCriblHTTP `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideCriblHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideCriblHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TLSSettingsClientSideCriblHTTP) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *TLSSettingsClientSideCriblHTTP) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *TLSSettingsClientSideCriblHTTP) GetServername() *string {
	if o == nil {
		return nil
	}
	return o.Servername
}

func (o *TLSSettingsClientSideCriblHTTP) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

func (o *TLSSettingsClientSideCriblHTTP) GetCaPath() *string {
	if o == nil {
		return nil
	}
	return o.CaPath
}

func (o *TLSSettingsClientSideCriblHTTP) GetPrivKeyPath() *string {
	if o == nil {
		return nil
	}
	return o.PrivKeyPath
}

func (o *TLSSettingsClientSideCriblHTTP) GetCertPath() *string {
	if o == nil {
		return nil
	}
	return o.CertPath
}

func (o *TLSSettingsClientSideCriblHTTP) GetPassphrase() *string {
	if o == nil {
		return nil
	}
	return o.Passphrase
}

func (o *TLSSettingsClientSideCriblHTTP) GetMinVersion() *OutputMinimumTLSVersionCriblHTTP {
	if o == nil {
		return nil
	}
	return o.MinVersion
}

func (o *TLSSettingsClientSideCriblHTTP) GetMaxVersion() *OutputMaximumTLSVersionCriblHTTP {
	if o == nil {
		return nil
	}
	return o.MaxVersion
}

// OutputCompressionCriblHTTP - Codec to use to compress the data before sending.
type OutputCompressionCriblHTTP string

const (
	OutputCompressionCriblHTTPNone OutputCompressionCriblHTTP = "none"
	OutputCompressionCriblHTTPGzip OutputCompressionCriblHTTP = "gzip"
)

func (e OutputCompressionCriblHTTP) ToPointer() *OutputCompressionCriblHTTP {
	return &e
}
func (e *OutputCompressionCriblHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = OutputCompressionCriblHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputCompressionCriblHTTP: %v", v)
	}
}

type ExtraHTTPHeaderCriblHTTP struct {
	// Field name
	Name *string `json:"name,omitempty"`
	// Field value
	Value string `json:"value"`
}

func (o *ExtraHTTPHeaderCriblHTTP) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderCriblHTTP) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeCriblHTTP - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeCriblHTTP string

const (
	FailedRequestLoggingModeCriblHTTPPayload           FailedRequestLoggingModeCriblHTTP = "payload"
	FailedRequestLoggingModeCriblHTTPPayloadAndHeaders FailedRequestLoggingModeCriblHTTP = "payloadAndHeaders"
	FailedRequestLoggingModeCriblHTTPNone              FailedRequestLoggingModeCriblHTTP = "none"
)

func (e FailedRequestLoggingModeCriblHTTP) ToPointer() *FailedRequestLoggingModeCriblHTTP {
	return &e
}
func (e *FailedRequestLoggingModeCriblHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeCriblHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeCriblHTTP: %v", v)
	}
}

type ResponseRetrySettingCriblHTTP struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingCriblHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingCriblHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingCriblHTTP) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingCriblHTTP) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingCriblHTTP) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingCriblHTTP) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsCriblHTTP struct {
	// Enable to retry on request timeout
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsCriblHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsCriblHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsCriblHTTP) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsCriblHTTP) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsCriblHTTP) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsCriblHTTP) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorCriblHTTP - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorCriblHTTP string

const (
	BackpressureBehaviorCriblHTTPBlock BackpressureBehaviorCriblHTTP = "block"
	BackpressureBehaviorCriblHTTPDrop  BackpressureBehaviorCriblHTTP = "drop"
	BackpressureBehaviorCriblHTTPQueue BackpressureBehaviorCriblHTTP = "queue"
)

func (e BackpressureBehaviorCriblHTTP) ToPointer() *BackpressureBehaviorCriblHTTP {
	return &e
}
func (e *BackpressureBehaviorCriblHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorCriblHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorCriblHTTP: %v", v)
	}
}

type URLCriblHTTP struct {
	// URL of a Cribl Worker to send events to, e.g., http://localhost:10200
	URL string `json:"url"`
	// Assign a weight (>0) to each endpoint to indicate its traffic-handling capability
	Weight *float64 `default:"1" json:"weight"`
}

func (u URLCriblHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(u, "", false)
}

func (u *URLCriblHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &u, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *URLCriblHTTP) GetURL() string {
	if o == nil {
		return ""
	}
	return o.URL
}

func (o *URLCriblHTTP) GetWeight() *float64 {
	if o == nil {
		return nil
	}
	return o.Weight
}

// PqCompressCompressionCriblHTTP - Codec to use to compress the persisted data.
type PqCompressCompressionCriblHTTP string

const (
	PqCompressCompressionCriblHTTPNone PqCompressCompressionCriblHTTP = "none"
	PqCompressCompressionCriblHTTPGzip PqCompressCompressionCriblHTTP = "gzip"
)

func (e PqCompressCompressionCriblHTTP) ToPointer() *PqCompressCompressionCriblHTTP {
	return &e
}
func (e *PqCompressCompressionCriblHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = PqCompressCompressionCriblHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PqCompressCompressionCriblHTTP: %v", v)
	}
}

// QueueFullBehaviorCriblHTTP - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorCriblHTTP string

const (
	QueueFullBehaviorCriblHTTPBlock QueueFullBehaviorCriblHTTP = "block"
	QueueFullBehaviorCriblHTTPDrop  QueueFullBehaviorCriblHTTP = "drop"
)

func (e QueueFullBehaviorCriblHTTP) ToPointer() *QueueFullBehaviorCriblHTTP {
	return &e
}
func (e *QueueFullBehaviorCriblHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorCriblHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorCriblHTTP: %v", v)
	}
}

// OutputModeCriblHTTP - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type OutputModeCriblHTTP string

const (
	OutputModeCriblHTTPError        OutputModeCriblHTTP = "error"
	OutputModeCriblHTTPBackpressure OutputModeCriblHTTP = "backpressure"
	OutputModeCriblHTTPAlways       OutputModeCriblHTTP = "always"
)

func (e OutputModeCriblHTTP) ToPointer() *OutputModeCriblHTTP {
	return &e
}
func (e *OutputModeCriblHTTP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = OutputModeCriblHTTP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputModeCriblHTTP: %v", v)
	}
}

type PqControlsCriblHTTP struct {
}

type OutputCriblHTTP struct {
	Status *TFStatus `json:"status,omitempty"`
	// Unique ID for this output
	ID   string              `json:"id"`
	Type OutputTypeCriblHTTP `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// For optimal performance, enable load balancing even if you have one hostname, as it can expand to multiple IPs. If this setting is disabled, consider enabling round-robin DNS.
	LoadBalanced *bool                           `default:"true" json:"loadBalanced"`
	TLS          *TLSSettingsClientSideCriblHTTP `json:"tls,omitempty"`
	// The number of minutes before the internally generated authentication token expires, valid values between 1 and 60.
	TokenTTLMinutes *float64 `default:"60" json:"tokenTTLMinutes"`
	// Fields to exclude from the event. By default, all internal fields except `__output` are sent. E.g.: `cribl_pipe`, `c*`. Wildcards supported.
	ExcludeFields []string `json:"excludeFields,omitempty"`
	// Codec to use to compress the data before sending.
	Compression *OutputCompressionCriblHTTP `default:"gzip" json:"compression"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Defaults to Yes. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max body size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events.
	ExtraHTTPHeaders []ExtraHTTPHeaderCriblHTTP `json:"extraHttpHeaders,omitempty"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeCriblHTTP `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable).
	ResponseRetrySettings []ResponseRetrySettingCriblHTTP `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsCriblHTTP  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorCriblHTTP `default:"block" json:"onBackpressure"`
	Description    *string                        `json:"description,omitempty"`
	// URL of a Cribl Worker to send events to, e.g., http://localhost:10200
	URL *string `json:"url,omitempty"`
	// Enables round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Exclude all IPs of the current host from the list of any resolved hostnames.
	ExcludeSelf *bool          `default:"false" json:"excludeSelf"`
	Urls        []URLCriblHTTP `json:"urls,omitempty"`
	// Re-resolve any hostnames every this many seconds and pick up destinations from A records.
	DNSResolvePeriodSec *float64 `default:"600" json:"dnsResolvePeriodSec"`
	// How far back in time to keep traffic stats for load balancing purposes.
	LoadBalanceStatsPeriodSec *float64 `default:"300" json:"loadBalanceStatsPeriodSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *PqCompressCompressionCriblHTTP `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorCriblHTTP `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *OutputModeCriblHTTP `default:"error" json:"pqMode"`
	PqControls *PqControlsCriblHTTP `json:"pqControls,omitempty"`
}

func (o OutputCriblHTTP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputCriblHTTP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputCriblHTTP) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

func (o *OutputCriblHTTP) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputCriblHTTP) GetType() OutputTypeCriblHTTP {
	if o == nil {
		return OutputTypeCriblHTTP("")
	}
	return o.Type
}

func (o *OutputCriblHTTP) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputCriblHTTP) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputCriblHTTP) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputCriblHTTP) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputCriblHTTP) GetLoadBalanced() *bool {
	if o == nil {
		return nil
	}
	return o.LoadBalanced
}

func (o *OutputCriblHTTP) GetTLS() *TLSSettingsClientSideCriblHTTP {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputCriblHTTP) GetTokenTTLMinutes() *float64 {
	if o == nil {
		return nil
	}
	return o.TokenTTLMinutes
}

func (o *OutputCriblHTTP) GetExcludeFields() []string {
	if o == nil {
		return nil
	}
	return o.ExcludeFields
}

func (o *OutputCriblHTTP) GetCompression() *OutputCompressionCriblHTTP {
	if o == nil {
		return nil
	}
	return o.Compression
}

func (o *OutputCriblHTTP) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputCriblHTTP) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputCriblHTTP) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputCriblHTTP) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputCriblHTTP) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputCriblHTTP) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputCriblHTTP) GetExtraHTTPHeaders() []ExtraHTTPHeaderCriblHTTP {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputCriblHTTP) GetFailedRequestLoggingMode() *FailedRequestLoggingModeCriblHTTP {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputCriblHTTP) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputCriblHTTP) GetResponseRetrySettings() []ResponseRetrySettingCriblHTTP {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputCriblHTTP) GetTimeoutRetrySettings() *TimeoutRetrySettingsCriblHTTP {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputCriblHTTP) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputCriblHTTP) GetOnBackpressure() *BackpressureBehaviorCriblHTTP {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputCriblHTTP) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputCriblHTTP) GetURL() *string {
	if o == nil {
		return nil
	}
	return o.URL
}

func (o *OutputCriblHTTP) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputCriblHTTP) GetExcludeSelf() *bool {
	if o == nil {
		return nil
	}
	return o.ExcludeSelf
}

func (o *OutputCriblHTTP) GetUrls() []URLCriblHTTP {
	if o == nil {
		return nil
	}
	return o.Urls
}

func (o *OutputCriblHTTP) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputCriblHTTP) GetLoadBalanceStatsPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.LoadBalanceStatsPeriodSec
}

func (o *OutputCriblHTTP) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputCriblHTTP) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputCriblHTTP) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputCriblHTTP) GetPqCompress() *PqCompressCompressionCriblHTTP {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputCriblHTTP) GetPqOnBackpressure() *QueueFullBehaviorCriblHTTP {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputCriblHTTP) GetPqMode() *OutputModeCriblHTTP {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputCriblHTTP) GetPqControls() *PqControlsCriblHTTP {
	if o == nil {
		return nil
	}
	return o.PqControls
}

type OutputTypeCriblTCP string

const (
	OutputTypeCriblTCPCriblTCP OutputTypeCriblTCP = "cribl_tcp"
)

func (e OutputTypeCriblTCP) ToPointer() *OutputTypeCriblTCP {
	return &e
}
func (e *OutputTypeCriblTCP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "cribl_tcp":
		*e = OutputTypeCriblTCP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeCriblTCP: %v", v)
	}
}

// OutputCompressionCriblTCP - Codec to use to compress the data before sending
type OutputCompressionCriblTCP string

const (
	OutputCompressionCriblTCPNone OutputCompressionCriblTCP = "none"
	OutputCompressionCriblTCPGzip OutputCompressionCriblTCP = "gzip"
)

func (e OutputCompressionCriblTCP) ToPointer() *OutputCompressionCriblTCP {
	return &e
}
func (e *OutputCompressionCriblTCP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = OutputCompressionCriblTCP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputCompressionCriblTCP: %v", v)
	}
}

// OutputMinimumTLSVersionCriblTCP - Minimum TLS version to use when connecting
type OutputMinimumTLSVersionCriblTCP string

const (
	OutputMinimumTLSVersionCriblTCPTlSv1  OutputMinimumTLSVersionCriblTCP = "TLSv1"
	OutputMinimumTLSVersionCriblTCPTlSv11 OutputMinimumTLSVersionCriblTCP = "TLSv1.1"
	OutputMinimumTLSVersionCriblTCPTlSv12 OutputMinimumTLSVersionCriblTCP = "TLSv1.2"
	OutputMinimumTLSVersionCriblTCPTlSv13 OutputMinimumTLSVersionCriblTCP = "TLSv1.3"
)

func (e OutputMinimumTLSVersionCriblTCP) ToPointer() *OutputMinimumTLSVersionCriblTCP {
	return &e
}
func (e *OutputMinimumTLSVersionCriblTCP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = OutputMinimumTLSVersionCriblTCP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMinimumTLSVersionCriblTCP: %v", v)
	}
}

// OutputMaximumTLSVersionCriblTCP - Maximum TLS version to use when connecting
type OutputMaximumTLSVersionCriblTCP string

const (
	OutputMaximumTLSVersionCriblTCPTlSv1  OutputMaximumTLSVersionCriblTCP = "TLSv1"
	OutputMaximumTLSVersionCriblTCPTlSv11 OutputMaximumTLSVersionCriblTCP = "TLSv1.1"
	OutputMaximumTLSVersionCriblTCPTlSv12 OutputMaximumTLSVersionCriblTCP = "TLSv1.2"
	OutputMaximumTLSVersionCriblTCPTlSv13 OutputMaximumTLSVersionCriblTCP = "TLSv1.3"
)

func (e OutputMaximumTLSVersionCriblTCP) ToPointer() *OutputMaximumTLSVersionCriblTCP {
	return &e
}
func (e *OutputMaximumTLSVersionCriblTCP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = OutputMaximumTLSVersionCriblTCP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMaximumTLSVersionCriblTCP: %v", v)
	}
}

type TLSSettingsClientSideCriblTCP struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certs that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (e.g., the system's CA). Defaults to Yes. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate.
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key.
	Passphrase *string `json:"passphrase,omitempty"`
	// Minimum TLS version to use when connecting
	MinVersion *OutputMinimumTLSVersionCriblTCP `json:"minVersion,omitempty"`
	// Maximum TLS version to use when connecting
	MaxVersion *OutputMaximumTLSVersionCriblTCP `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideCriblTCP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideCriblTCP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TLSSettingsClientSideCriblTCP) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *TLSSettingsClientSideCriblTCP) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *TLSSettingsClientSideCriblTCP) GetServername() *string {
	if o == nil {
		return nil
	}
	return o.Servername
}

func (o *TLSSettingsClientSideCriblTCP) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

func (o *TLSSettingsClientSideCriblTCP) GetCaPath() *string {
	if o == nil {
		return nil
	}
	return o.CaPath
}

func (o *TLSSettingsClientSideCriblTCP) GetPrivKeyPath() *string {
	if o == nil {
		return nil
	}
	return o.PrivKeyPath
}

func (o *TLSSettingsClientSideCriblTCP) GetCertPath() *string {
	if o == nil {
		return nil
	}
	return o.CertPath
}

func (o *TLSSettingsClientSideCriblTCP) GetPassphrase() *string {
	if o == nil {
		return nil
	}
	return o.Passphrase
}

func (o *TLSSettingsClientSideCriblTCP) GetMinVersion() *OutputMinimumTLSVersionCriblTCP {
	if o == nil {
		return nil
	}
	return o.MinVersion
}

func (o *TLSSettingsClientSideCriblTCP) GetMaxVersion() *OutputMaximumTLSVersionCriblTCP {
	if o == nil {
		return nil
	}
	return o.MaxVersion
}

// BackpressureBehaviorCriblTCP - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorCriblTCP string

const (
	BackpressureBehaviorCriblTCPBlock BackpressureBehaviorCriblTCP = "block"
	BackpressureBehaviorCriblTCPDrop  BackpressureBehaviorCriblTCP = "drop"
	BackpressureBehaviorCriblTCPQueue BackpressureBehaviorCriblTCP = "queue"
)

func (e BackpressureBehaviorCriblTCP) ToPointer() *BackpressureBehaviorCriblTCP {
	return &e
}
func (e *BackpressureBehaviorCriblTCP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorCriblTCP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorCriblTCP: %v", v)
	}
}

// TLSCriblTCP - Whether to inherit TLS configs from group setting or disable TLS.
type TLSCriblTCP string

const (
	TLSCriblTCPInherit TLSCriblTCP = "inherit"
	TLSCriblTCPFalse   TLSCriblTCP = "false"
)

func (e TLSCriblTCP) ToPointer() *TLSCriblTCP {
	return &e
}
func (e *TLSCriblTCP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "inherit":
		fallthrough
	case "false":
		*e = TLSCriblTCP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TLSCriblTCP: %v", v)
	}
}

type HostCriblTCP struct {
	// The hostname of the receiver.
	Host string `json:"host"`
	// The port to connect to on the provided host.
	Port *float64 `default:"10300" json:"port"`
	// Whether to inherit TLS configs from group setting or disable TLS.
	TLS *TLSCriblTCP `default:"inherit" json:"tls"`
	// Servername to use if establishing a TLS connection. If not specified, defaults to connection host (iff not an IP); otherwise, to the global TLS settings.
	Servername *string `json:"servername,omitempty"`
	// Assign a weight (>0) to each endpoint to indicate its traffic-handling capability
	Weight *float64 `default:"1" json:"weight"`
}

func (h HostCriblTCP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(h, "", false)
}

func (h *HostCriblTCP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &h, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *HostCriblTCP) GetHost() string {
	if o == nil {
		return ""
	}
	return o.Host
}

func (o *HostCriblTCP) GetPort() *float64 {
	if o == nil {
		return nil
	}
	return o.Port
}

func (o *HostCriblTCP) GetTLS() *TLSCriblTCP {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *HostCriblTCP) GetServername() *string {
	if o == nil {
		return nil
	}
	return o.Servername
}

func (o *HostCriblTCP) GetWeight() *float64 {
	if o == nil {
		return nil
	}
	return o.Weight
}

// PqCompressCompressionCriblTCP - Codec to use to compress the persisted data.
type PqCompressCompressionCriblTCP string

const (
	PqCompressCompressionCriblTCPNone PqCompressCompressionCriblTCP = "none"
	PqCompressCompressionCriblTCPGzip PqCompressCompressionCriblTCP = "gzip"
)

func (e PqCompressCompressionCriblTCP) ToPointer() *PqCompressCompressionCriblTCP {
	return &e
}
func (e *PqCompressCompressionCriblTCP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = PqCompressCompressionCriblTCP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PqCompressCompressionCriblTCP: %v", v)
	}
}

// QueueFullBehaviorCriblTCP - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorCriblTCP string

const (
	QueueFullBehaviorCriblTCPBlock QueueFullBehaviorCriblTCP = "block"
	QueueFullBehaviorCriblTCPDrop  QueueFullBehaviorCriblTCP = "drop"
)

func (e QueueFullBehaviorCriblTCP) ToPointer() *QueueFullBehaviorCriblTCP {
	return &e
}
func (e *QueueFullBehaviorCriblTCP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorCriblTCP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorCriblTCP: %v", v)
	}
}

// OutputModeCriblTCP - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type OutputModeCriblTCP string

const (
	OutputModeCriblTCPError        OutputModeCriblTCP = "error"
	OutputModeCriblTCPBackpressure OutputModeCriblTCP = "backpressure"
	OutputModeCriblTCPAlways       OutputModeCriblTCP = "always"
)

func (e OutputModeCriblTCP) ToPointer() *OutputModeCriblTCP {
	return &e
}
func (e *OutputModeCriblTCP) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = OutputModeCriblTCP(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputModeCriblTCP: %v", v)
	}
}

type PqControlsCriblTCP struct {
}

type OutputCriblTCP struct {
	// Unique ID for this output
	ID   string             `json:"id"`
	Type OutputTypeCriblTCP `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Use load-balanced destinations
	LoadBalanced *bool `default:"true" json:"loadBalanced"`
	// Codec to use to compress the data before sending
	Compression *OutputCompressionCriblTCP `default:"gzip" json:"compression"`
	// Use to troubleshoot issues with sending data
	LogFailedRequests *bool `default:"false" json:"logFailedRequests"`
	// Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
	ThrottleRatePerSec *string                        `default:"0" json:"throttleRatePerSec"`
	TLS                *TLSSettingsClientSideCriblTCP `json:"tls,omitempty"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead
	WriteTimeout *float64 `default:"60000" json:"writeTimeout"`
	// The number of minutes before the internally generated authentication token expires, valid values between 1 and 60
	TokenTTLMinutes *float64 `default:"60" json:"tokenTTLMinutes"`
	// Fields to exclude from the event. By default, all internal fields except `__output` are sent. E.g.: `cribl_pipe`, `c*`. Wildcards supported.
	ExcludeFields []string `json:"excludeFields,omitempty"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorCriblTCP `default:"block" json:"onBackpressure"`
	Description    *string                       `json:"description,omitempty"`
	// The hostname of the receiver
	Host *string `json:"host,omitempty"`
	// The port to connect to on the provided host
	Port *float64 `default:"10300" json:"port"`
	// Exclude all IPs of the current host from the list of any resolved hostnames.
	ExcludeSelf *bool `default:"false" json:"excludeSelf"`
	// Set of hosts to load-balance data to.
	Hosts []HostCriblTCP `json:"hosts,omitempty"`
	// Re-resolve any hostnames every this many seconds and pick up destinations from A records.
	DNSResolvePeriodSec *float64 `default:"600" json:"dnsResolvePeriodSec"`
	// How far back in time to keep traffic stats for load balancing purposes.
	LoadBalanceStatsPeriodSec *float64 `default:"300" json:"loadBalanceStatsPeriodSec"`
	// Maximum number of concurrent connections (per worker process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.
	MaxConcurrentSenders *float64 `default:"0" json:"maxConcurrentSenders"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *PqCompressCompressionCriblTCP `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorCriblTCP `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *OutputModeCriblTCP `default:"error" json:"pqMode"`
	PqControls *PqControlsCriblTCP `json:"pqControls,omitempty"`
	Status     *TFStatus           `json:"status,omitempty"`
}

func (o OutputCriblTCP) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputCriblTCP) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputCriblTCP) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputCriblTCP) GetType() OutputTypeCriblTCP {
	if o == nil {
		return OutputTypeCriblTCP("")
	}
	return o.Type
}

func (o *OutputCriblTCP) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputCriblTCP) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputCriblTCP) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputCriblTCP) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputCriblTCP) GetLoadBalanced() *bool {
	if o == nil {
		return nil
	}
	return o.LoadBalanced
}

func (o *OutputCriblTCP) GetCompression() *OutputCompressionCriblTCP {
	if o == nil {
		return nil
	}
	return o.Compression
}

func (o *OutputCriblTCP) GetLogFailedRequests() *bool {
	if o == nil {
		return nil
	}
	return o.LogFailedRequests
}

func (o *OutputCriblTCP) GetThrottleRatePerSec() *string {
	if o == nil {
		return nil
	}
	return o.ThrottleRatePerSec
}

func (o *OutputCriblTCP) GetTLS() *TLSSettingsClientSideCriblTCP {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputCriblTCP) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputCriblTCP) GetWriteTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteTimeout
}

func (o *OutputCriblTCP) GetTokenTTLMinutes() *float64 {
	if o == nil {
		return nil
	}
	return o.TokenTTLMinutes
}

func (o *OutputCriblTCP) GetExcludeFields() []string {
	if o == nil {
		return nil
	}
	return o.ExcludeFields
}

func (o *OutputCriblTCP) GetOnBackpressure() *BackpressureBehaviorCriblTCP {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputCriblTCP) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputCriblTCP) GetHost() *string {
	if o == nil {
		return nil
	}
	return o.Host
}

func (o *OutputCriblTCP) GetPort() *float64 {
	if o == nil {
		return nil
	}
	return o.Port
}

func (o *OutputCriblTCP) GetExcludeSelf() *bool {
	if o == nil {
		return nil
	}
	return o.ExcludeSelf
}

func (o *OutputCriblTCP) GetHosts() []HostCriblTCP {
	if o == nil {
		return nil
	}
	return o.Hosts
}

func (o *OutputCriblTCP) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputCriblTCP) GetLoadBalanceStatsPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.LoadBalanceStatsPeriodSec
}

func (o *OutputCriblTCP) GetMaxConcurrentSenders() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentSenders
}

func (o *OutputCriblTCP) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputCriblTCP) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputCriblTCP) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputCriblTCP) GetPqCompress() *PqCompressCompressionCriblTCP {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputCriblTCP) GetPqOnBackpressure() *QueueFullBehaviorCriblTCP {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputCriblTCP) GetPqMode() *OutputModeCriblTCP {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputCriblTCP) GetPqControls() *PqControlsCriblTCP {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputCriblTCP) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type OutputTypeDataset string

const (
	OutputTypeDatasetDataset OutputTypeDataset = "dataset"
)

func (e OutputTypeDataset) ToPointer() *OutputTypeDataset {
	return &e
}
func (e *OutputTypeDataset) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "dataset":
		*e = OutputTypeDataset(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeDataset: %v", v)
	}
}

// DefaultSeveritySeverity - Default value for event severity. If the `sev` or `__severity` fields are set on an event, the first one matching will override this value.
type DefaultSeveritySeverity string

const (
	DefaultSeveritySeverityFinest  DefaultSeveritySeverity = "finest"
	DefaultSeveritySeverityFiner   DefaultSeveritySeverity = "finer"
	DefaultSeveritySeverityFine    DefaultSeveritySeverity = "fine"
	DefaultSeveritySeverityInfo    DefaultSeveritySeverity = "info"
	DefaultSeveritySeverityWarning DefaultSeveritySeverity = "warning"
	DefaultSeveritySeverityError   DefaultSeveritySeverity = "error"
	DefaultSeveritySeverityFatal   DefaultSeveritySeverity = "fatal"
)

func (e DefaultSeveritySeverity) ToPointer() *DefaultSeveritySeverity {
	return &e
}
func (e *DefaultSeveritySeverity) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "finest":
		fallthrough
	case "finer":
		fallthrough
	case "fine":
		fallthrough
	case "info":
		fallthrough
	case "warning":
		fallthrough
	case "error":
		fallthrough
	case "fatal":
		*e = DefaultSeveritySeverity(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DefaultSeveritySeverity: %v", v)
	}
}

type ResponseRetrySettingDataset struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingDataset) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingDataset) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingDataset) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingDataset) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingDataset) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingDataset) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsDataset struct {
	// Enable to retry on request timeout
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsDataset) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsDataset) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsDataset) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsDataset) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsDataset) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsDataset) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// DataSetSite - DataSet site to which events should be sent
type DataSetSite string

const (
	DataSetSiteUs     DataSetSite = "us"
	DataSetSiteEu     DataSetSite = "eu"
	DataSetSiteCustom DataSetSite = "custom"
)

func (e DataSetSite) ToPointer() *DataSetSite {
	return &e
}
func (e *DataSetSite) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "us":
		fallthrough
	case "eu":
		fallthrough
	case "custom":
		*e = DataSetSite(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DataSetSite: %v", v)
	}
}

type ExtraHTTPHeaderDataset struct {
	// Field name
	Name *string `json:"name,omitempty"`
	// Field value
	Value string `json:"value"`
}

func (o *ExtraHTTPHeaderDataset) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderDataset) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeDataset - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeDataset string

const (
	FailedRequestLoggingModeDatasetPayload           FailedRequestLoggingModeDataset = "payload"
	FailedRequestLoggingModeDatasetPayloadAndHeaders FailedRequestLoggingModeDataset = "payloadAndHeaders"
	FailedRequestLoggingModeDatasetNone              FailedRequestLoggingModeDataset = "none"
)

func (e FailedRequestLoggingModeDataset) ToPointer() *FailedRequestLoggingModeDataset {
	return &e
}
func (e *FailedRequestLoggingModeDataset) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeDataset(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeDataset: %v", v)
	}
}

// BackpressureBehaviorDataset - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorDataset string

const (
	BackpressureBehaviorDatasetBlock BackpressureBehaviorDataset = "block"
	BackpressureBehaviorDatasetDrop  BackpressureBehaviorDataset = "drop"
	BackpressureBehaviorDatasetQueue BackpressureBehaviorDataset = "queue"
)

func (e BackpressureBehaviorDataset) ToPointer() *BackpressureBehaviorDataset {
	return &e
}
func (e *BackpressureBehaviorDataset) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorDataset(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorDataset: %v", v)
	}
}

// AuthenticationMethodDataset - Enter API key directly, or select a stored secret
type AuthenticationMethodDataset string

const (
	AuthenticationMethodDatasetManual AuthenticationMethodDataset = "manual"
	AuthenticationMethodDatasetSecret AuthenticationMethodDataset = "secret"
)

func (e AuthenticationMethodDataset) ToPointer() *AuthenticationMethodDataset {
	return &e
}
func (e *AuthenticationMethodDataset) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		*e = AuthenticationMethodDataset(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodDataset: %v", v)
	}
}

// CompressionDataset - Codec to use to compress the persisted data.
type CompressionDataset string

const (
	CompressionDatasetNone CompressionDataset = "none"
	CompressionDatasetGzip CompressionDataset = "gzip"
)

func (e CompressionDataset) ToPointer() *CompressionDataset {
	return &e
}
func (e *CompressionDataset) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionDataset(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionDataset: %v", v)
	}
}

// QueueFullBehaviorDataset - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorDataset string

const (
	QueueFullBehaviorDatasetBlock QueueFullBehaviorDataset = "block"
	QueueFullBehaviorDatasetDrop  QueueFullBehaviorDataset = "drop"
)

func (e QueueFullBehaviorDataset) ToPointer() *QueueFullBehaviorDataset {
	return &e
}
func (e *QueueFullBehaviorDataset) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorDataset(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorDataset: %v", v)
	}
}

// ModeDataset - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type ModeDataset string

const (
	ModeDatasetError        ModeDataset = "error"
	ModeDatasetBackpressure ModeDataset = "backpressure"
	ModeDatasetAlways       ModeDataset = "always"
)

func (e ModeDataset) ToPointer() *ModeDataset {
	return &e
}
func (e *ModeDataset) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeDataset(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeDataset: %v", v)
	}
}

type PqControlsDataset struct {
}

type OutputDataset struct {
	// Unique ID for this output
	ID   string            `json:"id"`
	Type OutputTypeDataset `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Name of the event field that contains the message or attributes to send. If not specified, all of the event's non-internal fields will be sent as attributes.
	MessageField *string `json:"messageField,omitempty"`
	// Fields to exclude from the event if the Message field is either unspecified or refers to an object. Ignored if the Message field is a string. If empty, we send all non-internal fields.
	ExcludeFields []string `json:"excludeFields,omitempty"`
	// Name of the event field that contains the `serverHost` identifier. If not specified, defaults to `cribl_<outputId>`.
	ServerHostField *string `json:"serverHostField,omitempty"`
	// Name of the event field that contains the timestamp. If not specified, defaults to `ts`, `_time`, or `Date.now()`, in that order.
	TimestampField *string `json:"timestampField,omitempty"`
	// Default value for event severity. If the `sev` or `__severity` fields are set on an event, the first one matching will override this value.
	DefaultSeverity *DefaultSeveritySeverity `default:"info" json:"defaultSeverity"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable).
	ResponseRetrySettings []ResponseRetrySettingDataset `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsDataset  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// DataSet site to which events should be sent
	Site *DataSetSite `default:"us" json:"site"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Defaults to Yes. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max body size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events.
	ExtraHTTPHeaders []ExtraHTTPHeaderDataset `json:"extraHttpHeaders,omitempty"`
	// Enables round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeDataset `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorDataset `default:"block" json:"onBackpressure"`
	// Enter API key directly, or select a stored secret
	AuthType *AuthenticationMethodDataset `default:"manual" json:"authType"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	Description        *string  `json:"description,omitempty"`
	CustomURL          *string  `json:"customUrl,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *CompressionDataset `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorDataset `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeDataset       `default:"error" json:"pqMode"`
	PqControls *PqControlsDataset `json:"pqControls,omitempty"`
	// A 'Log Write Access' API key for the DataSet account
	APIKey *string `json:"apiKey,omitempty"`
	// Select or create a stored text secret
	TextSecret *string   `json:"textSecret,omitempty"`
	Status     *TFStatus `json:"status,omitempty"`
}

func (o OutputDataset) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputDataset) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputDataset) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputDataset) GetType() OutputTypeDataset {
	if o == nil {
		return OutputTypeDataset("")
	}
	return o.Type
}

func (o *OutputDataset) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDataset) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDataset) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDataset) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputDataset) GetMessageField() *string {
	if o == nil {
		return nil
	}
	return o.MessageField
}

func (o *OutputDataset) GetExcludeFields() []string {
	if o == nil {
		return nil
	}
	return o.ExcludeFields
}

func (o *OutputDataset) GetServerHostField() *string {
	if o == nil {
		return nil
	}
	return o.ServerHostField
}

func (o *OutputDataset) GetTimestampField() *string {
	if o == nil {
		return nil
	}
	return o.TimestampField
}

func (o *OutputDataset) GetDefaultSeverity() *DefaultSeveritySeverity {
	if o == nil {
		return nil
	}
	return o.DefaultSeverity
}

func (o *OutputDataset) GetResponseRetrySettings() []ResponseRetrySettingDataset {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputDataset) GetTimeoutRetrySettings() *TimeoutRetrySettingsDataset {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputDataset) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputDataset) GetSite() *DataSetSite {
	if o == nil {
		return nil
	}
	return o.Site
}

func (o *OutputDataset) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputDataset) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputDataset) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputDataset) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputDataset) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputDataset) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputDataset) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputDataset) GetExtraHTTPHeaders() []ExtraHTTPHeaderDataset {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputDataset) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputDataset) GetFailedRequestLoggingMode() *FailedRequestLoggingModeDataset {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputDataset) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputDataset) GetOnBackpressure() *BackpressureBehaviorDataset {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputDataset) GetAuthType() *AuthenticationMethodDataset {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputDataset) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputDataset) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputDataset) GetCustomURL() *string {
	if o == nil {
		return nil
	}
	return o.CustomURL
}

func (o *OutputDataset) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputDataset) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputDataset) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputDataset) GetPqCompress() *CompressionDataset {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputDataset) GetPqOnBackpressure() *QueueFullBehaviorDataset {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputDataset) GetPqMode() *ModeDataset {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputDataset) GetPqControls() *PqControlsDataset {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputDataset) GetAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.APIKey
}

func (o *OutputDataset) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputDataset) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeServiceNow string

const (
	TypeServiceNowServiceNow TypeServiceNow = "service_now"
)

func (e TypeServiceNow) ToPointer() *TypeServiceNow {
	return &e
}
func (e *TypeServiceNow) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "service_now":
		*e = TypeServiceNow(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeServiceNow: %v", v)
	}
}

// OTLPVersion131 - The version of OTLP Protobuf definitions to use when structuring data to send
type OTLPVersion131 string

const (
	OTLPVersion131OneDot3Dot1 OTLPVersion131 = "1.3.1"
)

func (e OTLPVersion131) ToPointer() *OTLPVersion131 {
	return &e
}
func (e *OTLPVersion131) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "1.3.1":
		*e = OTLPVersion131(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OTLPVersion131: %v", v)
	}
}

// Protocol131 - Select a transport option for OpenTelemetry
type Protocol131 string

const (
	Protocol131Grpc Protocol131 = "grpc"
	Protocol131HTTP Protocol131 = "http"
)

func (e Protocol131) ToPointer() *Protocol131 {
	return &e
}
func (e *Protocol131) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "grpc":
		fallthrough
	case "http":
		*e = Protocol131(v)
		return nil
	default:
		return fmt.Errorf("invalid value for Protocol131: %v", v)
	}
}

// CompressCompression131 - Type of compression to apply to messages sent to the OpenTelemetry endpoint
type CompressCompression131 string

const (
	CompressCompression131None    CompressCompression131 = "none"
	CompressCompression131Deflate CompressCompression131 = "deflate"
	CompressCompression131Gzip    CompressCompression131 = "gzip"
)

func (e CompressCompression131) ToPointer() *CompressCompression131 {
	return &e
}
func (e *CompressCompression131) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "deflate":
		fallthrough
	case "gzip":
		*e = CompressCompression131(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressCompression131: %v", v)
	}
}

// HTTPCompressCompression131 - Type of compression to apply to messages sent to the OpenTelemetry endpoint
type HTTPCompressCompression131 string

const (
	HTTPCompressCompression131None HTTPCompressCompression131 = "none"
	HTTPCompressCompression131Gzip HTTPCompressCompression131 = "gzip"
)

func (e HTTPCompressCompression131) ToPointer() *HTTPCompressCompression131 {
	return &e
}
func (e *HTTPCompressCompression131) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = HTTPCompressCompression131(v)
		return nil
	default:
		return fmt.Errorf("invalid value for HTTPCompressCompression131: %v", v)
	}
}

type Metadatum131 struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (m Metadatum131) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(m, "", false)
}

func (m *Metadatum131) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &m, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *Metadatum131) GetKey() *string {
	if o == nil {
		return nil
	}
	return o.Key
}

func (o *Metadatum131) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingMode131 - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingMode131 string

const (
	FailedRequestLoggingMode131Payload           FailedRequestLoggingMode131 = "payload"
	FailedRequestLoggingMode131PayloadAndHeaders FailedRequestLoggingMode131 = "payloadAndHeaders"
	FailedRequestLoggingMode131None              FailedRequestLoggingMode131 = "none"
)

func (e FailedRequestLoggingMode131) ToPointer() *FailedRequestLoggingMode131 {
	return &e
}
func (e *FailedRequestLoggingMode131) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingMode131(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingMode131: %v", v)
	}
}

// BackpressureBehavior131 - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehavior131 string

const (
	BackpressureBehavior131Block BackpressureBehavior131 = "block"
	BackpressureBehavior131Drop  BackpressureBehavior131 = "drop"
	BackpressureBehavior131Queue BackpressureBehavior131 = "queue"
)

func (e BackpressureBehavior131) ToPointer() *BackpressureBehavior131 {
	return &e
}
func (e *BackpressureBehavior131) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehavior131(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehavior131: %v", v)
	}
}

type ExtraHTTPHeader131 struct {
	// Field name
	Name *string `json:"name,omitempty"`
	// Field value
	Value string `json:"value"`
}

func (o *ExtraHTTPHeader131) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeader131) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type ResponseRetrySetting131 struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySetting131) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySetting131) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySetting131) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySetting131) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySetting131) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySetting131) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettings131 struct {
	// Enable to retry on request timeout
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettings131) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettings131) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettings131) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettings131) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettings131) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettings131) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// MinimumTLSVersion131 - Minimum TLS version to use when connecting
type MinimumTLSVersion131 string

const (
	MinimumTLSVersion131TlSv1  MinimumTLSVersion131 = "TLSv1"
	MinimumTLSVersion131TlSv11 MinimumTLSVersion131 = "TLSv1.1"
	MinimumTLSVersion131TlSv12 MinimumTLSVersion131 = "TLSv1.2"
	MinimumTLSVersion131TlSv13 MinimumTLSVersion131 = "TLSv1.3"
)

func (e MinimumTLSVersion131) ToPointer() *MinimumTLSVersion131 {
	return &e
}
func (e *MinimumTLSVersion131) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = MinimumTLSVersion131(v)
		return nil
	default:
		return fmt.Errorf("invalid value for MinimumTLSVersion131: %v", v)
	}
}

// MaximumTLSVersion131 - Maximum TLS version to use when connecting
type MaximumTLSVersion131 string

const (
	MaximumTLSVersion131TlSv1  MaximumTLSVersion131 = "TLSv1"
	MaximumTLSVersion131TlSv11 MaximumTLSVersion131 = "TLSv1.1"
	MaximumTLSVersion131TlSv12 MaximumTLSVersion131 = "TLSv1.2"
	MaximumTLSVersion131TlSv13 MaximumTLSVersion131 = "TLSv1.3"
)

func (e MaximumTLSVersion131) ToPointer() *MaximumTLSVersion131 {
	return &e
}
func (e *MaximumTLSVersion131) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = MaximumTLSVersion131(v)
		return nil
	default:
		return fmt.Errorf("invalid value for MaximumTLSVersion131: %v", v)
	}
}

type TLSSettingsClientSide131 struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certs that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (e.g., the system's CA). Defaults to Yes. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// The name of the predefined certificate.
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key.
	Passphrase *string `json:"passphrase,omitempty"`
	// Minimum TLS version to use when connecting
	MinVersion *MinimumTLSVersion131 `json:"minVersion,omitempty"`
	// Maximum TLS version to use when connecting
	MaxVersion *MaximumTLSVersion131 `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSide131) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSide131) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TLSSettingsClientSide131) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *TLSSettingsClientSide131) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *TLSSettingsClientSide131) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

func (o *TLSSettingsClientSide131) GetCaPath() *string {
	if o == nil {
		return nil
	}
	return o.CaPath
}

func (o *TLSSettingsClientSide131) GetPrivKeyPath() *string {
	if o == nil {
		return nil
	}
	return o.PrivKeyPath
}

func (o *TLSSettingsClientSide131) GetCertPath() *string {
	if o == nil {
		return nil
	}
	return o.CertPath
}

func (o *TLSSettingsClientSide131) GetPassphrase() *string {
	if o == nil {
		return nil
	}
	return o.Passphrase
}

func (o *TLSSettingsClientSide131) GetMinVersion() *MinimumTLSVersion131 {
	if o == nil {
		return nil
	}
	return o.MinVersion
}

func (o *TLSSettingsClientSide131) GetMaxVersion() *MaximumTLSVersion131 {
	if o == nil {
		return nil
	}
	return o.MaxVersion
}

// PqCompressCompression131 - Codec to use to compress the persisted data.
type PqCompressCompression131 string

const (
	PqCompressCompression131None PqCompressCompression131 = "none"
	PqCompressCompression131Gzip PqCompressCompression131 = "gzip"
)

func (e PqCompressCompression131) ToPointer() *PqCompressCompression131 {
	return &e
}
func (e *PqCompressCompression131) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = PqCompressCompression131(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PqCompressCompression131: %v", v)
	}
}

// QueueFullBehavior131 - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehavior131 string

const (
	QueueFullBehavior131Block QueueFullBehavior131 = "block"
	QueueFullBehavior131Drop  QueueFullBehavior131 = "drop"
)

func (e QueueFullBehavior131) ToPointer() *QueueFullBehavior131 {
	return &e
}
func (e *QueueFullBehavior131) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehavior131(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehavior131: %v", v)
	}
}

// Mode131 - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type Mode131 string

const (
	Mode131Error        Mode131 = "error"
	Mode131Backpressure Mode131 = "backpressure"
	Mode131Always       Mode131 = "always"
)

func (e Mode131) ToPointer() *Mode131 {
	return &e
}
func (e *Mode131) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = Mode131(v)
		return nil
	default:
		return fmt.Errorf("invalid value for Mode131: %v", v)
	}
}

type PqControls131 struct {
}

type OutputServiceNow struct {
	// Unique ID for this output
	ID   *string         `json:"id,omitempty"`
	Type *TypeServiceNow `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The endpoint where ServiceNow events will be sent. Enter any valid URL or an IP address (IPv4 or IPv6; enclose IPv6 addresses in square brackets)
	Endpoint *string `default:"ingest.lightstep.com:443" json:"endpoint"`
	// Select or create a stored text secret
	TokenSecret   string  `json:"tokenSecret"`
	AuthTokenName *string `default:"lightstep-access-token" json:"authTokenName"`
	// The version of OTLP Protobuf definitions to use when structuring data to send
	OtlpVersion *OTLPVersion131 `default:"1.3.1" json:"otlpVersion"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"2048" json:"maxPayloadSizeKB"`
	// Select a transport option for OpenTelemetry
	Protocol *Protocol131 `default:"grpc" json:"protocol"`
	// Type of compression to apply to messages sent to the OpenTelemetry endpoint
	Compress *CompressCompression131 `default:"gzip" json:"compress"`
	// Type of compression to apply to messages sent to the OpenTelemetry endpoint
	HTTPCompress *HTTPCompressCompression131 `default:"gzip" json:"httpCompress"`
	// If you want to send traces to the default `{endpoint}/v1/traces` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPTracesEndpointOverride *string `json:"httpTracesEndpointOverride,omitempty"`
	// If you want to send metrics to the default `{endpoint}/v1/metrics` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPMetricsEndpointOverride *string `json:"httpMetricsEndpointOverride,omitempty"`
	// If you want to send logs to the default `{endpoint}/v1/logs` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPLogsEndpointOverride *string `json:"httpLogsEndpointOverride,omitempty"`
	// List of key-value pairs to send with each gRPC request. Value supports JavaScript expressions that are evaluated just once, when the destination gets started. To pass credentials as metadata, use 'C.Secret'.
	Metadata []Metadatum131 `json:"metadata,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max body size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingMode131 `default:"none" json:"failedRequestLoggingMode"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// How often the sender should ping the peer to keep the connection open
	KeepAliveTime *float64 `default:"30" json:"keepAliveTime"`
	// Disable to close the connection immediately after sending the outgoing request
	KeepAlive *bool `default:"true" json:"keepAlive"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehavior131 `default:"block" json:"onBackpressure"`
	Description    *string                  `json:"description,omitempty"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Defaults to Yes. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Enables round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Headers to add to all events.
	ExtraHTTPHeaders []ExtraHTTPHeader131 `json:"extraHttpHeaders,omitempty"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable).
	ResponseRetrySettings []ResponseRetrySetting131 `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettings131  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool                     `default:"false" json:"responseHonorRetryAfterHeader"`
	TLS                           *TLSSettingsClientSide131 `json:"tls,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *PqCompressCompression131 `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehavior131 `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *Mode131       `default:"error" json:"pqMode"`
	PqControls *PqControls131 `json:"pqControls,omitempty"`
	Status     *TFStatus      `json:"status,omitempty"`
}

func (o OutputServiceNow) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputServiceNow) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputServiceNow) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputServiceNow) GetType() *TypeServiceNow {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputServiceNow) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputServiceNow) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputServiceNow) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputServiceNow) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputServiceNow) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputServiceNow) GetTokenSecret() string {
	if o == nil {
		return ""
	}
	return o.TokenSecret
}

func (o *OutputServiceNow) GetAuthTokenName() *string {
	if o == nil {
		return nil
	}
	return o.AuthTokenName
}

func (o *OutputServiceNow) GetOtlpVersion() *OTLPVersion131 {
	if o == nil {
		return nil
	}
	return o.OtlpVersion
}

func (o *OutputServiceNow) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputServiceNow) GetProtocol() *Protocol131 {
	if o == nil {
		return nil
	}
	return o.Protocol
}

func (o *OutputServiceNow) GetCompress() *CompressCompression131 {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputServiceNow) GetHTTPCompress() *HTTPCompressCompression131 {
	if o == nil {
		return nil
	}
	return o.HTTPCompress
}

func (o *OutputServiceNow) GetHTTPTracesEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPTracesEndpointOverride
}

func (o *OutputServiceNow) GetHTTPMetricsEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPMetricsEndpointOverride
}

func (o *OutputServiceNow) GetHTTPLogsEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPLogsEndpointOverride
}

func (o *OutputServiceNow) GetMetadata() []Metadatum131 {
	if o == nil {
		return nil
	}
	return o.Metadata
}

func (o *OutputServiceNow) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputServiceNow) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputServiceNow) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputServiceNow) GetFailedRequestLoggingMode() *FailedRequestLoggingMode131 {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputServiceNow) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputServiceNow) GetKeepAliveTime() *float64 {
	if o == nil {
		return nil
	}
	return o.KeepAliveTime
}

func (o *OutputServiceNow) GetKeepAlive() *bool {
	if o == nil {
		return nil
	}
	return o.KeepAlive
}

func (o *OutputServiceNow) GetOnBackpressure() *BackpressureBehavior131 {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputServiceNow) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputServiceNow) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputServiceNow) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputServiceNow) GetExtraHTTPHeaders() []ExtraHTTPHeader131 {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputServiceNow) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputServiceNow) GetResponseRetrySettings() []ResponseRetrySetting131 {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputServiceNow) GetTimeoutRetrySettings() *TimeoutRetrySettings131 {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputServiceNow) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputServiceNow) GetTLS() *TLSSettingsClientSide131 {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputServiceNow) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputServiceNow) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputServiceNow) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputServiceNow) GetPqCompress() *PqCompressCompression131 {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputServiceNow) GetPqOnBackpressure() *QueueFullBehavior131 {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputServiceNow) GetPqMode() *Mode131 {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputServiceNow) GetPqControls() *PqControls131 {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputServiceNow) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type OutputTypeOpenTelemetry string

const (
	OutputTypeOpenTelemetryOpenTelemetry OutputTypeOpenTelemetry = "open_telemetry"
)

func (e OutputTypeOpenTelemetry) ToPointer() *OutputTypeOpenTelemetry {
	return &e
}
func (e *OutputTypeOpenTelemetry) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "open_telemetry":
		*e = OutputTypeOpenTelemetry(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeOpenTelemetry: %v", v)
	}
}

// OutputProtocolOpenTelemetry - Select a transport option for OpenTelemetry
type OutputProtocolOpenTelemetry string

const (
	OutputProtocolOpenTelemetryGrpc OutputProtocolOpenTelemetry = "grpc"
	OutputProtocolOpenTelemetryHTTP OutputProtocolOpenTelemetry = "http"
)

func (e OutputProtocolOpenTelemetry) ToPointer() *OutputProtocolOpenTelemetry {
	return &e
}
func (e *OutputProtocolOpenTelemetry) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "grpc":
		fallthrough
	case "http":
		*e = OutputProtocolOpenTelemetry(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputProtocolOpenTelemetry: %v", v)
	}
}

// OutputOTLPVersionOpenTelemetry - The version of OTLP Protobuf definitions to use when structuring data to send
type OutputOTLPVersionOpenTelemetry string

const (
	OutputOTLPVersionOpenTelemetryZeroDot10Dot0 OutputOTLPVersionOpenTelemetry = "0.10.0"
	OutputOTLPVersionOpenTelemetryOneDot3Dot1   OutputOTLPVersionOpenTelemetry = "1.3.1"
)

func (e OutputOTLPVersionOpenTelemetry) ToPointer() *OutputOTLPVersionOpenTelemetry {
	return &e
}
func (e *OutputOTLPVersionOpenTelemetry) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "0.10.0":
		fallthrough
	case "1.3.1":
		*e = OutputOTLPVersionOpenTelemetry(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputOTLPVersionOpenTelemetry: %v", v)
	}
}

// OutputCompressCompressionOpenTelemetry - Type of compression to apply to messages sent to the OpenTelemetry endpoint
type OutputCompressCompressionOpenTelemetry string

const (
	OutputCompressCompressionOpenTelemetryNone    OutputCompressCompressionOpenTelemetry = "none"
	OutputCompressCompressionOpenTelemetryDeflate OutputCompressCompressionOpenTelemetry = "deflate"
	OutputCompressCompressionOpenTelemetryGzip    OutputCompressCompressionOpenTelemetry = "gzip"
)

func (e OutputCompressCompressionOpenTelemetry) ToPointer() *OutputCompressCompressionOpenTelemetry {
	return &e
}
func (e *OutputCompressCompressionOpenTelemetry) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "deflate":
		fallthrough
	case "gzip":
		*e = OutputCompressCompressionOpenTelemetry(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputCompressCompressionOpenTelemetry: %v", v)
	}
}

// HTTPCompressCompressionOpenTelemetry - Type of compression to apply to messages sent to the OpenTelemetry endpoint
type HTTPCompressCompressionOpenTelemetry string

const (
	HTTPCompressCompressionOpenTelemetryNone HTTPCompressCompressionOpenTelemetry = "none"
	HTTPCompressCompressionOpenTelemetryGzip HTTPCompressCompressionOpenTelemetry = "gzip"
)

func (e HTTPCompressCompressionOpenTelemetry) ToPointer() *HTTPCompressCompressionOpenTelemetry {
	return &e
}
func (e *HTTPCompressCompressionOpenTelemetry) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = HTTPCompressCompressionOpenTelemetry(v)
		return nil
	default:
		return fmt.Errorf("invalid value for HTTPCompressCompressionOpenTelemetry: %v", v)
	}
}

// OutputAuthenticationTypeOpenTelemetry - OpenTelemetry authentication type
type OutputAuthenticationTypeOpenTelemetry string

const (
	OutputAuthenticationTypeOpenTelemetryNone              OutputAuthenticationTypeOpenTelemetry = "none"
	OutputAuthenticationTypeOpenTelemetryBasic             OutputAuthenticationTypeOpenTelemetry = "basic"
	OutputAuthenticationTypeOpenTelemetryCredentialsSecret OutputAuthenticationTypeOpenTelemetry = "credentialsSecret"
	OutputAuthenticationTypeOpenTelemetryToken             OutputAuthenticationTypeOpenTelemetry = "token"
	OutputAuthenticationTypeOpenTelemetryTextSecret        OutputAuthenticationTypeOpenTelemetry = "textSecret"
	OutputAuthenticationTypeOpenTelemetryOauth             OutputAuthenticationTypeOpenTelemetry = "oauth"
)

func (e OutputAuthenticationTypeOpenTelemetry) ToPointer() *OutputAuthenticationTypeOpenTelemetry {
	return &e
}
func (e *OutputAuthenticationTypeOpenTelemetry) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "basic":
		fallthrough
	case "credentialsSecret":
		fallthrough
	case "token":
		fallthrough
	case "textSecret":
		fallthrough
	case "oauth":
		*e = OutputAuthenticationTypeOpenTelemetry(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputAuthenticationTypeOpenTelemetry: %v", v)
	}
}

type OutputMetadatumOpenTelemetry struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (o OutputMetadatumOpenTelemetry) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputMetadatumOpenTelemetry) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *OutputMetadatumOpenTelemetry) GetKey() *string {
	if o == nil {
		return nil
	}
	return o.Key
}

func (o *OutputMetadatumOpenTelemetry) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeOpenTelemetry - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeOpenTelemetry string

const (
	FailedRequestLoggingModeOpenTelemetryPayload           FailedRequestLoggingModeOpenTelemetry = "payload"
	FailedRequestLoggingModeOpenTelemetryPayloadAndHeaders FailedRequestLoggingModeOpenTelemetry = "payloadAndHeaders"
	FailedRequestLoggingModeOpenTelemetryNone              FailedRequestLoggingModeOpenTelemetry = "none"
)

func (e FailedRequestLoggingModeOpenTelemetry) ToPointer() *FailedRequestLoggingModeOpenTelemetry {
	return &e
}
func (e *FailedRequestLoggingModeOpenTelemetry) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeOpenTelemetry(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeOpenTelemetry: %v", v)
	}
}

// BackpressureBehaviorOpenTelemetry - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorOpenTelemetry string

const (
	BackpressureBehaviorOpenTelemetryBlock BackpressureBehaviorOpenTelemetry = "block"
	BackpressureBehaviorOpenTelemetryDrop  BackpressureBehaviorOpenTelemetry = "drop"
	BackpressureBehaviorOpenTelemetryQueue BackpressureBehaviorOpenTelemetry = "queue"
)

func (e BackpressureBehaviorOpenTelemetry) ToPointer() *BackpressureBehaviorOpenTelemetry {
	return &e
}
func (e *BackpressureBehaviorOpenTelemetry) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorOpenTelemetry(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorOpenTelemetry: %v", v)
	}
}

type OutputOauthParamOpenTelemetry struct {
	// OAuth parameter name
	Name string `json:"name"`
	// OAuth parameter value
	Value string `json:"value"`
}

func (o *OutputOauthParamOpenTelemetry) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OutputOauthParamOpenTelemetry) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OutputOauthHeaderOpenTelemetry struct {
	// OAuth header name
	Name string `json:"name"`
	// OAuth header value
	Value string `json:"value"`
}

func (o *OutputOauthHeaderOpenTelemetry) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OutputOauthHeaderOpenTelemetry) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type ExtraHTTPHeaderOpenTelemetry struct {
	// Field name
	Name *string `json:"name,omitempty"`
	// Field value
	Value string `json:"value"`
}

func (o *ExtraHTTPHeaderOpenTelemetry) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderOpenTelemetry) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type ResponseRetrySettingOpenTelemetry struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingOpenTelemetry) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingOpenTelemetry) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingOpenTelemetry) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingOpenTelemetry) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingOpenTelemetry) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingOpenTelemetry) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsOpenTelemetry struct {
	// Enable to retry on request timeout
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsOpenTelemetry) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsOpenTelemetry) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsOpenTelemetry) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsOpenTelemetry) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsOpenTelemetry) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsOpenTelemetry) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// OutputMinimumTLSVersionOpenTelemetry - Minimum TLS version to use when connecting
type OutputMinimumTLSVersionOpenTelemetry string

const (
	OutputMinimumTLSVersionOpenTelemetryTlSv1  OutputMinimumTLSVersionOpenTelemetry = "TLSv1"
	OutputMinimumTLSVersionOpenTelemetryTlSv11 OutputMinimumTLSVersionOpenTelemetry = "TLSv1.1"
	OutputMinimumTLSVersionOpenTelemetryTlSv12 OutputMinimumTLSVersionOpenTelemetry = "TLSv1.2"
	OutputMinimumTLSVersionOpenTelemetryTlSv13 OutputMinimumTLSVersionOpenTelemetry = "TLSv1.3"
)

func (e OutputMinimumTLSVersionOpenTelemetry) ToPointer() *OutputMinimumTLSVersionOpenTelemetry {
	return &e
}
func (e *OutputMinimumTLSVersionOpenTelemetry) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = OutputMinimumTLSVersionOpenTelemetry(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMinimumTLSVersionOpenTelemetry: %v", v)
	}
}

// OutputMaximumTLSVersionOpenTelemetry - Maximum TLS version to use when connecting
type OutputMaximumTLSVersionOpenTelemetry string

const (
	OutputMaximumTLSVersionOpenTelemetryTlSv1  OutputMaximumTLSVersionOpenTelemetry = "TLSv1"
	OutputMaximumTLSVersionOpenTelemetryTlSv11 OutputMaximumTLSVersionOpenTelemetry = "TLSv1.1"
	OutputMaximumTLSVersionOpenTelemetryTlSv12 OutputMaximumTLSVersionOpenTelemetry = "TLSv1.2"
	OutputMaximumTLSVersionOpenTelemetryTlSv13 OutputMaximumTLSVersionOpenTelemetry = "TLSv1.3"
)

func (e OutputMaximumTLSVersionOpenTelemetry) ToPointer() *OutputMaximumTLSVersionOpenTelemetry {
	return &e
}
func (e *OutputMaximumTLSVersionOpenTelemetry) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = OutputMaximumTLSVersionOpenTelemetry(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMaximumTLSVersionOpenTelemetry: %v", v)
	}
}

type TLSSettingsClientSideOpenTelemetry struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certs that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (e.g., the system's CA). Defaults to Yes. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// The name of the predefined certificate.
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key.
	Passphrase *string `json:"passphrase,omitempty"`
	// Minimum TLS version to use when connecting
	MinVersion *OutputMinimumTLSVersionOpenTelemetry `json:"minVersion,omitempty"`
	// Maximum TLS version to use when connecting
	MaxVersion *OutputMaximumTLSVersionOpenTelemetry `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideOpenTelemetry) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideOpenTelemetry) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TLSSettingsClientSideOpenTelemetry) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *TLSSettingsClientSideOpenTelemetry) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *TLSSettingsClientSideOpenTelemetry) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

func (o *TLSSettingsClientSideOpenTelemetry) GetCaPath() *string {
	if o == nil {
		return nil
	}
	return o.CaPath
}

func (o *TLSSettingsClientSideOpenTelemetry) GetPrivKeyPath() *string {
	if o == nil {
		return nil
	}
	return o.PrivKeyPath
}

func (o *TLSSettingsClientSideOpenTelemetry) GetCertPath() *string {
	if o == nil {
		return nil
	}
	return o.CertPath
}

func (o *TLSSettingsClientSideOpenTelemetry) GetPassphrase() *string {
	if o == nil {
		return nil
	}
	return o.Passphrase
}

func (o *TLSSettingsClientSideOpenTelemetry) GetMinVersion() *OutputMinimumTLSVersionOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.MinVersion
}

func (o *TLSSettingsClientSideOpenTelemetry) GetMaxVersion() *OutputMaximumTLSVersionOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.MaxVersion
}

// PqCompressCompressionOpenTelemetry - Codec to use to compress the persisted data.
type PqCompressCompressionOpenTelemetry string

const (
	PqCompressCompressionOpenTelemetryNone PqCompressCompressionOpenTelemetry = "none"
	PqCompressCompressionOpenTelemetryGzip PqCompressCompressionOpenTelemetry = "gzip"
)

func (e PqCompressCompressionOpenTelemetry) ToPointer() *PqCompressCompressionOpenTelemetry {
	return &e
}
func (e *PqCompressCompressionOpenTelemetry) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = PqCompressCompressionOpenTelemetry(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PqCompressCompressionOpenTelemetry: %v", v)
	}
}

// QueueFullBehaviorOpenTelemetry - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorOpenTelemetry string

const (
	QueueFullBehaviorOpenTelemetryBlock QueueFullBehaviorOpenTelemetry = "block"
	QueueFullBehaviorOpenTelemetryDrop  QueueFullBehaviorOpenTelemetry = "drop"
)

func (e QueueFullBehaviorOpenTelemetry) ToPointer() *QueueFullBehaviorOpenTelemetry {
	return &e
}
func (e *QueueFullBehaviorOpenTelemetry) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorOpenTelemetry(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorOpenTelemetry: %v", v)
	}
}

// OutputModeOpenTelemetry - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type OutputModeOpenTelemetry string

const (
	OutputModeOpenTelemetryError        OutputModeOpenTelemetry = "error"
	OutputModeOpenTelemetryBackpressure OutputModeOpenTelemetry = "backpressure"
	OutputModeOpenTelemetryAlways       OutputModeOpenTelemetry = "always"
)

func (e OutputModeOpenTelemetry) ToPointer() *OutputModeOpenTelemetry {
	return &e
}
func (e *OutputModeOpenTelemetry) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = OutputModeOpenTelemetry(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputModeOpenTelemetry: %v", v)
	}
}

type PqControlsOpenTelemetry struct {
}

type OutputOpenTelemetry struct {
	// Unique ID for this output
	ID   *string                 `json:"id,omitempty"`
	Type OutputTypeOpenTelemetry `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Select a transport option for OpenTelemetry
	Protocol *OutputProtocolOpenTelemetry `default:"grpc" json:"protocol"`
	// The endpoint where OTel events will be sent. Enter any valid URL or an IP address (IPv4 or IPv6; enclose IPv6 addresses in square brackets). Unspecified ports will default to 4317, unless the endpoint is an HTTPS-based URL or TLS is enabled, in which case 443 will be used.
	Endpoint string `json:"endpoint"`
	// The version of OTLP Protobuf definitions to use when structuring data to send
	OtlpVersion *OutputOTLPVersionOpenTelemetry `default:"0.10.0" json:"otlpVersion"`
	// Type of compression to apply to messages sent to the OpenTelemetry endpoint
	Compress *OutputCompressCompressionOpenTelemetry `default:"gzip" json:"compress"`
	// Type of compression to apply to messages sent to the OpenTelemetry endpoint
	HTTPCompress *HTTPCompressCompressionOpenTelemetry `default:"gzip" json:"httpCompress"`
	// OpenTelemetry authentication type
	AuthType *OutputAuthenticationTypeOpenTelemetry `default:"none" json:"authType"`
	// If you want to send traces to the default `{endpoint}/v1/traces` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPTracesEndpointOverride *string `json:"httpTracesEndpointOverride,omitempty"`
	// If you want to send metrics to the default `{endpoint}/v1/metrics` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPMetricsEndpointOverride *string `json:"httpMetricsEndpointOverride,omitempty"`
	// If you want to send logs to the default `{endpoint}/v1/logs` endpoint, leave this field empty; otherwise, specify the desired endpoint
	HTTPLogsEndpointOverride *string `json:"httpLogsEndpointOverride,omitempty"`
	// List of key-value pairs to send with each gRPC request. Value supports JavaScript expressions that are evaluated just once, when the destination gets started. To pass credentials as metadata, use 'C.Secret'.
	Metadata []OutputMetadatumOpenTelemetry `json:"metadata,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max body size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeOpenTelemetry `default:"none" json:"failedRequestLoggingMode"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// How often the sender should ping the peer to keep the connection open
	KeepAliveTime *float64 `default:"30" json:"keepAliveTime"`
	// Disable to close the connection immediately after sending the outgoing request
	KeepAlive *bool `default:"true" json:"keepAlive"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorOpenTelemetry `default:"block" json:"onBackpressure"`
	Description    *string                            `json:"description,omitempty"`
	Username       *string                            `json:"username,omitempty"`
	Password       *string                            `json:"password,omitempty"`
	// Bearer token to include in the authorization header
	Token *string `json:"token,omitempty"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// URL for OAuth
	LoginURL *string `json:"loginUrl,omitempty"`
	// Secret parameter name to pass in request body
	SecretParamName *string `json:"secretParamName,omitempty"`
	// Secret parameter value to pass in request body
	Secret *string `json:"secret,omitempty"`
	// Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
	TokenAttributeName *string `json:"tokenAttributeName,omitempty"`
	// JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
	AuthHeaderExpr *string `default:"Bearer \\${token}" json:"authHeaderExpr"`
	// How often the OAuth token should be refreshed.
	TokenTimeoutSecs *float64 `default:"3600" json:"tokenTimeoutSecs"`
	// Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthParams []OutputOauthParamOpenTelemetry `json:"oauthParams,omitempty"`
	// Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthHeaders []OutputOauthHeaderOpenTelemetry `json:"oauthHeaders,omitempty"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Defaults to Yes. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Enables round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Headers to add to all events.
	ExtraHTTPHeaders []ExtraHTTPHeaderOpenTelemetry `json:"extraHttpHeaders,omitempty"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable).
	ResponseRetrySettings []ResponseRetrySettingOpenTelemetry `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsOpenTelemetry  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool                               `default:"false" json:"responseHonorRetryAfterHeader"`
	TLS                           *TLSSettingsClientSideOpenTelemetry `json:"tls,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *PqCompressCompressionOpenTelemetry `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorOpenTelemetry `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *OutputModeOpenTelemetry `default:"error" json:"pqMode"`
	PqControls *PqControlsOpenTelemetry `json:"pqControls,omitempty"`
	Status     *TFStatus                `json:"status,omitempty"`
}

func (o OutputOpenTelemetry) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputOpenTelemetry) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputOpenTelemetry) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputOpenTelemetry) GetType() OutputTypeOpenTelemetry {
	if o == nil {
		return OutputTypeOpenTelemetry("")
	}
	return o.Type
}

func (o *OutputOpenTelemetry) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputOpenTelemetry) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputOpenTelemetry) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputOpenTelemetry) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputOpenTelemetry) GetProtocol() *OutputProtocolOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.Protocol
}

func (o *OutputOpenTelemetry) GetEndpoint() string {
	if o == nil {
		return ""
	}
	return o.Endpoint
}

func (o *OutputOpenTelemetry) GetOtlpVersion() *OutputOTLPVersionOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.OtlpVersion
}

func (o *OutputOpenTelemetry) GetCompress() *OutputCompressCompressionOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputOpenTelemetry) GetHTTPCompress() *HTTPCompressCompressionOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.HTTPCompress
}

func (o *OutputOpenTelemetry) GetAuthType() *OutputAuthenticationTypeOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputOpenTelemetry) GetHTTPTracesEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPTracesEndpointOverride
}

func (o *OutputOpenTelemetry) GetHTTPMetricsEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPMetricsEndpointOverride
}

func (o *OutputOpenTelemetry) GetHTTPLogsEndpointOverride() *string {
	if o == nil {
		return nil
	}
	return o.HTTPLogsEndpointOverride
}

func (o *OutputOpenTelemetry) GetMetadata() []OutputMetadatumOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.Metadata
}

func (o *OutputOpenTelemetry) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputOpenTelemetry) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputOpenTelemetry) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputOpenTelemetry) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputOpenTelemetry) GetFailedRequestLoggingMode() *FailedRequestLoggingModeOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputOpenTelemetry) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputOpenTelemetry) GetKeepAliveTime() *float64 {
	if o == nil {
		return nil
	}
	return o.KeepAliveTime
}

func (o *OutputOpenTelemetry) GetKeepAlive() *bool {
	if o == nil {
		return nil
	}
	return o.KeepAlive
}

func (o *OutputOpenTelemetry) GetOnBackpressure() *BackpressureBehaviorOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputOpenTelemetry) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputOpenTelemetry) GetUsername() *string {
	if o == nil {
		return nil
	}
	return o.Username
}

func (o *OutputOpenTelemetry) GetPassword() *string {
	if o == nil {
		return nil
	}
	return o.Password
}

func (o *OutputOpenTelemetry) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputOpenTelemetry) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

func (o *OutputOpenTelemetry) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputOpenTelemetry) GetLoginURL() *string {
	if o == nil {
		return nil
	}
	return o.LoginURL
}

func (o *OutputOpenTelemetry) GetSecretParamName() *string {
	if o == nil {
		return nil
	}
	return o.SecretParamName
}

func (o *OutputOpenTelemetry) GetSecret() *string {
	if o == nil {
		return nil
	}
	return o.Secret
}

func (o *OutputOpenTelemetry) GetTokenAttributeName() *string {
	if o == nil {
		return nil
	}
	return o.TokenAttributeName
}

func (o *OutputOpenTelemetry) GetAuthHeaderExpr() *string {
	if o == nil {
		return nil
	}
	return o.AuthHeaderExpr
}

func (o *OutputOpenTelemetry) GetTokenTimeoutSecs() *float64 {
	if o == nil {
		return nil
	}
	return o.TokenTimeoutSecs
}

func (o *OutputOpenTelemetry) GetOauthParams() []OutputOauthParamOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.OauthParams
}

func (o *OutputOpenTelemetry) GetOauthHeaders() []OutputOauthHeaderOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.OauthHeaders
}

func (o *OutputOpenTelemetry) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputOpenTelemetry) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputOpenTelemetry) GetExtraHTTPHeaders() []ExtraHTTPHeaderOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputOpenTelemetry) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputOpenTelemetry) GetResponseRetrySettings() []ResponseRetrySettingOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputOpenTelemetry) GetTimeoutRetrySettings() *TimeoutRetrySettingsOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputOpenTelemetry) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputOpenTelemetry) GetTLS() *TLSSettingsClientSideOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputOpenTelemetry) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputOpenTelemetry) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputOpenTelemetry) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputOpenTelemetry) GetPqCompress() *PqCompressCompressionOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputOpenTelemetry) GetPqOnBackpressure() *QueueFullBehaviorOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputOpenTelemetry) GetPqMode() *OutputModeOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputOpenTelemetry) GetPqControls() *PqControlsOpenTelemetry {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputOpenTelemetry) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeRing string

const (
	TypeRingRing TypeRing = "ring"
)

func (e TypeRing) ToPointer() *TypeRing {
	return &e
}
func (e *TypeRing) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "ring":
		*e = TypeRing(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeRing: %v", v)
	}
}

// DataFormatRing - Format of the output data.
type DataFormatRing string

const (
	DataFormatRingJSON DataFormatRing = "json"
	DataFormatRingRaw  DataFormatRing = "raw"
)

func (e DataFormatRing) ToPointer() *DataFormatRing {
	return &e
}
func (e *DataFormatRing) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "json":
		fallthrough
	case "raw":
		*e = DataFormatRing(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DataFormatRing: %v", v)
	}
}

type OutputDataCompressionFormat string

const (
	OutputDataCompressionFormatNone OutputDataCompressionFormat = "none"
	OutputDataCompressionFormatGzip OutputDataCompressionFormat = "gzip"
)

func (e OutputDataCompressionFormat) ToPointer() *OutputDataCompressionFormat {
	return &e
}
func (e *OutputDataCompressionFormat) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = OutputDataCompressionFormat(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputDataCompressionFormat: %v", v)
	}
}

// BackpressureBehaviorRing - Whether to block or drop events when all receivers are exerting backpressure
type BackpressureBehaviorRing string

const (
	BackpressureBehaviorRingBlock BackpressureBehaviorRing = "block"
	BackpressureBehaviorRingDrop  BackpressureBehaviorRing = "drop"
)

func (e BackpressureBehaviorRing) ToPointer() *BackpressureBehaviorRing {
	return &e
}
func (e *BackpressureBehaviorRing) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = BackpressureBehaviorRing(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorRing: %v", v)
	}
}

type OutputRing struct {
	// Unique ID for this output
	ID   string   `json:"id"`
	Type TypeRing `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Format of the output data.
	Format *DataFormatRing `default:"json" json:"format"`
	// JS expression to define how files are partitioned and organized. If left blank, Cribl Stream will fallback on event.__partition.
	PartitionExpr *string `json:"partitionExpr,omitempty"`
	// Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.
	MaxDataSize *string `default:"1GB" json:"maxDataSize"`
	// Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.
	MaxDataTime *string                      `default:"24h" json:"maxDataTime"`
	Compress    *OutputDataCompressionFormat `default:"gzip" json:"compress"`
	// Path to use to write metrics. Defaults to $CRIBL_HOME/state/<id>
	DestPath *string `json:"destPath,omitempty"`
	// Whether to block or drop events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorRing `default:"block" json:"onBackpressure"`
	Description    *string                   `json:"description,omitempty"`
	Status         *TFStatus                 `json:"status,omitempty"`
}

func (o OutputRing) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputRing) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputRing) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputRing) GetType() TypeRing {
	if o == nil {
		return TypeRing("")
	}
	return o.Type
}

func (o *OutputRing) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputRing) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputRing) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputRing) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputRing) GetFormat() *DataFormatRing {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputRing) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputRing) GetMaxDataSize() *string {
	if o == nil {
		return nil
	}
	return o.MaxDataSize
}

func (o *OutputRing) GetMaxDataTime() *string {
	if o == nil {
		return nil
	}
	return o.MaxDataTime
}

func (o *OutputRing) GetCompress() *OutputDataCompressionFormat {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputRing) GetDestPath() *string {
	if o == nil {
		return nil
	}
	return o.DestPath
}

func (o *OutputRing) GetOnBackpressure() *BackpressureBehaviorRing {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputRing) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputRing) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type OutputTypePrometheus string

const (
	OutputTypePrometheusPrometheus OutputTypePrometheus = "prometheus"
)

func (e OutputTypePrometheus) ToPointer() *OutputTypePrometheus {
	return &e
}
func (e *OutputTypePrometheus) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "prometheus":
		*e = OutputTypePrometheus(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypePrometheus: %v", v)
	}
}

type ExtraHTTPHeaderPrometheus struct {
	// Field name
	Name *string `json:"name,omitempty"`
	// Field value
	Value string `json:"value"`
}

func (o *ExtraHTTPHeaderPrometheus) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderPrometheus) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModePrometheus - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModePrometheus string

const (
	FailedRequestLoggingModePrometheusPayload           FailedRequestLoggingModePrometheus = "payload"
	FailedRequestLoggingModePrometheusPayloadAndHeaders FailedRequestLoggingModePrometheus = "payloadAndHeaders"
	FailedRequestLoggingModePrometheusNone              FailedRequestLoggingModePrometheus = "none"
)

func (e FailedRequestLoggingModePrometheus) ToPointer() *FailedRequestLoggingModePrometheus {
	return &e
}
func (e *FailedRequestLoggingModePrometheus) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModePrometheus(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModePrometheus: %v", v)
	}
}

type ResponseRetrySettingPrometheus struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingPrometheus) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingPrometheus) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingPrometheus) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingPrometheus) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingPrometheus) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingPrometheus) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsPrometheus struct {
	// Enable to retry on request timeout
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsPrometheus) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsPrometheus) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsPrometheus) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsPrometheus) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsPrometheus) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsPrometheus) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorPrometheus - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorPrometheus string

const (
	BackpressureBehaviorPrometheusBlock BackpressureBehaviorPrometheus = "block"
	BackpressureBehaviorPrometheusDrop  BackpressureBehaviorPrometheus = "drop"
	BackpressureBehaviorPrometheusQueue BackpressureBehaviorPrometheus = "queue"
)

func (e BackpressureBehaviorPrometheus) ToPointer() *BackpressureBehaviorPrometheus {
	return &e
}
func (e *BackpressureBehaviorPrometheus) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorPrometheus(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorPrometheus: %v", v)
	}
}

// AuthenticationTypePrometheus - Remote Write authentication type
type AuthenticationTypePrometheus string

const (
	AuthenticationTypePrometheusNone              AuthenticationTypePrometheus = "none"
	AuthenticationTypePrometheusBasic             AuthenticationTypePrometheus = "basic"
	AuthenticationTypePrometheusCredentialsSecret AuthenticationTypePrometheus = "credentialsSecret"
	AuthenticationTypePrometheusToken             AuthenticationTypePrometheus = "token"
	AuthenticationTypePrometheusTextSecret        AuthenticationTypePrometheus = "textSecret"
	AuthenticationTypePrometheusOauth             AuthenticationTypePrometheus = "oauth"
)

func (e AuthenticationTypePrometheus) ToPointer() *AuthenticationTypePrometheus {
	return &e
}
func (e *AuthenticationTypePrometheus) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "basic":
		fallthrough
	case "credentialsSecret":
		fallthrough
	case "token":
		fallthrough
	case "textSecret":
		fallthrough
	case "oauth":
		*e = AuthenticationTypePrometheus(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationTypePrometheus: %v", v)
	}
}

// PqCompressCompressionPrometheus - Codec to use to compress the persisted data.
type PqCompressCompressionPrometheus string

const (
	PqCompressCompressionPrometheusNone PqCompressCompressionPrometheus = "none"
	PqCompressCompressionPrometheusGzip PqCompressCompressionPrometheus = "gzip"
)

func (e PqCompressCompressionPrometheus) ToPointer() *PqCompressCompressionPrometheus {
	return &e
}
func (e *PqCompressCompressionPrometheus) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = PqCompressCompressionPrometheus(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PqCompressCompressionPrometheus: %v", v)
	}
}

// QueueFullBehaviorPrometheus - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorPrometheus string

const (
	QueueFullBehaviorPrometheusBlock QueueFullBehaviorPrometheus = "block"
	QueueFullBehaviorPrometheusDrop  QueueFullBehaviorPrometheus = "drop"
)

func (e QueueFullBehaviorPrometheus) ToPointer() *QueueFullBehaviorPrometheus {
	return &e
}
func (e *QueueFullBehaviorPrometheus) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorPrometheus(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorPrometheus: %v", v)
	}
}

// OutputModePrometheus - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type OutputModePrometheus string

const (
	OutputModePrometheusError        OutputModePrometheus = "error"
	OutputModePrometheusBackpressure OutputModePrometheus = "backpressure"
	OutputModePrometheusAlways       OutputModePrometheus = "always"
)

func (e OutputModePrometheus) ToPointer() *OutputModePrometheus {
	return &e
}
func (e *OutputModePrometheus) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = OutputModePrometheus(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputModePrometheus: %v", v)
	}
}

type PqControlsPrometheus struct {
}

type OauthParamPrometheus struct {
	// OAuth parameter name
	Name string `json:"name"`
	// OAuth parameter value
	Value string `json:"value"`
}

func (o *OauthParamPrometheus) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OauthParamPrometheus) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OauthHeaderPrometheus struct {
	// OAuth header name
	Name string `json:"name"`
	// OAuth header value
	Value string `json:"value"`
}

func (o *OauthHeaderPrometheus) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OauthHeaderPrometheus) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OutputPrometheus struct {
	// Unique ID for this output
	ID   *string              `json:"id,omitempty"`
	Type OutputTypePrometheus `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions to generated metrics.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The endpoint to send metrics to.
	URL string `json:"url"`
	// A JS expression that can be used to rename metrics. E.g.: name.replace(/\./g, '_') will replace all '.' characters in a metric's name with the supported '_' character. Use the 'name' global variable to access the metric's name.  You can access event fields' values via __e.<fieldName>.
	MetricRenameExpr *string `default:"name.replace(/[^a-zA-Z0-9_]/g, '_')" json:"metricRenameExpr"`
	// Whether to generate and send metadata (`type` and `metricFamilyName`) requests.
	SendMetadata *bool `default:"true" json:"sendMetadata"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Defaults to Yes. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max body size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events.
	ExtraHTTPHeaders []ExtraHTTPHeaderPrometheus `json:"extraHttpHeaders,omitempty"`
	// Enables round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModePrometheus `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable).
	ResponseRetrySettings []ResponseRetrySettingPrometheus `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsPrometheus  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorPrometheus `default:"block" json:"onBackpressure"`
	// Remote Write authentication type
	AuthType    *AuthenticationTypePrometheus `default:"none" json:"authType"`
	Description *string                       `json:"description,omitempty"`
	// How frequently metrics metadata is sent out. Value cannot be smaller than the base Flush period (sec) set above.
	MetricsFlushPeriodSec *float64 `default:"60" json:"metricsFlushPeriodSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *PqCompressCompressionPrometheus `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorPrometheus `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *OutputModePrometheus `default:"error" json:"pqMode"`
	PqControls *PqControlsPrometheus `json:"pqControls,omitempty"`
	Username   *string               `json:"username,omitempty"`
	Password   *string               `json:"password,omitempty"`
	// Bearer token to include in the authorization header
	Token *string `json:"token,omitempty"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// URL for OAuth
	LoginURL *string `json:"loginUrl,omitempty"`
	// Secret parameter name to pass in request body
	SecretParamName *string `json:"secretParamName,omitempty"`
	// Secret parameter value to pass in request body
	Secret *string `json:"secret,omitempty"`
	// Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
	TokenAttributeName *string `json:"tokenAttributeName,omitempty"`
	// JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
	AuthHeaderExpr *string `default:"Bearer \\${token}" json:"authHeaderExpr"`
	// How often the OAuth token should be refreshed.
	TokenTimeoutSecs *float64 `default:"3600" json:"tokenTimeoutSecs"`
	// Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthParams []OauthParamPrometheus `json:"oauthParams,omitempty"`
	// Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthHeaders []OauthHeaderPrometheus `json:"oauthHeaders,omitempty"`
	Status       *TFStatus               `json:"status,omitempty"`
}

func (o OutputPrometheus) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputPrometheus) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputPrometheus) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputPrometheus) GetType() OutputTypePrometheus {
	if o == nil {
		return OutputTypePrometheus("")
	}
	return o.Type
}

func (o *OutputPrometheus) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputPrometheus) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputPrometheus) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputPrometheus) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputPrometheus) GetURL() string {
	if o == nil {
		return ""
	}
	return o.URL
}

func (o *OutputPrometheus) GetMetricRenameExpr() *string {
	if o == nil {
		return nil
	}
	return o.MetricRenameExpr
}

func (o *OutputPrometheus) GetSendMetadata() *bool {
	if o == nil {
		return nil
	}
	return o.SendMetadata
}

func (o *OutputPrometheus) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputPrometheus) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputPrometheus) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputPrometheus) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputPrometheus) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputPrometheus) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputPrometheus) GetExtraHTTPHeaders() []ExtraHTTPHeaderPrometheus {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputPrometheus) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputPrometheus) GetFailedRequestLoggingMode() *FailedRequestLoggingModePrometheus {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputPrometheus) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputPrometheus) GetResponseRetrySettings() []ResponseRetrySettingPrometheus {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputPrometheus) GetTimeoutRetrySettings() *TimeoutRetrySettingsPrometheus {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputPrometheus) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputPrometheus) GetOnBackpressure() *BackpressureBehaviorPrometheus {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputPrometheus) GetAuthType() *AuthenticationTypePrometheus {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputPrometheus) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputPrometheus) GetMetricsFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MetricsFlushPeriodSec
}

func (o *OutputPrometheus) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputPrometheus) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputPrometheus) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputPrometheus) GetPqCompress() *PqCompressCompressionPrometheus {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputPrometheus) GetPqOnBackpressure() *QueueFullBehaviorPrometheus {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputPrometheus) GetPqMode() *OutputModePrometheus {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputPrometheus) GetPqControls() *PqControlsPrometheus {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputPrometheus) GetUsername() *string {
	if o == nil {
		return nil
	}
	return o.Username
}

func (o *OutputPrometheus) GetPassword() *string {
	if o == nil {
		return nil
	}
	return o.Password
}

func (o *OutputPrometheus) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputPrometheus) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

func (o *OutputPrometheus) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputPrometheus) GetLoginURL() *string {
	if o == nil {
		return nil
	}
	return o.LoginURL
}

func (o *OutputPrometheus) GetSecretParamName() *string {
	if o == nil {
		return nil
	}
	return o.SecretParamName
}

func (o *OutputPrometheus) GetSecret() *string {
	if o == nil {
		return nil
	}
	return o.Secret
}

func (o *OutputPrometheus) GetTokenAttributeName() *string {
	if o == nil {
		return nil
	}
	return o.TokenAttributeName
}

func (o *OutputPrometheus) GetAuthHeaderExpr() *string {
	if o == nil {
		return nil
	}
	return o.AuthHeaderExpr
}

func (o *OutputPrometheus) GetTokenTimeoutSecs() *float64 {
	if o == nil {
		return nil
	}
	return o.TokenTimeoutSecs
}

func (o *OutputPrometheus) GetOauthParams() []OauthParamPrometheus {
	if o == nil {
		return nil
	}
	return o.OauthParams
}

func (o *OutputPrometheus) GetOauthHeaders() []OauthHeaderPrometheus {
	if o == nil {
		return nil
	}
	return o.OauthHeaders
}

func (o *OutputPrometheus) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type OutputTypeLoki string

const (
	OutputTypeLokiLoki OutputTypeLoki = "loki"
)

func (e OutputTypeLoki) ToPointer() *OutputTypeLoki {
	return &e
}
func (e *OutputTypeLoki) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "loki":
		*e = OutputTypeLoki(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeLoki: %v", v)
	}
}

// MessageFormatLoki - Which format to use when sending logs to Loki (Protobuf or JSON).  Defaults to Protobuf.
type MessageFormatLoki string

const (
	MessageFormatLokiProtobuf MessageFormatLoki = "protobuf"
	MessageFormatLokiJSON     MessageFormatLoki = "json"
)

func (e MessageFormatLoki) ToPointer() *MessageFormatLoki {
	return &e
}
func (e *MessageFormatLoki) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "protobuf":
		fallthrough
	case "json":
		*e = MessageFormatLoki(v)
		return nil
	default:
		return fmt.Errorf("invalid value for MessageFormatLoki: %v", v)
	}
}

type LabelLoki struct {
	// Name of the label.
	Name *string `default:"" json:"name"`
	// Value of the label.
	Value string `json:"value"`
}

func (l LabelLoki) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(l, "", false)
}

func (l *LabelLoki) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &l, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *LabelLoki) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *LabelLoki) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// OutputAuthenticationTypeLoki - The authentication method to use for the HTTP requests
type OutputAuthenticationTypeLoki string

const (
	OutputAuthenticationTypeLokiNone              OutputAuthenticationTypeLoki = "none"
	OutputAuthenticationTypeLokiToken             OutputAuthenticationTypeLoki = "token"
	OutputAuthenticationTypeLokiTextSecret        OutputAuthenticationTypeLoki = "textSecret"
	OutputAuthenticationTypeLokiBasic             OutputAuthenticationTypeLoki = "basic"
	OutputAuthenticationTypeLokiCredentialsSecret OutputAuthenticationTypeLoki = "credentialsSecret"
)

func (e OutputAuthenticationTypeLoki) ToPointer() *OutputAuthenticationTypeLoki {
	return &e
}
func (e *OutputAuthenticationTypeLoki) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "token":
		fallthrough
	case "textSecret":
		fallthrough
	case "basic":
		fallthrough
	case "credentialsSecret":
		*e = OutputAuthenticationTypeLoki(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputAuthenticationTypeLoki: %v", v)
	}
}

type ExtraHTTPHeaderLoki struct {
	// Field name
	Name *string `json:"name,omitempty"`
	// Field value
	Value string `json:"value"`
}

func (o *ExtraHTTPHeaderLoki) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderLoki) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeLoki - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeLoki string

const (
	FailedRequestLoggingModeLokiPayload           FailedRequestLoggingModeLoki = "payload"
	FailedRequestLoggingModeLokiPayloadAndHeaders FailedRequestLoggingModeLoki = "payloadAndHeaders"
	FailedRequestLoggingModeLokiNone              FailedRequestLoggingModeLoki = "none"
)

func (e FailedRequestLoggingModeLoki) ToPointer() *FailedRequestLoggingModeLoki {
	return &e
}
func (e *FailedRequestLoggingModeLoki) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeLoki(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeLoki: %v", v)
	}
}

type ResponseRetrySettingLoki struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingLoki) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingLoki) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingLoki) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingLoki) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingLoki) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingLoki) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsLoki struct {
	// Enable to retry on request timeout
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsLoki) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsLoki) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsLoki) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsLoki) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsLoki) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsLoki) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorLoki - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorLoki string

const (
	BackpressureBehaviorLokiBlock BackpressureBehaviorLoki = "block"
	BackpressureBehaviorLokiDrop  BackpressureBehaviorLoki = "drop"
	BackpressureBehaviorLokiQueue BackpressureBehaviorLoki = "queue"
)

func (e BackpressureBehaviorLoki) ToPointer() *BackpressureBehaviorLoki {
	return &e
}
func (e *BackpressureBehaviorLoki) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorLoki(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorLoki: %v", v)
	}
}

// PqCompressCompressionLoki - Codec to use to compress the persisted data.
type PqCompressCompressionLoki string

const (
	PqCompressCompressionLokiNone PqCompressCompressionLoki = "none"
	PqCompressCompressionLokiGzip PqCompressCompressionLoki = "gzip"
)

func (e PqCompressCompressionLoki) ToPointer() *PqCompressCompressionLoki {
	return &e
}
func (e *PqCompressCompressionLoki) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = PqCompressCompressionLoki(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PqCompressCompressionLoki: %v", v)
	}
}

// QueueFullBehaviorLoki - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorLoki string

const (
	QueueFullBehaviorLokiBlock QueueFullBehaviorLoki = "block"
	QueueFullBehaviorLokiDrop  QueueFullBehaviorLoki = "drop"
)

func (e QueueFullBehaviorLoki) ToPointer() *QueueFullBehaviorLoki {
	return &e
}
func (e *QueueFullBehaviorLoki) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorLoki(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorLoki: %v", v)
	}
}

// OutputModeLoki - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type OutputModeLoki string

const (
	OutputModeLokiError        OutputModeLoki = "error"
	OutputModeLokiBackpressure OutputModeLoki = "backpressure"
	OutputModeLokiAlways       OutputModeLoki = "always"
)

func (e OutputModeLoki) ToPointer() *OutputModeLoki {
	return &e
}
func (e *OutputModeLoki) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = OutputModeLoki(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputModeLoki: %v", v)
	}
}

type PqControlsLoki struct {
}

type OutputLoki struct {
	// Unique ID for this output
	ID   *string        `json:"id,omitempty"`
	Type OutputTypeLoki `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as labels to generated logs.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The endpoint to send logs to.
	URL string `json:"url"`
	// Name of the event field that contains the message to send. If not specified, Stream sends a JSON representation of the whole event.
	Message *string `json:"message,omitempty"`
	// Which format to use when sending logs to Loki (Protobuf or JSON).  Defaults to Protobuf.
	MessageFormat *MessageFormatLoki `default:"protobuf" json:"messageFormat"`
	// List of labels to send with logs. Labels define Loki streams, so use static labels to avoid proliferating label value combinations and streams. Can be merged and/or overridden by the event's __labels field (e.g.: '__labels: {host: "cribl.io", level: "error"}').
	Labels []LabelLoki `json:"labels,omitempty"`
	// The authentication method to use for the HTTP requests
	AuthType *OutputAuthenticationTypeLoki `default:"none" json:"authType"`
	// Maximum number of ongoing requests before blocking. Warning: Setting this value > 1 can cause Loki to complain about entries being delivered out of order.
	Concurrency *float64 `default:"1" json:"concurrency"`
	// Maximum size, in KB, of the request body. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki to complain about entries being delivered out of order.
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Defaults to 0 (unlimited). Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki to complain about entries being delivered out of order.
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Defaults to Yes. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Maximum time between requests. Small values can reduce the payload size below the configured 'Max record size' and 'Max events per request'. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki to complain about entries being delivered out of order.
	FlushPeriodSec *float64 `default:"15" json:"flushPeriodSec"`
	// Headers to add to all events.
	ExtraHTTPHeaders []ExtraHTTPHeaderLoki `json:"extraHttpHeaders,omitempty"`
	// Enables round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeLoki `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable).
	ResponseRetrySettings []ResponseRetrySettingLoki `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsLoki  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorLoki `default:"block" json:"onBackpressure"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	Description        *string  `json:"description,omitempty"`
	// Whether to compress the payload body before sending.
	Compress *bool `default:"true" json:"compress"`
	// Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. E.g.: <your-username>:<your-api-key>.
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// Username for authentication
	Username *string `json:"username,omitempty"`
	// Password (a.k.a API key in Grafana Cloud domain) for authentication
	Password *string `json:"password,omitempty"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *PqCompressCompressionLoki `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorLoki `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *OutputModeLoki `default:"error" json:"pqMode"`
	PqControls *PqControlsLoki `json:"pqControls,omitempty"`
	Status     *TFStatus       `json:"status,omitempty"`
}

func (o OutputLoki) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputLoki) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputLoki) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputLoki) GetType() OutputTypeLoki {
	if o == nil {
		return OutputTypeLoki("")
	}
	return o.Type
}

func (o *OutputLoki) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputLoki) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputLoki) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputLoki) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputLoki) GetURL() string {
	if o == nil {
		return ""
	}
	return o.URL
}

func (o *OutputLoki) GetMessage() *string {
	if o == nil {
		return nil
	}
	return o.Message
}

func (o *OutputLoki) GetMessageFormat() *MessageFormatLoki {
	if o == nil {
		return nil
	}
	return o.MessageFormat
}

func (o *OutputLoki) GetLabels() []LabelLoki {
	if o == nil {
		return nil
	}
	return o.Labels
}

func (o *OutputLoki) GetAuthType() *OutputAuthenticationTypeLoki {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputLoki) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputLoki) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputLoki) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputLoki) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputLoki) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputLoki) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputLoki) GetExtraHTTPHeaders() []ExtraHTTPHeaderLoki {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputLoki) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputLoki) GetFailedRequestLoggingMode() *FailedRequestLoggingModeLoki {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputLoki) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputLoki) GetResponseRetrySettings() []ResponseRetrySettingLoki {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputLoki) GetTimeoutRetrySettings() *TimeoutRetrySettingsLoki {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputLoki) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputLoki) GetOnBackpressure() *BackpressureBehaviorLoki {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputLoki) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputLoki) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputLoki) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputLoki) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputLoki) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputLoki) GetUsername() *string {
	if o == nil {
		return nil
	}
	return o.Username
}

func (o *OutputLoki) GetPassword() *string {
	if o == nil {
		return nil
	}
	return o.Password
}

func (o *OutputLoki) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

func (o *OutputLoki) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputLoki) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputLoki) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputLoki) GetPqCompress() *PqCompressCompressionLoki {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputLoki) GetPqOnBackpressure() *QueueFullBehaviorLoki {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputLoki) GetPqMode() *OutputModeLoki {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputLoki) GetPqControls() *PqControlsLoki {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputLoki) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type OutputGrafanaCloudType2 string

const (
	OutputGrafanaCloudType2GrafanaCloud OutputGrafanaCloudType2 = "grafana_cloud"
)

func (e OutputGrafanaCloudType2) ToPointer() *OutputGrafanaCloudType2 {
	return &e
}
func (e *OutputGrafanaCloudType2) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "grafana_cloud":
		*e = OutputGrafanaCloudType2(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputGrafanaCloudType2: %v", v)
	}
}

// OutputGrafanaCloudMessageFormat2 - Which format to use when sending logs to Loki (Protobuf or JSON).  Defaults to Protobuf.
type OutputGrafanaCloudMessageFormat2 string

const (
	OutputGrafanaCloudMessageFormat2Protobuf OutputGrafanaCloudMessageFormat2 = "protobuf"
	OutputGrafanaCloudMessageFormat2JSON     OutputGrafanaCloudMessageFormat2 = "json"
)

func (e OutputGrafanaCloudMessageFormat2) ToPointer() *OutputGrafanaCloudMessageFormat2 {
	return &e
}
func (e *OutputGrafanaCloudMessageFormat2) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "protobuf":
		fallthrough
	case "json":
		*e = OutputGrafanaCloudMessageFormat2(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputGrafanaCloudMessageFormat2: %v", v)
	}
}

type OutputGrafanaCloudLabel2 struct {
	// Name of the label.
	Name *string `default:"" json:"name"`
	// Value of the label.
	Value string `json:"value"`
}

func (o OutputGrafanaCloudLabel2) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGrafanaCloudLabel2) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *OutputGrafanaCloudLabel2) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *OutputGrafanaCloudLabel2) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// OutputGrafanaCloudPrometheusAuthAuthenticationType2 - The authentication method to use for the HTTP requests
type OutputGrafanaCloudPrometheusAuthAuthenticationType2 string

const (
	OutputGrafanaCloudPrometheusAuthAuthenticationType2None              OutputGrafanaCloudPrometheusAuthAuthenticationType2 = "none"
	OutputGrafanaCloudPrometheusAuthAuthenticationType2Token             OutputGrafanaCloudPrometheusAuthAuthenticationType2 = "token"
	OutputGrafanaCloudPrometheusAuthAuthenticationType2TextSecret        OutputGrafanaCloudPrometheusAuthAuthenticationType2 = "textSecret"
	OutputGrafanaCloudPrometheusAuthAuthenticationType2Basic             OutputGrafanaCloudPrometheusAuthAuthenticationType2 = "basic"
	OutputGrafanaCloudPrometheusAuthAuthenticationType2CredentialsSecret OutputGrafanaCloudPrometheusAuthAuthenticationType2 = "credentialsSecret"
)

func (e OutputGrafanaCloudPrometheusAuthAuthenticationType2) ToPointer() *OutputGrafanaCloudPrometheusAuthAuthenticationType2 {
	return &e
}
func (e *OutputGrafanaCloudPrometheusAuthAuthenticationType2) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "token":
		fallthrough
	case "textSecret":
		fallthrough
	case "basic":
		fallthrough
	case "credentialsSecret":
		*e = OutputGrafanaCloudPrometheusAuthAuthenticationType2(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputGrafanaCloudPrometheusAuthAuthenticationType2: %v", v)
	}
}

type OutputPrometheusAuth2 struct {
	// The authentication method to use for the HTTP requests
	AuthType *OutputGrafanaCloudPrometheusAuthAuthenticationType2 `default:"basic" json:"authType"`
	// Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. E.g.: <your-username>:<your-api-key>.
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// Username for authentication
	Username *string `json:"username,omitempty"`
	// Password (a.k.a API key in Grafana Cloud domain) for authentication
	Password *string `json:"password,omitempty"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
}

func (o OutputPrometheusAuth2) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputPrometheusAuth2) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *OutputPrometheusAuth2) GetAuthType() *OutputGrafanaCloudPrometheusAuthAuthenticationType2 {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputPrometheusAuth2) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputPrometheusAuth2) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputPrometheusAuth2) GetUsername() *string {
	if o == nil {
		return nil
	}
	return o.Username
}

func (o *OutputPrometheusAuth2) GetPassword() *string {
	if o == nil {
		return nil
	}
	return o.Password
}

func (o *OutputPrometheusAuth2) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

// OutputGrafanaCloudLokiAuthAuthenticationType2 - The authentication method to use for the HTTP requests
type OutputGrafanaCloudLokiAuthAuthenticationType2 string

const (
	OutputGrafanaCloudLokiAuthAuthenticationType2None              OutputGrafanaCloudLokiAuthAuthenticationType2 = "none"
	OutputGrafanaCloudLokiAuthAuthenticationType2Token             OutputGrafanaCloudLokiAuthAuthenticationType2 = "token"
	OutputGrafanaCloudLokiAuthAuthenticationType2TextSecret        OutputGrafanaCloudLokiAuthAuthenticationType2 = "textSecret"
	OutputGrafanaCloudLokiAuthAuthenticationType2Basic             OutputGrafanaCloudLokiAuthAuthenticationType2 = "basic"
	OutputGrafanaCloudLokiAuthAuthenticationType2CredentialsSecret OutputGrafanaCloudLokiAuthAuthenticationType2 = "credentialsSecret"
)

func (e OutputGrafanaCloudLokiAuthAuthenticationType2) ToPointer() *OutputGrafanaCloudLokiAuthAuthenticationType2 {
	return &e
}
func (e *OutputGrafanaCloudLokiAuthAuthenticationType2) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "token":
		fallthrough
	case "textSecret":
		fallthrough
	case "basic":
		fallthrough
	case "credentialsSecret":
		*e = OutputGrafanaCloudLokiAuthAuthenticationType2(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputGrafanaCloudLokiAuthAuthenticationType2: %v", v)
	}
}

type OutputLokiAuth2 struct {
	// The authentication method to use for the HTTP requests
	AuthType *OutputGrafanaCloudLokiAuthAuthenticationType2 `default:"basic" json:"authType"`
	// Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. E.g.: <your-username>:<your-api-key>.
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// Username for authentication
	Username *string `json:"username,omitempty"`
	// Password (a.k.a API key in Grafana Cloud domain) for authentication
	Password *string `json:"password,omitempty"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
}

func (o OutputLokiAuth2) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputLokiAuth2) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *OutputLokiAuth2) GetAuthType() *OutputGrafanaCloudLokiAuthAuthenticationType2 {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputLokiAuth2) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputLokiAuth2) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputLokiAuth2) GetUsername() *string {
	if o == nil {
		return nil
	}
	return o.Username
}

func (o *OutputLokiAuth2) GetPassword() *string {
	if o == nil {
		return nil
	}
	return o.Password
}

func (o *OutputLokiAuth2) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

type OutputGrafanaCloudExtraHTTPHeader2 struct {
	// Field name
	Name *string `json:"name,omitempty"`
	// Field value
	Value string `json:"value"`
}

func (o *OutputGrafanaCloudExtraHTTPHeader2) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *OutputGrafanaCloudExtraHTTPHeader2) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// OutputGrafanaCloudFailedRequestLoggingMode2 - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type OutputGrafanaCloudFailedRequestLoggingMode2 string

const (
	OutputGrafanaCloudFailedRequestLoggingMode2Payload           OutputGrafanaCloudFailedRequestLoggingMode2 = "payload"
	OutputGrafanaCloudFailedRequestLoggingMode2PayloadAndHeaders OutputGrafanaCloudFailedRequestLoggingMode2 = "payloadAndHeaders"
	OutputGrafanaCloudFailedRequestLoggingMode2None              OutputGrafanaCloudFailedRequestLoggingMode2 = "none"
)

func (e OutputGrafanaCloudFailedRequestLoggingMode2) ToPointer() *OutputGrafanaCloudFailedRequestLoggingMode2 {
	return &e
}
func (e *OutputGrafanaCloudFailedRequestLoggingMode2) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = OutputGrafanaCloudFailedRequestLoggingMode2(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputGrafanaCloudFailedRequestLoggingMode2: %v", v)
	}
}

type OutputGrafanaCloudResponseRetrySetting2 struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (o OutputGrafanaCloudResponseRetrySetting2) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGrafanaCloudResponseRetrySetting2) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *OutputGrafanaCloudResponseRetrySetting2) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *OutputGrafanaCloudResponseRetrySetting2) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *OutputGrafanaCloudResponseRetrySetting2) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *OutputGrafanaCloudResponseRetrySetting2) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type OutputGrafanaCloudTimeoutRetrySettings2 struct {
	// Enable to retry on request timeout
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (o OutputGrafanaCloudTimeoutRetrySettings2) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGrafanaCloudTimeoutRetrySettings2) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *OutputGrafanaCloudTimeoutRetrySettings2) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *OutputGrafanaCloudTimeoutRetrySettings2) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *OutputGrafanaCloudTimeoutRetrySettings2) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *OutputGrafanaCloudTimeoutRetrySettings2) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// OutputGrafanaCloudBackpressureBehavior2 - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type OutputGrafanaCloudBackpressureBehavior2 string

const (
	OutputGrafanaCloudBackpressureBehavior2Block OutputGrafanaCloudBackpressureBehavior2 = "block"
	OutputGrafanaCloudBackpressureBehavior2Drop  OutputGrafanaCloudBackpressureBehavior2 = "drop"
	OutputGrafanaCloudBackpressureBehavior2Queue OutputGrafanaCloudBackpressureBehavior2 = "queue"
)

func (e OutputGrafanaCloudBackpressureBehavior2) ToPointer() *OutputGrafanaCloudBackpressureBehavior2 {
	return &e
}
func (e *OutputGrafanaCloudBackpressureBehavior2) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = OutputGrafanaCloudBackpressureBehavior2(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputGrafanaCloudBackpressureBehavior2: %v", v)
	}
}

// OutputGrafanaCloudCompression2 - Codec to use to compress the persisted data.
type OutputGrafanaCloudCompression2 string

const (
	OutputGrafanaCloudCompression2None OutputGrafanaCloudCompression2 = "none"
	OutputGrafanaCloudCompression2Gzip OutputGrafanaCloudCompression2 = "gzip"
)

func (e OutputGrafanaCloudCompression2) ToPointer() *OutputGrafanaCloudCompression2 {
	return &e
}
func (e *OutputGrafanaCloudCompression2) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = OutputGrafanaCloudCompression2(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputGrafanaCloudCompression2: %v", v)
	}
}

// OutputGrafanaCloudQueueFullBehavior2 - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type OutputGrafanaCloudQueueFullBehavior2 string

const (
	OutputGrafanaCloudQueueFullBehavior2Block OutputGrafanaCloudQueueFullBehavior2 = "block"
	OutputGrafanaCloudQueueFullBehavior2Drop  OutputGrafanaCloudQueueFullBehavior2 = "drop"
)

func (e OutputGrafanaCloudQueueFullBehavior2) ToPointer() *OutputGrafanaCloudQueueFullBehavior2 {
	return &e
}
func (e *OutputGrafanaCloudQueueFullBehavior2) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = OutputGrafanaCloudQueueFullBehavior2(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputGrafanaCloudQueueFullBehavior2: %v", v)
	}
}

// OutputGrafanaCloudMode2 - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type OutputGrafanaCloudMode2 string

const (
	OutputGrafanaCloudMode2Error        OutputGrafanaCloudMode2 = "error"
	OutputGrafanaCloudMode2Backpressure OutputGrafanaCloudMode2 = "backpressure"
	OutputGrafanaCloudMode2Always       OutputGrafanaCloudMode2 = "always"
)

func (e OutputGrafanaCloudMode2) ToPointer() *OutputGrafanaCloudMode2 {
	return &e
}
func (e *OutputGrafanaCloudMode2) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = OutputGrafanaCloudMode2(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputGrafanaCloudMode2: %v", v)
	}
}

type OutputGrafanaCloudPqControls2 struct {
}

type OutputGrafanaCloudGrafanaCloud2 struct {
	// Unique ID for this output
	ID   string                  `json:"id"`
	Type OutputGrafanaCloudType2 `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions and labels to generated metrics and logs, respectively.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The endpoint to send logs to, e.g.: https://logs-prod-us-central1.grafana.net
	LokiURL *string `json:"lokiUrl,omitempty"`
	// The remote_write endpoint to send Prometheus metrics to, e.g.: https://prometheus-blocks-prod-us-central1.grafana.net/api/prom/push
	PrometheusURL string `json:"prometheusUrl"`
	// Name of the event field that contains the message to send. If not specified, Stream sends a JSON representation of the whole event.
	Message *string `json:"message,omitempty"`
	// Which format to use when sending logs to Loki (Protobuf or JSON).  Defaults to Protobuf.
	MessageFormat *OutputGrafanaCloudMessageFormat2 `default:"protobuf" json:"messageFormat"`
	// List of labels to send with logs. Labels define Loki streams, so use static labels to avoid proliferating label value combinations and streams. Can be merged and/or overridden by the event's __labels field (e.g.: '__labels: {host: "cribl.io", level: "error"}').
	Labels []OutputGrafanaCloudLabel2 `json:"labels,omitempty"`
	// A JS expression that can be used to rename metrics. E.g.: name.replace(/\./g, '_') will replace all '.' characters in a metric's name with the supported '_' character. Use the 'name' global variable to access the metric's name.  You can access event fields' values via __e.<fieldName>.
	MetricRenameExpr *string                `default:"name.replace(/[^a-zA-Z0-9_]/g, '_')" json:"metricRenameExpr"`
	PrometheusAuth   *OutputPrometheusAuth2 `json:"prometheusAuth,omitempty"`
	LokiAuth         *OutputLokiAuth2       `json:"lokiAuth,omitempty"`
	// Maximum number of ongoing requests before blocking. Warning: Setting this value > 1 can cause Loki and Prometheus to complain about entries being delivered out of order.
	Concurrency *float64 `default:"1" json:"concurrency"`
	// Maximum size, in KB, of the request body. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited). Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Defaults to Yes. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Maximum time between requests. Small values can reduce the payload size below the configured 'Max record size' and 'Max events per request'. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.
	FlushPeriodSec *float64 `default:"15" json:"flushPeriodSec"`
	// Headers to add to all events.
	ExtraHTTPHeaders []OutputGrafanaCloudExtraHTTPHeader2 `json:"extraHttpHeaders,omitempty"`
	// Enables round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *OutputGrafanaCloudFailedRequestLoggingMode2 `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable).
	ResponseRetrySettings []OutputGrafanaCloudResponseRetrySetting2 `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *OutputGrafanaCloudTimeoutRetrySettings2  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *OutputGrafanaCloudBackpressureBehavior2 `default:"block" json:"onBackpressure"`
	Description    *string                                  `json:"description,omitempty"`
	// Whether to compress the payload body before sending. Applies only to Loki's JSON payloads, as both Prometheus' and Loki's Protobuf variant are snappy-compressed by default.
	Compress *bool `default:"true" json:"compress"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *OutputGrafanaCloudCompression2 `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *OutputGrafanaCloudQueueFullBehavior2 `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *OutputGrafanaCloudMode2       `default:"error" json:"pqMode"`
	PqControls *OutputGrafanaCloudPqControls2 `json:"pqControls,omitempty"`
	Status     *TFStatus                      `json:"status,omitempty"`
}

func (o OutputGrafanaCloudGrafanaCloud2) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGrafanaCloudGrafanaCloud2) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetType() OutputGrafanaCloudType2 {
	if o == nil {
		return OutputGrafanaCloudType2("")
	}
	return o.Type
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetLokiURL() *string {
	if o == nil {
		return nil
	}
	return o.LokiURL
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPrometheusURL() string {
	if o == nil {
		return ""
	}
	return o.PrometheusURL
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetMessage() *string {
	if o == nil {
		return nil
	}
	return o.Message
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetMessageFormat() *OutputGrafanaCloudMessageFormat2 {
	if o == nil {
		return nil
	}
	return o.MessageFormat
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetLabels() []OutputGrafanaCloudLabel2 {
	if o == nil {
		return nil
	}
	return o.Labels
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetMetricRenameExpr() *string {
	if o == nil {
		return nil
	}
	return o.MetricRenameExpr
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPrometheusAuth() *OutputPrometheusAuth2 {
	if o == nil {
		return nil
	}
	return o.PrometheusAuth
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetLokiAuth() *OutputLokiAuth2 {
	if o == nil {
		return nil
	}
	return o.LokiAuth
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetExtraHTTPHeaders() []OutputGrafanaCloudExtraHTTPHeader2 {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetFailedRequestLoggingMode() *OutputGrafanaCloudFailedRequestLoggingMode2 {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetResponseRetrySettings() []OutputGrafanaCloudResponseRetrySetting2 {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetTimeoutRetrySettings() *OutputGrafanaCloudTimeoutRetrySettings2 {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetOnBackpressure() *OutputGrafanaCloudBackpressureBehavior2 {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPqCompress() *OutputGrafanaCloudCompression2 {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPqOnBackpressure() *OutputGrafanaCloudQueueFullBehavior2 {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPqMode() *OutputGrafanaCloudMode2 {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetPqControls() *OutputGrafanaCloudPqControls2 {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputGrafanaCloudGrafanaCloud2) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type OutputGrafanaCloudType1 string

const (
	OutputGrafanaCloudType1GrafanaCloud OutputGrafanaCloudType1 = "grafana_cloud"
)

func (e OutputGrafanaCloudType1) ToPointer() *OutputGrafanaCloudType1 {
	return &e
}
func (e *OutputGrafanaCloudType1) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "grafana_cloud":
		*e = OutputGrafanaCloudType1(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputGrafanaCloudType1: %v", v)
	}
}

// OutputGrafanaCloudMessageFormat1 - Which format to use when sending logs to Loki (Protobuf or JSON).  Defaults to Protobuf.
type OutputGrafanaCloudMessageFormat1 string

const (
	OutputGrafanaCloudMessageFormat1Protobuf OutputGrafanaCloudMessageFormat1 = "protobuf"
	OutputGrafanaCloudMessageFormat1JSON     OutputGrafanaCloudMessageFormat1 = "json"
)

func (e OutputGrafanaCloudMessageFormat1) ToPointer() *OutputGrafanaCloudMessageFormat1 {
	return &e
}
func (e *OutputGrafanaCloudMessageFormat1) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "protobuf":
		fallthrough
	case "json":
		*e = OutputGrafanaCloudMessageFormat1(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputGrafanaCloudMessageFormat1: %v", v)
	}
}

type OutputGrafanaCloudLabel1 struct {
	// Name of the label.
	Name *string `default:"" json:"name"`
	// Value of the label.
	Value string `json:"value"`
}

func (o OutputGrafanaCloudLabel1) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGrafanaCloudLabel1) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *OutputGrafanaCloudLabel1) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *OutputGrafanaCloudLabel1) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// OutputGrafanaCloudPrometheusAuthAuthenticationType1 - The authentication method to use for the HTTP requests
type OutputGrafanaCloudPrometheusAuthAuthenticationType1 string

const (
	OutputGrafanaCloudPrometheusAuthAuthenticationType1None              OutputGrafanaCloudPrometheusAuthAuthenticationType1 = "none"
	OutputGrafanaCloudPrometheusAuthAuthenticationType1Token             OutputGrafanaCloudPrometheusAuthAuthenticationType1 = "token"
	OutputGrafanaCloudPrometheusAuthAuthenticationType1TextSecret        OutputGrafanaCloudPrometheusAuthAuthenticationType1 = "textSecret"
	OutputGrafanaCloudPrometheusAuthAuthenticationType1Basic             OutputGrafanaCloudPrometheusAuthAuthenticationType1 = "basic"
	OutputGrafanaCloudPrometheusAuthAuthenticationType1CredentialsSecret OutputGrafanaCloudPrometheusAuthAuthenticationType1 = "credentialsSecret"
)

func (e OutputGrafanaCloudPrometheusAuthAuthenticationType1) ToPointer() *OutputGrafanaCloudPrometheusAuthAuthenticationType1 {
	return &e
}
func (e *OutputGrafanaCloudPrometheusAuthAuthenticationType1) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "token":
		fallthrough
	case "textSecret":
		fallthrough
	case "basic":
		fallthrough
	case "credentialsSecret":
		*e = OutputGrafanaCloudPrometheusAuthAuthenticationType1(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputGrafanaCloudPrometheusAuthAuthenticationType1: %v", v)
	}
}

type OutputPrometheusAuth1 struct {
	// The authentication method to use for the HTTP requests
	AuthType *OutputGrafanaCloudPrometheusAuthAuthenticationType1 `default:"basic" json:"authType"`
	// Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. E.g.: <your-username>:<your-api-key>.
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// Username for authentication
	Username *string `json:"username,omitempty"`
	// Password (a.k.a API key in Grafana Cloud domain) for authentication
	Password *string `json:"password,omitempty"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
}

func (o OutputPrometheusAuth1) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputPrometheusAuth1) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *OutputPrometheusAuth1) GetAuthType() *OutputGrafanaCloudPrometheusAuthAuthenticationType1 {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputPrometheusAuth1) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputPrometheusAuth1) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputPrometheusAuth1) GetUsername() *string {
	if o == nil {
		return nil
	}
	return o.Username
}

func (o *OutputPrometheusAuth1) GetPassword() *string {
	if o == nil {
		return nil
	}
	return o.Password
}

func (o *OutputPrometheusAuth1) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

// OutputGrafanaCloudLokiAuthAuthenticationType1 - The authentication method to use for the HTTP requests
type OutputGrafanaCloudLokiAuthAuthenticationType1 string

const (
	OutputGrafanaCloudLokiAuthAuthenticationType1None              OutputGrafanaCloudLokiAuthAuthenticationType1 = "none"
	OutputGrafanaCloudLokiAuthAuthenticationType1Token             OutputGrafanaCloudLokiAuthAuthenticationType1 = "token"
	OutputGrafanaCloudLokiAuthAuthenticationType1TextSecret        OutputGrafanaCloudLokiAuthAuthenticationType1 = "textSecret"
	OutputGrafanaCloudLokiAuthAuthenticationType1Basic             OutputGrafanaCloudLokiAuthAuthenticationType1 = "basic"
	OutputGrafanaCloudLokiAuthAuthenticationType1CredentialsSecret OutputGrafanaCloudLokiAuthAuthenticationType1 = "credentialsSecret"
)

func (e OutputGrafanaCloudLokiAuthAuthenticationType1) ToPointer() *OutputGrafanaCloudLokiAuthAuthenticationType1 {
	return &e
}
func (e *OutputGrafanaCloudLokiAuthAuthenticationType1) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "token":
		fallthrough
	case "textSecret":
		fallthrough
	case "basic":
		fallthrough
	case "credentialsSecret":
		*e = OutputGrafanaCloudLokiAuthAuthenticationType1(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputGrafanaCloudLokiAuthAuthenticationType1: %v", v)
	}
}

type OutputLokiAuth1 struct {
	// The authentication method to use for the HTTP requests
	AuthType *OutputGrafanaCloudLokiAuthAuthenticationType1 `default:"basic" json:"authType"`
	// Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. E.g.: <your-username>:<your-api-key>.
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// Username for authentication
	Username *string `json:"username,omitempty"`
	// Password (a.k.a API key in Grafana Cloud domain) for authentication
	Password *string `json:"password,omitempty"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
}

func (o OutputLokiAuth1) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputLokiAuth1) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *OutputLokiAuth1) GetAuthType() *OutputGrafanaCloudLokiAuthAuthenticationType1 {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputLokiAuth1) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputLokiAuth1) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputLokiAuth1) GetUsername() *string {
	if o == nil {
		return nil
	}
	return o.Username
}

func (o *OutputLokiAuth1) GetPassword() *string {
	if o == nil {
		return nil
	}
	return o.Password
}

func (o *OutputLokiAuth1) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

type OutputGrafanaCloudExtraHTTPHeader1 struct {
	// Field name
	Name *string `json:"name,omitempty"`
	// Field value
	Value string `json:"value"`
}

func (o *OutputGrafanaCloudExtraHTTPHeader1) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *OutputGrafanaCloudExtraHTTPHeader1) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// OutputGrafanaCloudFailedRequestLoggingMode1 - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type OutputGrafanaCloudFailedRequestLoggingMode1 string

const (
	OutputGrafanaCloudFailedRequestLoggingMode1Payload           OutputGrafanaCloudFailedRequestLoggingMode1 = "payload"
	OutputGrafanaCloudFailedRequestLoggingMode1PayloadAndHeaders OutputGrafanaCloudFailedRequestLoggingMode1 = "payloadAndHeaders"
	OutputGrafanaCloudFailedRequestLoggingMode1None              OutputGrafanaCloudFailedRequestLoggingMode1 = "none"
)

func (e OutputGrafanaCloudFailedRequestLoggingMode1) ToPointer() *OutputGrafanaCloudFailedRequestLoggingMode1 {
	return &e
}
func (e *OutputGrafanaCloudFailedRequestLoggingMode1) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = OutputGrafanaCloudFailedRequestLoggingMode1(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputGrafanaCloudFailedRequestLoggingMode1: %v", v)
	}
}

type OutputGrafanaCloudResponseRetrySetting1 struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (o OutputGrafanaCloudResponseRetrySetting1) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGrafanaCloudResponseRetrySetting1) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *OutputGrafanaCloudResponseRetrySetting1) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *OutputGrafanaCloudResponseRetrySetting1) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *OutputGrafanaCloudResponseRetrySetting1) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *OutputGrafanaCloudResponseRetrySetting1) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type OutputGrafanaCloudTimeoutRetrySettings1 struct {
	// Enable to retry on request timeout
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (o OutputGrafanaCloudTimeoutRetrySettings1) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGrafanaCloudTimeoutRetrySettings1) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *OutputGrafanaCloudTimeoutRetrySettings1) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *OutputGrafanaCloudTimeoutRetrySettings1) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *OutputGrafanaCloudTimeoutRetrySettings1) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *OutputGrafanaCloudTimeoutRetrySettings1) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// OutputGrafanaCloudBackpressureBehavior1 - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type OutputGrafanaCloudBackpressureBehavior1 string

const (
	OutputGrafanaCloudBackpressureBehavior1Block OutputGrafanaCloudBackpressureBehavior1 = "block"
	OutputGrafanaCloudBackpressureBehavior1Drop  OutputGrafanaCloudBackpressureBehavior1 = "drop"
	OutputGrafanaCloudBackpressureBehavior1Queue OutputGrafanaCloudBackpressureBehavior1 = "queue"
)

func (e OutputGrafanaCloudBackpressureBehavior1) ToPointer() *OutputGrafanaCloudBackpressureBehavior1 {
	return &e
}
func (e *OutputGrafanaCloudBackpressureBehavior1) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = OutputGrafanaCloudBackpressureBehavior1(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputGrafanaCloudBackpressureBehavior1: %v", v)
	}
}

// OutputGrafanaCloudCompression1 - Codec to use to compress the persisted data.
type OutputGrafanaCloudCompression1 string

const (
	OutputGrafanaCloudCompression1None OutputGrafanaCloudCompression1 = "none"
	OutputGrafanaCloudCompression1Gzip OutputGrafanaCloudCompression1 = "gzip"
)

func (e OutputGrafanaCloudCompression1) ToPointer() *OutputGrafanaCloudCompression1 {
	return &e
}
func (e *OutputGrafanaCloudCompression1) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = OutputGrafanaCloudCompression1(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputGrafanaCloudCompression1: %v", v)
	}
}

// OutputGrafanaCloudQueueFullBehavior1 - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type OutputGrafanaCloudQueueFullBehavior1 string

const (
	OutputGrafanaCloudQueueFullBehavior1Block OutputGrafanaCloudQueueFullBehavior1 = "block"
	OutputGrafanaCloudQueueFullBehavior1Drop  OutputGrafanaCloudQueueFullBehavior1 = "drop"
)

func (e OutputGrafanaCloudQueueFullBehavior1) ToPointer() *OutputGrafanaCloudQueueFullBehavior1 {
	return &e
}
func (e *OutputGrafanaCloudQueueFullBehavior1) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = OutputGrafanaCloudQueueFullBehavior1(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputGrafanaCloudQueueFullBehavior1: %v", v)
	}
}

// OutputGrafanaCloudMode1 - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type OutputGrafanaCloudMode1 string

const (
	OutputGrafanaCloudMode1Error        OutputGrafanaCloudMode1 = "error"
	OutputGrafanaCloudMode1Backpressure OutputGrafanaCloudMode1 = "backpressure"
	OutputGrafanaCloudMode1Always       OutputGrafanaCloudMode1 = "always"
)

func (e OutputGrafanaCloudMode1) ToPointer() *OutputGrafanaCloudMode1 {
	return &e
}
func (e *OutputGrafanaCloudMode1) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = OutputGrafanaCloudMode1(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputGrafanaCloudMode1: %v", v)
	}
}

type OutputGrafanaCloudPqControls1 struct {
}

type OutputGrafanaCloudGrafanaCloud1 struct {
	// Unique ID for this output
	ID   string                  `json:"id"`
	Type OutputGrafanaCloudType1 `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions and labels to generated metrics and logs, respectively.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The endpoint to send logs to, e.g.: https://logs-prod-us-central1.grafana.net
	LokiURL string `json:"lokiUrl"`
	// The remote_write endpoint to send Prometheus metrics to, e.g.: https://prometheus-blocks-prod-us-central1.grafana.net/api/prom/push
	PrometheusURL *string `json:"prometheusUrl,omitempty"`
	// Name of the event field that contains the message to send. If not specified, Stream sends a JSON representation of the whole event.
	Message *string `json:"message,omitempty"`
	// Which format to use when sending logs to Loki (Protobuf or JSON).  Defaults to Protobuf.
	MessageFormat *OutputGrafanaCloudMessageFormat1 `default:"protobuf" json:"messageFormat"`
	// List of labels to send with logs. Labels define Loki streams, so use static labels to avoid proliferating label value combinations and streams. Can be merged and/or overridden by the event's __labels field (e.g.: '__labels: {host: "cribl.io", level: "error"}').
	Labels []OutputGrafanaCloudLabel1 `json:"labels,omitempty"`
	// A JS expression that can be used to rename metrics. E.g.: name.replace(/\./g, '_') will replace all '.' characters in a metric's name with the supported '_' character. Use the 'name' global variable to access the metric's name.  You can access event fields' values via __e.<fieldName>.
	MetricRenameExpr *string                `default:"name.replace(/[^a-zA-Z0-9_]/g, '_')" json:"metricRenameExpr"`
	PrometheusAuth   *OutputPrometheusAuth1 `json:"prometheusAuth,omitempty"`
	LokiAuth         *OutputLokiAuth1       `json:"lokiAuth,omitempty"`
	// Maximum number of ongoing requests before blocking. Warning: Setting this value > 1 can cause Loki and Prometheus to complain about entries being delivered out of order.
	Concurrency *float64 `default:"1" json:"concurrency"`
	// Maximum size, in KB, of the request body. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited). Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Defaults to Yes. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Maximum time between requests. Small values can reduce the payload size below the configured 'Max record size' and 'Max events per request'. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.
	FlushPeriodSec *float64 `default:"15" json:"flushPeriodSec"`
	// Headers to add to all events.
	ExtraHTTPHeaders []OutputGrafanaCloudExtraHTTPHeader1 `json:"extraHttpHeaders,omitempty"`
	// Enables round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *OutputGrafanaCloudFailedRequestLoggingMode1 `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable).
	ResponseRetrySettings []OutputGrafanaCloudResponseRetrySetting1 `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *OutputGrafanaCloudTimeoutRetrySettings1  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *OutputGrafanaCloudBackpressureBehavior1 `default:"block" json:"onBackpressure"`
	Description    *string                                  `json:"description,omitempty"`
	// Whether to compress the payload body before sending. Applies only to Loki's JSON payloads, as both Prometheus' and Loki's Protobuf variant are snappy-compressed by default.
	Compress *bool `default:"true" json:"compress"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *OutputGrafanaCloudCompression1 `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *OutputGrafanaCloudQueueFullBehavior1 `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *OutputGrafanaCloudMode1       `default:"error" json:"pqMode"`
	PqControls *OutputGrafanaCloudPqControls1 `json:"pqControls,omitempty"`
	Status     *TFStatus                      `json:"status,omitempty"`
}

func (o OutputGrafanaCloudGrafanaCloud1) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGrafanaCloudGrafanaCloud1) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetType() OutputGrafanaCloudType1 {
	if o == nil {
		return OutputGrafanaCloudType1("")
	}
	return o.Type
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetLokiURL() string {
	if o == nil {
		return ""
	}
	return o.LokiURL
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPrometheusURL() *string {
	if o == nil {
		return nil
	}
	return o.PrometheusURL
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetMessage() *string {
	if o == nil {
		return nil
	}
	return o.Message
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetMessageFormat() *OutputGrafanaCloudMessageFormat1 {
	if o == nil {
		return nil
	}
	return o.MessageFormat
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetLabels() []OutputGrafanaCloudLabel1 {
	if o == nil {
		return nil
	}
	return o.Labels
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetMetricRenameExpr() *string {
	if o == nil {
		return nil
	}
	return o.MetricRenameExpr
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPrometheusAuth() *OutputPrometheusAuth1 {
	if o == nil {
		return nil
	}
	return o.PrometheusAuth
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetLokiAuth() *OutputLokiAuth1 {
	if o == nil {
		return nil
	}
	return o.LokiAuth
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetExtraHTTPHeaders() []OutputGrafanaCloudExtraHTTPHeader1 {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetFailedRequestLoggingMode() *OutputGrafanaCloudFailedRequestLoggingMode1 {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetResponseRetrySettings() []OutputGrafanaCloudResponseRetrySetting1 {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetTimeoutRetrySettings() *OutputGrafanaCloudTimeoutRetrySettings1 {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetOnBackpressure() *OutputGrafanaCloudBackpressureBehavior1 {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPqCompress() *OutputGrafanaCloudCompression1 {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPqOnBackpressure() *OutputGrafanaCloudQueueFullBehavior1 {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPqMode() *OutputGrafanaCloudMode1 {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetPqControls() *OutputGrafanaCloudPqControls1 {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputGrafanaCloudGrafanaCloud1) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type OutputGrafanaCloudType string

const (
	OutputGrafanaCloudTypeOutputGrafanaCloudGrafanaCloud1 OutputGrafanaCloudType = "OutputGrafanaCloud_GrafanaCloud_1"
	OutputGrafanaCloudTypeOutputGrafanaCloudGrafanaCloud2 OutputGrafanaCloudType = "OutputGrafanaCloud_GrafanaCloud_2"
)

type OutputGrafanaCloud struct {
	OutputGrafanaCloudGrafanaCloud1 *OutputGrafanaCloudGrafanaCloud1 `queryParam:"inline"`
	OutputGrafanaCloudGrafanaCloud2 *OutputGrafanaCloudGrafanaCloud2 `queryParam:"inline"`

	Type OutputGrafanaCloudType
}

func CreateOutputGrafanaCloudOutputGrafanaCloudGrafanaCloud1(outputGrafanaCloudGrafanaCloud1 OutputGrafanaCloudGrafanaCloud1) OutputGrafanaCloud {
	typ := OutputGrafanaCloudTypeOutputGrafanaCloudGrafanaCloud1

	return OutputGrafanaCloud{
		OutputGrafanaCloudGrafanaCloud1: &outputGrafanaCloudGrafanaCloud1,
		Type:                            typ,
	}
}

func CreateOutputGrafanaCloudOutputGrafanaCloudGrafanaCloud2(outputGrafanaCloudGrafanaCloud2 OutputGrafanaCloudGrafanaCloud2) OutputGrafanaCloud {
	typ := OutputGrafanaCloudTypeOutputGrafanaCloudGrafanaCloud2

	return OutputGrafanaCloud{
		OutputGrafanaCloudGrafanaCloud2: &outputGrafanaCloudGrafanaCloud2,
		Type:                            typ,
	}
}

func (u *OutputGrafanaCloud) UnmarshalJSON(data []byte) error {

	var outputGrafanaCloudGrafanaCloud1 OutputGrafanaCloudGrafanaCloud1 = OutputGrafanaCloudGrafanaCloud1{}
	if err := utils.UnmarshalJSON(data, &outputGrafanaCloudGrafanaCloud1, "", true, true); err == nil {
		u.OutputGrafanaCloudGrafanaCloud1 = &outputGrafanaCloudGrafanaCloud1
		u.Type = OutputGrafanaCloudTypeOutputGrafanaCloudGrafanaCloud1
		return nil
	}

	var outputGrafanaCloudGrafanaCloud2 OutputGrafanaCloudGrafanaCloud2 = OutputGrafanaCloudGrafanaCloud2{}
	if err := utils.UnmarshalJSON(data, &outputGrafanaCloudGrafanaCloud2, "", true, true); err == nil {
		u.OutputGrafanaCloudGrafanaCloud2 = &outputGrafanaCloudGrafanaCloud2
		u.Type = OutputGrafanaCloudTypeOutputGrafanaCloudGrafanaCloud2
		return nil
	}

	return fmt.Errorf("could not unmarshal `%s` into any supported union types for OutputGrafanaCloud", string(data))
}

func (u OutputGrafanaCloud) MarshalJSON() ([]byte, error) {
	if u.OutputGrafanaCloudGrafanaCloud1 != nil {
		return utils.MarshalJSON(u.OutputGrafanaCloudGrafanaCloud1, "", true)
	}

	if u.OutputGrafanaCloudGrafanaCloud2 != nil {
		return utils.MarshalJSON(u.OutputGrafanaCloudGrafanaCloud2, "", true)
	}

	return nil, errors.New("could not marshal union type OutputGrafanaCloud: all fields are null")
}

type TypeDatadog string

const (
	TypeDatadogDatadog TypeDatadog = "datadog"
)

func (e TypeDatadog) ToPointer() *TypeDatadog {
	return &e
}
func (e *TypeDatadog) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "datadog":
		*e = TypeDatadog(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeDatadog: %v", v)
	}
}

// SendLogsAs - The content type to use when sending logs.
type SendLogsAs string

const (
	SendLogsAsText SendLogsAs = "text"
	SendLogsAsJSON SendLogsAs = "json"
)

func (e SendLogsAs) ToPointer() *SendLogsAs {
	return &e
}
func (e *SendLogsAs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "text":
		fallthrough
	case "json":
		*e = SendLogsAs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SendLogsAs: %v", v)
	}
}

// SeverityDatadog - Default value for message severity. When you send logs as JSON objects, the event's '__severity' field (if set) will override this value.
type SeverityDatadog string

const (
	SeverityDatadogEmergency SeverityDatadog = "emergency"
	SeverityDatadogAlert     SeverityDatadog = "alert"
	SeverityDatadogCritical  SeverityDatadog = "critical"
	SeverityDatadogError     SeverityDatadog = "error"
	SeverityDatadogWarning   SeverityDatadog = "warning"
	SeverityDatadogNotice    SeverityDatadog = "notice"
	SeverityDatadogInfo      SeverityDatadog = "info"
	SeverityDatadogDebug     SeverityDatadog = "debug"
)

func (e SeverityDatadog) ToPointer() *SeverityDatadog {
	return &e
}
func (e *SeverityDatadog) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "emergency":
		fallthrough
	case "alert":
		fallthrough
	case "critical":
		fallthrough
	case "error":
		fallthrough
	case "warning":
		fallthrough
	case "notice":
		fallthrough
	case "info":
		fallthrough
	case "debug":
		*e = SeverityDatadog(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SeverityDatadog: %v", v)
	}
}

// DatadogSite - Datadog site to which events should be sent
type DatadogSite string

const (
	DatadogSiteUs     DatadogSite = "us"
	DatadogSiteUs3    DatadogSite = "us3"
	DatadogSiteUs5    DatadogSite = "us5"
	DatadogSiteEu     DatadogSite = "eu"
	DatadogSiteFed1   DatadogSite = "fed1"
	DatadogSiteAp1    DatadogSite = "ap1"
	DatadogSiteCustom DatadogSite = "custom"
)

func (e DatadogSite) ToPointer() *DatadogSite {
	return &e
}
func (e *DatadogSite) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "us":
		fallthrough
	case "us3":
		fallthrough
	case "us5":
		fallthrough
	case "eu":
		fallthrough
	case "fed1":
		fallthrough
	case "ap1":
		fallthrough
	case "custom":
		*e = DatadogSite(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DatadogSite: %v", v)
	}
}

type ExtraHTTPHeaderDatadog struct {
	// Field name
	Name *string `json:"name,omitempty"`
	// Field value
	Value string `json:"value"`
}

func (o *ExtraHTTPHeaderDatadog) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderDatadog) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeDatadog - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeDatadog string

const (
	FailedRequestLoggingModeDatadogPayload           FailedRequestLoggingModeDatadog = "payload"
	FailedRequestLoggingModeDatadogPayloadAndHeaders FailedRequestLoggingModeDatadog = "payloadAndHeaders"
	FailedRequestLoggingModeDatadogNone              FailedRequestLoggingModeDatadog = "none"
)

func (e FailedRequestLoggingModeDatadog) ToPointer() *FailedRequestLoggingModeDatadog {
	return &e
}
func (e *FailedRequestLoggingModeDatadog) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeDatadog(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeDatadog: %v", v)
	}
}

type ResponseRetrySettingDatadog struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingDatadog) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingDatadog) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingDatadog) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingDatadog) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingDatadog) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingDatadog) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsDatadog struct {
	// Enable to retry on request timeout
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsDatadog) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsDatadog) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsDatadog) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsDatadog) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsDatadog) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsDatadog) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorDatadog - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorDatadog string

const (
	BackpressureBehaviorDatadogBlock BackpressureBehaviorDatadog = "block"
	BackpressureBehaviorDatadogDrop  BackpressureBehaviorDatadog = "drop"
	BackpressureBehaviorDatadogQueue BackpressureBehaviorDatadog = "queue"
)

func (e BackpressureBehaviorDatadog) ToPointer() *BackpressureBehaviorDatadog {
	return &e
}
func (e *BackpressureBehaviorDatadog) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorDatadog(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorDatadog: %v", v)
	}
}

// AuthenticationMethodDatadog - Enter API key directly, or select a stored secret
type AuthenticationMethodDatadog string

const (
	AuthenticationMethodDatadogManual AuthenticationMethodDatadog = "manual"
	AuthenticationMethodDatadogSecret AuthenticationMethodDatadog = "secret"
)

func (e AuthenticationMethodDatadog) ToPointer() *AuthenticationMethodDatadog {
	return &e
}
func (e *AuthenticationMethodDatadog) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		*e = AuthenticationMethodDatadog(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodDatadog: %v", v)
	}
}

// CompressionDatadog - Codec to use to compress the persisted data.
type CompressionDatadog string

const (
	CompressionDatadogNone CompressionDatadog = "none"
	CompressionDatadogGzip CompressionDatadog = "gzip"
)

func (e CompressionDatadog) ToPointer() *CompressionDatadog {
	return &e
}
func (e *CompressionDatadog) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionDatadog(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionDatadog: %v", v)
	}
}

// QueueFullBehaviorDatadog - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorDatadog string

const (
	QueueFullBehaviorDatadogBlock QueueFullBehaviorDatadog = "block"
	QueueFullBehaviorDatadogDrop  QueueFullBehaviorDatadog = "drop"
)

func (e QueueFullBehaviorDatadog) ToPointer() *QueueFullBehaviorDatadog {
	return &e
}
func (e *QueueFullBehaviorDatadog) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorDatadog(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorDatadog: %v", v)
	}
}

// ModeDatadog - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type ModeDatadog string

const (
	ModeDatadogError        ModeDatadog = "error"
	ModeDatadogBackpressure ModeDatadog = "backpressure"
	ModeDatadogAlways       ModeDatadog = "always"
)

func (e ModeDatadog) ToPointer() *ModeDatadog {
	return &e
}
func (e *ModeDatadog) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeDatadog(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeDatadog: %v", v)
	}
}

type PqControlsDatadog struct {
}

type OutputDatadog struct {
	// Unique ID for this output
	ID   string      `json:"id"`
	Type TypeDatadog `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The content type to use when sending logs.
	ContentType *SendLogsAs `default:"json" json:"contentType"`
	// Name of the event field that contains the message to send. If not specified, Stream sends a JSON representation of the whole event.
	Message *string `json:"message,omitempty"`
	// Name of the source to send with logs. When you send logs as JSON objects, the event's 'source' field (if set) will override this value.
	Source *string `json:"source,omitempty"`
	// Name of the host to send with logs. When you send logs as JSON objects, the event's 'host' field (if set) will override this value.
	Host *string `json:"host,omitempty"`
	// Name of the service to send with logs. When you send logs as JSON objects, the event's '__service' field (if set) will override this value.
	Service *string `json:"service,omitempty"`
	// List of tags to send with logs (e.g., 'env:prod', 'env_staging:east').
	Tags []string `json:"tags,omitempty"`
	// When enabled, batches events by API key and the ddtags field on the event. When disabled, batches events only by API key. If incoming events have high cardinality in the ddtags field, disabling this setting may improve Destination performance.
	BatchByTags *bool `default:"true" json:"batchByTags"`
	// If enabled, the API key can be set from the event's '__agent_api_key' field.
	AllowAPIKeyFromEvents *bool `default:"false" json:"allowApiKeyFromEvents"`
	// Default value for message severity. When you send logs as JSON objects, the event's '__severity' field (if set) will override this value.
	Severity *SeverityDatadog `json:"severity,omitempty"`
	// Datadog site to which events should be sent
	Site *DatadogSite `default:"us" json:"site"`
	// If not enabled, Datadog will transform 'counter' metrics to 'gauge'. [Learn more about Datadog metrics types.](https://docs.datadoghq.com/metrics/types/?tab=count)
	SendCountersAsCount *bool `default:"false" json:"sendCountersAsCount"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Defaults to Yes. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max body size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events.
	ExtraHTTPHeaders []ExtraHTTPHeaderDatadog `json:"extraHttpHeaders,omitempty"`
	// Enables round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeDatadog `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable).
	ResponseRetrySettings []ResponseRetrySettingDatadog `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsDatadog  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorDatadog `default:"block" json:"onBackpressure"`
	// Enter API key directly, or select a stored secret
	AuthType *AuthenticationMethodDatadog `default:"manual" json:"authType"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	Description        *string  `json:"description,omitempty"`
	CustomURL          *string  `json:"customUrl,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *CompressionDatadog `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorDatadog `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeDatadog       `default:"error" json:"pqMode"`
	PqControls *PqControlsDatadog `json:"pqControls,omitempty"`
	// Organization's API key in Datadog
	APIKey *string `json:"apiKey,omitempty"`
	// Select or create a stored text secret
	TextSecret *string   `json:"textSecret,omitempty"`
	Status     *TFStatus `json:"status,omitempty"`
}

func (o OutputDatadog) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputDatadog) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputDatadog) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputDatadog) GetType() TypeDatadog {
	if o == nil {
		return TypeDatadog("")
	}
	return o.Type
}

func (o *OutputDatadog) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDatadog) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDatadog) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDatadog) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputDatadog) GetContentType() *SendLogsAs {
	if o == nil {
		return nil
	}
	return o.ContentType
}

func (o *OutputDatadog) GetMessage() *string {
	if o == nil {
		return nil
	}
	return o.Message
}

func (o *OutputDatadog) GetSource() *string {
	if o == nil {
		return nil
	}
	return o.Source
}

func (o *OutputDatadog) GetHost() *string {
	if o == nil {
		return nil
	}
	return o.Host
}

func (o *OutputDatadog) GetService() *string {
	if o == nil {
		return nil
	}
	return o.Service
}

func (o *OutputDatadog) GetTags() []string {
	if o == nil {
		return nil
	}
	return o.Tags
}

func (o *OutputDatadog) GetBatchByTags() *bool {
	if o == nil {
		return nil
	}
	return o.BatchByTags
}

func (o *OutputDatadog) GetAllowAPIKeyFromEvents() *bool {
	if o == nil {
		return nil
	}
	return o.AllowAPIKeyFromEvents
}

func (o *OutputDatadog) GetSeverity() *SeverityDatadog {
	if o == nil {
		return nil
	}
	return o.Severity
}

func (o *OutputDatadog) GetSite() *DatadogSite {
	if o == nil {
		return nil
	}
	return o.Site
}

func (o *OutputDatadog) GetSendCountersAsCount() *bool {
	if o == nil {
		return nil
	}
	return o.SendCountersAsCount
}

func (o *OutputDatadog) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputDatadog) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputDatadog) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputDatadog) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputDatadog) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputDatadog) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputDatadog) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputDatadog) GetExtraHTTPHeaders() []ExtraHTTPHeaderDatadog {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputDatadog) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputDatadog) GetFailedRequestLoggingMode() *FailedRequestLoggingModeDatadog {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputDatadog) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputDatadog) GetResponseRetrySettings() []ResponseRetrySettingDatadog {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputDatadog) GetTimeoutRetrySettings() *TimeoutRetrySettingsDatadog {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputDatadog) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputDatadog) GetOnBackpressure() *BackpressureBehaviorDatadog {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputDatadog) GetAuthType() *AuthenticationMethodDatadog {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputDatadog) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputDatadog) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputDatadog) GetCustomURL() *string {
	if o == nil {
		return nil
	}
	return o.CustomURL
}

func (o *OutputDatadog) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputDatadog) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputDatadog) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputDatadog) GetPqCompress() *CompressionDatadog {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputDatadog) GetPqOnBackpressure() *QueueFullBehaviorDatadog {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputDatadog) GetPqMode() *ModeDatadog {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputDatadog) GetPqControls() *PqControlsDatadog {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputDatadog) GetAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.APIKey
}

func (o *OutputDatadog) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputDatadog) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeSumoLogic string

const (
	TypeSumoLogicSumoLogic TypeSumoLogic = "sumo_logic"
)

func (e TypeSumoLogic) ToPointer() *TypeSumoLogic {
	return &e
}
func (e *TypeSumoLogic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "sumo_logic":
		*e = TypeSumoLogic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeSumoLogic: %v", v)
	}
}

// DataFormatSumoLogic - Optionally, preserve the raw event format instead of json-ifying it.
type DataFormatSumoLogic string

const (
	DataFormatSumoLogicJSON DataFormatSumoLogic = "json"
	DataFormatSumoLogicRaw  DataFormatSumoLogic = "raw"
)

func (e DataFormatSumoLogic) ToPointer() *DataFormatSumoLogic {
	return &e
}
func (e *DataFormatSumoLogic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "json":
		fallthrough
	case "raw":
		*e = DataFormatSumoLogic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DataFormatSumoLogic: %v", v)
	}
}

type ExtraHTTPHeaderSumoLogic struct {
	// Field name
	Name *string `json:"name,omitempty"`
	// Field value
	Value string `json:"value"`
}

func (o *ExtraHTTPHeaderSumoLogic) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderSumoLogic) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeSumoLogic - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeSumoLogic string

const (
	FailedRequestLoggingModeSumoLogicPayload           FailedRequestLoggingModeSumoLogic = "payload"
	FailedRequestLoggingModeSumoLogicPayloadAndHeaders FailedRequestLoggingModeSumoLogic = "payloadAndHeaders"
	FailedRequestLoggingModeSumoLogicNone              FailedRequestLoggingModeSumoLogic = "none"
)

func (e FailedRequestLoggingModeSumoLogic) ToPointer() *FailedRequestLoggingModeSumoLogic {
	return &e
}
func (e *FailedRequestLoggingModeSumoLogic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeSumoLogic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeSumoLogic: %v", v)
	}
}

type ResponseRetrySettingSumoLogic struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingSumoLogic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingSumoLogic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingSumoLogic) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingSumoLogic) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingSumoLogic) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingSumoLogic) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsSumoLogic struct {
	// Enable to retry on request timeout
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsSumoLogic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsSumoLogic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsSumoLogic) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsSumoLogic) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsSumoLogic) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsSumoLogic) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorSumoLogic - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorSumoLogic string

const (
	BackpressureBehaviorSumoLogicBlock BackpressureBehaviorSumoLogic = "block"
	BackpressureBehaviorSumoLogicDrop  BackpressureBehaviorSumoLogic = "drop"
	BackpressureBehaviorSumoLogicQueue BackpressureBehaviorSumoLogic = "queue"
)

func (e BackpressureBehaviorSumoLogic) ToPointer() *BackpressureBehaviorSumoLogic {
	return &e
}
func (e *BackpressureBehaviorSumoLogic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorSumoLogic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorSumoLogic: %v", v)
	}
}

// CompressionSumoLogic - Codec to use to compress the persisted data.
type CompressionSumoLogic string

const (
	CompressionSumoLogicNone CompressionSumoLogic = "none"
	CompressionSumoLogicGzip CompressionSumoLogic = "gzip"
)

func (e CompressionSumoLogic) ToPointer() *CompressionSumoLogic {
	return &e
}
func (e *CompressionSumoLogic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionSumoLogic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionSumoLogic: %v", v)
	}
}

// QueueFullBehaviorSumoLogic - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSumoLogic string

const (
	QueueFullBehaviorSumoLogicBlock QueueFullBehaviorSumoLogic = "block"
	QueueFullBehaviorSumoLogicDrop  QueueFullBehaviorSumoLogic = "drop"
)

func (e QueueFullBehaviorSumoLogic) ToPointer() *QueueFullBehaviorSumoLogic {
	return &e
}
func (e *QueueFullBehaviorSumoLogic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorSumoLogic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorSumoLogic: %v", v)
	}
}

// ModeSumoLogic - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type ModeSumoLogic string

const (
	ModeSumoLogicError        ModeSumoLogic = "error"
	ModeSumoLogicBackpressure ModeSumoLogic = "backpressure"
	ModeSumoLogicAlways       ModeSumoLogic = "always"
)

func (e ModeSumoLogic) ToPointer() *ModeSumoLogic {
	return &e
}
func (e *ModeSumoLogic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeSumoLogic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeSumoLogic: %v", v)
	}
}

type PqControlsSumoLogic struct {
}

type OutputSumoLogic struct {
	// Unique ID for this output
	ID   *string       `json:"id,omitempty"`
	Type TypeSumoLogic `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Sumo Logic HTTP collector URL to which events should be sent.
	URL string `json:"url"`
	// Optionally, override the source name configured on the Sumo Logic HTTP collector. This can also be overridden at the event level with the __sourceName field.
	CustomSource *string `json:"customSource,omitempty"`
	// Optionally, override the source category configured on the Sumo Logic HTTP collector. This can also be overridden at the event level with the __sourceCategory field.
	CustomCategory *string `json:"customCategory,omitempty"`
	// Optionally, preserve the raw event format instead of json-ifying it.
	Format *DataFormatSumoLogic `default:"json" json:"format"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"1024" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Defaults to Yes. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max body size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events.
	ExtraHTTPHeaders []ExtraHTTPHeaderSumoLogic `json:"extraHttpHeaders,omitempty"`
	// Enables round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeSumoLogic `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable).
	ResponseRetrySettings []ResponseRetrySettingSumoLogic `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsSumoLogic  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorSumoLogic `default:"block" json:"onBackpressure"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	Description        *string  `json:"description,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *CompressionSumoLogic `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorSumoLogic `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeSumoLogic       `default:"error" json:"pqMode"`
	PqControls *PqControlsSumoLogic `json:"pqControls,omitempty"`
	Status     *TFStatus            `json:"status,omitempty"`
}

func (o OutputSumoLogic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSumoLogic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputSumoLogic) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputSumoLogic) GetType() TypeSumoLogic {
	if o == nil {
		return TypeSumoLogic("")
	}
	return o.Type
}

func (o *OutputSumoLogic) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSumoLogic) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSumoLogic) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSumoLogic) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSumoLogic) GetURL() string {
	if o == nil {
		return ""
	}
	return o.URL
}

func (o *OutputSumoLogic) GetCustomSource() *string {
	if o == nil {
		return nil
	}
	return o.CustomSource
}

func (o *OutputSumoLogic) GetCustomCategory() *string {
	if o == nil {
		return nil
	}
	return o.CustomCategory
}

func (o *OutputSumoLogic) GetFormat() *DataFormatSumoLogic {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputSumoLogic) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputSumoLogic) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputSumoLogic) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputSumoLogic) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputSumoLogic) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputSumoLogic) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputSumoLogic) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputSumoLogic) GetExtraHTTPHeaders() []ExtraHTTPHeaderSumoLogic {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputSumoLogic) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputSumoLogic) GetFailedRequestLoggingMode() *FailedRequestLoggingModeSumoLogic {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputSumoLogic) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputSumoLogic) GetResponseRetrySettings() []ResponseRetrySettingSumoLogic {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputSumoLogic) GetTimeoutRetrySettings() *TimeoutRetrySettingsSumoLogic {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputSumoLogic) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputSumoLogic) GetOnBackpressure() *BackpressureBehaviorSumoLogic {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSumoLogic) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputSumoLogic) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSumoLogic) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSumoLogic) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSumoLogic) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSumoLogic) GetPqCompress() *CompressionSumoLogic {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSumoLogic) GetPqOnBackpressure() *QueueFullBehaviorSumoLogic {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSumoLogic) GetPqMode() *ModeSumoLogic {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSumoLogic) GetPqControls() *PqControlsSumoLogic {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputSumoLogic) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type OutputTypeSnmp string

const (
	OutputTypeSnmpSnmp OutputTypeSnmp = "snmp"
)

func (e OutputTypeSnmp) ToPointer() *OutputTypeSnmp {
	return &e
}
func (e *OutputTypeSnmp) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "snmp":
		*e = OutputTypeSnmp(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeSnmp: %v", v)
	}
}

type HostSnmp struct {
	// Destination host
	Host string `json:"host"`
	// Destination port, default is 162
	Port *float64 `default:"162" json:"port"`
}

func (h HostSnmp) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(h, "", false)
}

func (h *HostSnmp) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &h, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *HostSnmp) GetHost() string {
	if o == nil {
		return ""
	}
	return o.Host
}

func (o *HostSnmp) GetPort() *float64 {
	if o == nil {
		return nil
	}
	return o.Port
}

type OutputSnmp struct {
	// Unique ID for this output
	ID   *string        `json:"id,omitempty"`
	Type OutputTypeSnmp `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// One or more SNMP destinations to forward traps to
	Hosts []HostSnmp `json:"hosts"`
	// How often to resolve the destination hostname to an IP address. Ignored if all destinations are IP addresses. A value of 0 means every trap sent will incur a DNS lookup.
	DNSResolvePeriodSec *float64  `default:"0" json:"dnsResolvePeriodSec"`
	Description         *string   `json:"description,omitempty"`
	Status              *TFStatus `json:"status,omitempty"`
}

func (o OutputSnmp) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSnmp) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputSnmp) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputSnmp) GetType() OutputTypeSnmp {
	if o == nil {
		return OutputTypeSnmp("")
	}
	return o.Type
}

func (o *OutputSnmp) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSnmp) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSnmp) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSnmp) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSnmp) GetHosts() []HostSnmp {
	if o == nil {
		return []HostSnmp{}
	}
	return o.Hosts
}

func (o *OutputSnmp) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputSnmp) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSnmp) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type OutputTypeSqs string

const (
	OutputTypeSqsSqs OutputTypeSqs = "sqs"
)

func (e OutputTypeSqs) ToPointer() *OutputTypeSqs {
	return &e
}
func (e *OutputTypeSqs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "sqs":
		*e = OutputTypeSqs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeSqs: %v", v)
	}
}

// OutputQueueType - The queue type used (or created). Defaults to Standard.
type OutputQueueType string

const (
	OutputQueueTypeStandard OutputQueueType = "standard"
	OutputQueueTypeFifo     OutputQueueType = "fifo"
)

func (e OutputQueueType) ToPointer() *OutputQueueType {
	return &e
}
func (e *OutputQueueType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "standard":
		fallthrough
	case "fifo":
		*e = OutputQueueType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputQueueType: %v", v)
	}
}

// OutputAuthenticationMethodSqs - AWS authentication method. Choose Auto to use IAM roles.
type OutputAuthenticationMethodSqs string

const (
	OutputAuthenticationMethodSqsAuto   OutputAuthenticationMethodSqs = "auto"
	OutputAuthenticationMethodSqsManual OutputAuthenticationMethodSqs = "manual"
	OutputAuthenticationMethodSqsSecret OutputAuthenticationMethodSqs = "secret"
)

func (e OutputAuthenticationMethodSqs) ToPointer() *OutputAuthenticationMethodSqs {
	return &e
}
func (e *OutputAuthenticationMethodSqs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "auto":
		fallthrough
	case "manual":
		fallthrough
	case "secret":
		*e = OutputAuthenticationMethodSqs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputAuthenticationMethodSqs: %v", v)
	}
}

// OutputSignatureVersionSqs - Signature version to use for signing SQS requests
type OutputSignatureVersionSqs string

const (
	OutputSignatureVersionSqsV2 OutputSignatureVersionSqs = "v2"
	OutputSignatureVersionSqsV4 OutputSignatureVersionSqs = "v4"
)

func (e OutputSignatureVersionSqs) ToPointer() *OutputSignatureVersionSqs {
	return &e
}
func (e *OutputSignatureVersionSqs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "v2":
		fallthrough
	case "v4":
		*e = OutputSignatureVersionSqs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputSignatureVersionSqs: %v", v)
	}
}

// BackpressureBehaviorSqs - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorSqs string

const (
	BackpressureBehaviorSqsBlock BackpressureBehaviorSqs = "block"
	BackpressureBehaviorSqsDrop  BackpressureBehaviorSqs = "drop"
	BackpressureBehaviorSqsQueue BackpressureBehaviorSqs = "queue"
)

func (e BackpressureBehaviorSqs) ToPointer() *BackpressureBehaviorSqs {
	return &e
}
func (e *BackpressureBehaviorSqs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorSqs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorSqs: %v", v)
	}
}

// PqCompressCompressionSqs - Codec to use to compress the persisted data.
type PqCompressCompressionSqs string

const (
	PqCompressCompressionSqsNone PqCompressCompressionSqs = "none"
	PqCompressCompressionSqsGzip PqCompressCompressionSqs = "gzip"
)

func (e PqCompressCompressionSqs) ToPointer() *PqCompressCompressionSqs {
	return &e
}
func (e *PqCompressCompressionSqs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = PqCompressCompressionSqs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PqCompressCompressionSqs: %v", v)
	}
}

// QueueFullBehaviorSqs - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSqs string

const (
	QueueFullBehaviorSqsBlock QueueFullBehaviorSqs = "block"
	QueueFullBehaviorSqsDrop  QueueFullBehaviorSqs = "drop"
)

func (e QueueFullBehaviorSqs) ToPointer() *QueueFullBehaviorSqs {
	return &e
}
func (e *QueueFullBehaviorSqs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorSqs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorSqs: %v", v)
	}
}

// OutputModeSqs - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type OutputModeSqs string

const (
	OutputModeSqsError        OutputModeSqs = "error"
	OutputModeSqsBackpressure OutputModeSqs = "backpressure"
	OutputModeSqsAlways       OutputModeSqs = "always"
)

func (e OutputModeSqs) ToPointer() *OutputModeSqs {
	return &e
}
func (e *OutputModeSqs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = OutputModeSqs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputModeSqs: %v", v)
	}
}

type PqControlsSqs struct {
}

type OutputSqs struct {
	// Unique ID for this output
	ID   *string        `json:"id,omitempty"`
	Type *OutputTypeSqs `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The name, URL, or ARN of the SQS queue to send events to. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.
	QueueName string `json:"queueName"`
	// The queue type used (or created). Defaults to Standard.
	QueueType *OutputQueueType `default:"standard" json:"queueType"`
	// SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.
	AwsAccountID *string `json:"awsAccountId,omitempty"`
	// This parameter applies only to FIFO queues. The tag that specifies that a message belongs to a specific message group. Messages that belong to the same message group are processed in a FIFO manner. Use event field __messageGroupId to override this value.
	MessageGroupID *string `default:"cribl" json:"messageGroupId"`
	// Create queue if it does not exist.
	CreateQueue *bool `default:"true" json:"createQueue"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *OutputAuthenticationMethodSqs `default:"auto" json:"awsAuthenticationMethod"`
	AwsSecretKey            *string                        `json:"awsSecretKey,omitempty"`
	// AWS Region where the SQS queue is located. Required, unless the Queue entry is a URL or ARN that includes a Region.
	Region *string `json:"region,omitempty"`
	// SQS service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to SQS-compatible endpoint.
	Endpoint *string `json:"endpoint,omitempty"`
	// Signature version to use for signing SQS requests
	SignatureVersion *OutputSignatureVersionSqs `default:"v4" json:"signatureVersion"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Use Assume Role credentials to access SQS
	EnableAssumeRole *bool `default:"false" json:"enableAssumeRole"`
	// Amazon Resource Name (ARN) of the role to assume
	AssumeRoleArn *string `json:"assumeRoleArn,omitempty"`
	// External ID to use when assuming role
	AssumeRoleExternalID *string `json:"assumeRoleExternalId,omitempty"`
	// Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
	DurationSeconds *float64 `default:"3600" json:"durationSeconds"`
	// Maximum number of queued batches before blocking.
	MaxQueueSize *float64 `default:"100" json:"maxQueueSize"`
	// Maximum size (KB) of batches to send. Per the SQS spec, the max allowed value is 256 KB.
	MaxRecordSizeKB *float64 `default:"256" json:"maxRecordSizeKB"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// The maximum number of in-progress API requests before backpressure is applied.
	MaxInProgress *float64 `default:"10" json:"maxInProgress"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorSqs `default:"block" json:"onBackpressure"`
	Description    *string                  `json:"description,omitempty"`
	AwsAPIKey      *string                  `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *PqCompressCompressionSqs `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorSqs `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *OutputModeSqs `default:"error" json:"pqMode"`
	PqControls *PqControlsSqs `json:"pqControls,omitempty"`
	Status     *TFStatus      `json:"status,omitempty"`
}

func (o OutputSqs) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSqs) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputSqs) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputSqs) GetType() *OutputTypeSqs {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputSqs) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSqs) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSqs) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSqs) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSqs) GetQueueName() string {
	if o == nil {
		return ""
	}
	return o.QueueName
}

func (o *OutputSqs) GetQueueType() *OutputQueueType {
	if o == nil {
		return nil
	}
	return o.QueueType
}

func (o *OutputSqs) GetAwsAccountID() *string {
	if o == nil {
		return nil
	}
	return o.AwsAccountID
}

func (o *OutputSqs) GetMessageGroupID() *string {
	if o == nil {
		return nil
	}
	return o.MessageGroupID
}

func (o *OutputSqs) GetCreateQueue() *bool {
	if o == nil {
		return nil
	}
	return o.CreateQueue
}

func (o *OutputSqs) GetAwsAuthenticationMethod() *OutputAuthenticationMethodSqs {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputSqs) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputSqs) GetRegion() *string {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputSqs) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputSqs) GetSignatureVersion() *OutputSignatureVersionSqs {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputSqs) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputSqs) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputSqs) GetEnableAssumeRole() *bool {
	if o == nil {
		return nil
	}
	return o.EnableAssumeRole
}

func (o *OutputSqs) GetAssumeRoleArn() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleArn
}

func (o *OutputSqs) GetAssumeRoleExternalID() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleExternalID
}

func (o *OutputSqs) GetDurationSeconds() *float64 {
	if o == nil {
		return nil
	}
	return o.DurationSeconds
}

func (o *OutputSqs) GetMaxQueueSize() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxQueueSize
}

func (o *OutputSqs) GetMaxRecordSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSizeKB
}

func (o *OutputSqs) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputSqs) GetMaxInProgress() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxInProgress
}

func (o *OutputSqs) GetOnBackpressure() *BackpressureBehaviorSqs {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSqs) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSqs) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputSqs) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputSqs) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSqs) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSqs) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSqs) GetPqCompress() *PqCompressCompressionSqs {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSqs) GetPqOnBackpressure() *QueueFullBehaviorSqs {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSqs) GetPqMode() *OutputModeSqs {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSqs) GetPqControls() *PqControlsSqs {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputSqs) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeSns string

const (
	TypeSnsSns TypeSns = "sns"
)

func (e TypeSns) ToPointer() *TypeSns {
	return &e
}
func (e *TypeSns) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "sns":
		*e = TypeSns(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeSns: %v", v)
	}
}

// AuthenticationMethodSns - AWS authentication method. Choose Auto to use IAM roles.
type AuthenticationMethodSns string

const (
	AuthenticationMethodSnsAuto   AuthenticationMethodSns = "auto"
	AuthenticationMethodSnsManual AuthenticationMethodSns = "manual"
	AuthenticationMethodSnsSecret AuthenticationMethodSns = "secret"
)

func (e AuthenticationMethodSns) ToPointer() *AuthenticationMethodSns {
	return &e
}
func (e *AuthenticationMethodSns) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "auto":
		fallthrough
	case "manual":
		fallthrough
	case "secret":
		*e = AuthenticationMethodSns(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodSns: %v", v)
	}
}

// SignatureVersionSns - Signature version to use for signing SNS requests
type SignatureVersionSns string

const (
	SignatureVersionSnsV2 SignatureVersionSns = "v2"
	SignatureVersionSnsV4 SignatureVersionSns = "v4"
)

func (e SignatureVersionSns) ToPointer() *SignatureVersionSns {
	return &e
}
func (e *SignatureVersionSns) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "v2":
		fallthrough
	case "v4":
		*e = SignatureVersionSns(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SignatureVersionSns: %v", v)
	}
}

// BackpressureBehaviorSns - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorSns string

const (
	BackpressureBehaviorSnsBlock BackpressureBehaviorSns = "block"
	BackpressureBehaviorSnsDrop  BackpressureBehaviorSns = "drop"
	BackpressureBehaviorSnsQueue BackpressureBehaviorSns = "queue"
)

func (e BackpressureBehaviorSns) ToPointer() *BackpressureBehaviorSns {
	return &e
}
func (e *BackpressureBehaviorSns) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorSns(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorSns: %v", v)
	}
}

// CompressionSns - Codec to use to compress the persisted data.
type CompressionSns string

const (
	CompressionSnsNone CompressionSns = "none"
	CompressionSnsGzip CompressionSns = "gzip"
)

func (e CompressionSns) ToPointer() *CompressionSns {
	return &e
}
func (e *CompressionSns) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionSns(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionSns: %v", v)
	}
}

// QueueFullBehaviorSns - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSns string

const (
	QueueFullBehaviorSnsBlock QueueFullBehaviorSns = "block"
	QueueFullBehaviorSnsDrop  QueueFullBehaviorSns = "drop"
)

func (e QueueFullBehaviorSns) ToPointer() *QueueFullBehaviorSns {
	return &e
}
func (e *QueueFullBehaviorSns) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorSns(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorSns: %v", v)
	}
}

// ModeSns - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type ModeSns string

const (
	ModeSnsError        ModeSns = "error"
	ModeSnsBackpressure ModeSns = "backpressure"
	ModeSnsAlways       ModeSns = "always"
)

func (e ModeSns) ToPointer() *ModeSns {
	return &e
}
func (e *ModeSns) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeSns(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeSns: %v", v)
	}
}

type PqControlsSns struct {
}

type OutputSns struct {
	// Unique ID for this output
	ID   *string  `json:"id,omitempty"`
	Type *TypeSns `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The ARN of the SNS topic to send events to. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. E.g., 'https://host:port/myQueueName'. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. E.g., referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.
	TopicArn string `json:"topicArn"`
	// Messages in the same group are processed in a FIFO manner. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.
	MessageGroupID string `json:"messageGroupId"`
	// Maximum number of retries before the output returns an error. Note that not all errors are retryable. The retries use an exponential backoff policy.
	MaxRetries *float64 `json:"maxRetries,omitempty"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *AuthenticationMethodSns `default:"auto" json:"awsAuthenticationMethod"`
	AwsSecretKey            *string                  `json:"awsSecretKey,omitempty"`
	// Region where the SNS is located
	Region *string `json:"region,omitempty"`
	// SNS service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to SNS-compatible endpoint.
	Endpoint *string `json:"endpoint,omitempty"`
	// Signature version to use for signing SNS requests
	SignatureVersion *SignatureVersionSns `default:"v4" json:"signatureVersion"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Use Assume Role credentials to access SNS
	EnableAssumeRole *bool `default:"false" json:"enableAssumeRole"`
	// Amazon Resource Name (ARN) of the role to assume
	AssumeRoleArn *string `json:"assumeRoleArn,omitempty"`
	// External ID to use when assuming role
	AssumeRoleExternalID *string `json:"assumeRoleExternalId,omitempty"`
	// Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
	DurationSeconds *float64 `default:"3600" json:"durationSeconds"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorSns `default:"block" json:"onBackpressure"`
	Description    *string                  `json:"description,omitempty"`
	AwsAPIKey      *string                  `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *CompressionSns `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorSns `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeSns       `default:"error" json:"pqMode"`
	PqControls *PqControlsSns `json:"pqControls,omitempty"`
	Status     *TFStatus      `json:"status,omitempty"`
}

func (o OutputSns) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSns) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputSns) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputSns) GetType() *TypeSns {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputSns) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSns) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSns) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSns) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSns) GetTopicArn() string {
	if o == nil {
		return ""
	}
	return o.TopicArn
}

func (o *OutputSns) GetMessageGroupID() string {
	if o == nil {
		return ""
	}
	return o.MessageGroupID
}

func (o *OutputSns) GetMaxRetries() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetries
}

func (o *OutputSns) GetAwsAuthenticationMethod() *AuthenticationMethodSns {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputSns) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputSns) GetRegion() *string {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputSns) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputSns) GetSignatureVersion() *SignatureVersionSns {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputSns) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputSns) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputSns) GetEnableAssumeRole() *bool {
	if o == nil {
		return nil
	}
	return o.EnableAssumeRole
}

func (o *OutputSns) GetAssumeRoleArn() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleArn
}

func (o *OutputSns) GetAssumeRoleExternalID() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleExternalID
}

func (o *OutputSns) GetDurationSeconds() *float64 {
	if o == nil {
		return nil
	}
	return o.DurationSeconds
}

func (o *OutputSns) GetOnBackpressure() *BackpressureBehaviorSns {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSns) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSns) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputSns) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputSns) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSns) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSns) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSns) GetPqCompress() *CompressionSns {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSns) GetPqOnBackpressure() *QueueFullBehaviorSns {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSns) GetPqMode() *ModeSns {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSns) GetPqControls() *PqControlsSns {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputSns) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeRouter string

const (
	TypeRouterRouter TypeRouter = "router"
)

func (e TypeRouter) ToPointer() *TypeRouter {
	return &e
}
func (e *TypeRouter) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "router":
		*e = TypeRouter(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeRouter: %v", v)
	}
}

type OutputRule struct {
	// JavaScript expression to select events to send to output
	Filter string `json:"filter"`
	// Output to send matching events to
	Output string `json:"output"`
	// Description of this rule's purpose
	Description *string `json:"description,omitempty"`
	// Flag to control whether to stop the event from being checked against other rules
	Final *bool `default:"true" json:"final"`
}

func (o OutputRule) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputRule) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *OutputRule) GetFilter() string {
	if o == nil {
		return ""
	}
	return o.Filter
}

func (o *OutputRule) GetOutput() string {
	if o == nil {
		return ""
	}
	return o.Output
}

func (o *OutputRule) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputRule) GetFinal() *bool {
	if o == nil {
		return nil
	}
	return o.Final
}

type OutputRouter struct {
	// Unique ID for this output
	ID   *string    `json:"id,omitempty"`
	Type TypeRouter `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Event routing rules
	Rules       []OutputRule `json:"rules"`
	Description *string      `json:"description,omitempty"`
	Status      *TFStatus    `json:"status,omitempty"`
}

func (o *OutputRouter) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputRouter) GetType() TypeRouter {
	if o == nil {
		return TypeRouter("")
	}
	return o.Type
}

func (o *OutputRouter) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputRouter) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputRouter) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputRouter) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputRouter) GetRules() []OutputRule {
	if o == nil {
		return []OutputRule{}
	}
	return o.Rules
}

func (o *OutputRouter) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputRouter) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeGraphite string

const (
	TypeGraphiteGraphite TypeGraphite = "graphite"
)

func (e TypeGraphite) ToPointer() *TypeGraphite {
	return &e
}
func (e *TypeGraphite) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "graphite":
		*e = TypeGraphite(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeGraphite: %v", v)
	}
}

// DestinationProtocolGraphite - Protocol to use when communicating with the destination.
type DestinationProtocolGraphite string

const (
	DestinationProtocolGraphiteUDP DestinationProtocolGraphite = "udp"
	DestinationProtocolGraphiteTCP DestinationProtocolGraphite = "tcp"
)

func (e DestinationProtocolGraphite) ToPointer() *DestinationProtocolGraphite {
	return &e
}
func (e *DestinationProtocolGraphite) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "udp":
		fallthrough
	case "tcp":
		*e = DestinationProtocolGraphite(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationProtocolGraphite: %v", v)
	}
}

// BackpressureBehaviorGraphite - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorGraphite string

const (
	BackpressureBehaviorGraphiteBlock BackpressureBehaviorGraphite = "block"
	BackpressureBehaviorGraphiteDrop  BackpressureBehaviorGraphite = "drop"
	BackpressureBehaviorGraphiteQueue BackpressureBehaviorGraphite = "queue"
)

func (e BackpressureBehaviorGraphite) ToPointer() *BackpressureBehaviorGraphite {
	return &e
}
func (e *BackpressureBehaviorGraphite) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorGraphite(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorGraphite: %v", v)
	}
}

// CompressionGraphite - Codec to use to compress the persisted data.
type CompressionGraphite string

const (
	CompressionGraphiteNone CompressionGraphite = "none"
	CompressionGraphiteGzip CompressionGraphite = "gzip"
)

func (e CompressionGraphite) ToPointer() *CompressionGraphite {
	return &e
}
func (e *CompressionGraphite) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionGraphite(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionGraphite: %v", v)
	}
}

// QueueFullBehaviorGraphite - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorGraphite string

const (
	QueueFullBehaviorGraphiteBlock QueueFullBehaviorGraphite = "block"
	QueueFullBehaviorGraphiteDrop  QueueFullBehaviorGraphite = "drop"
)

func (e QueueFullBehaviorGraphite) ToPointer() *QueueFullBehaviorGraphite {
	return &e
}
func (e *QueueFullBehaviorGraphite) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorGraphite(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorGraphite: %v", v)
	}
}

// ModeGraphite - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type ModeGraphite string

const (
	ModeGraphiteError        ModeGraphite = "error"
	ModeGraphiteBackpressure ModeGraphite = "backpressure"
	ModeGraphiteAlways       ModeGraphite = "always"
)

func (e ModeGraphite) ToPointer() *ModeGraphite {
	return &e
}
func (e *ModeGraphite) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeGraphite(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeGraphite: %v", v)
	}
}

type PqControlsGraphite struct {
}

type OutputGraphite struct {
	// Unique ID for this output
	ID   *string       `json:"id,omitempty"`
	Type *TypeGraphite `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Protocol to use when communicating with the destination.
	Protocol *DestinationProtocolGraphite `default:"udp" json:"protocol"`
	// The hostname of the destination.
	Host string `json:"host"`
	// Destination port.
	Port *float64 `default:"8125" json:"port"`
	// When protocol is UDP, specifies the maximum size of packets sent to the destination. Also known as the MTU for the network path to the destination system.
	Mtu *float64 `default:"512" json:"mtu"`
	// When protocol is TCP, specifies how often buffers should be flushed, resulting in records sent to the destination.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every batch sent will incur a DNS lookup.
	DNSResolvePeriodSec *float64 `default:"0" json:"dnsResolvePeriodSec"`
	Description         *string  `json:"description,omitempty"`
	// Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
	ThrottleRatePerSec *string `default:"0" json:"throttleRatePerSec"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead
	WriteTimeout *float64 `default:"60000" json:"writeTimeout"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorGraphite `default:"block" json:"onBackpressure"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *CompressionGraphite `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorGraphite `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeGraphite       `default:"error" json:"pqMode"`
	PqControls *PqControlsGraphite `json:"pqControls,omitempty"`
	Status     *TFStatus           `json:"status,omitempty"`
}

func (o OutputGraphite) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGraphite) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputGraphite) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputGraphite) GetType() *TypeGraphite {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputGraphite) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputGraphite) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputGraphite) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputGraphite) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputGraphite) GetProtocol() *DestinationProtocolGraphite {
	if o == nil {
		return nil
	}
	return o.Protocol
}

func (o *OutputGraphite) GetHost() string {
	if o == nil {
		return ""
	}
	return o.Host
}

func (o *OutputGraphite) GetPort() *float64 {
	if o == nil {
		return nil
	}
	return o.Port
}

func (o *OutputGraphite) GetMtu() *float64 {
	if o == nil {
		return nil
	}
	return o.Mtu
}

func (o *OutputGraphite) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputGraphite) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputGraphite) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputGraphite) GetThrottleRatePerSec() *string {
	if o == nil {
		return nil
	}
	return o.ThrottleRatePerSec
}

func (o *OutputGraphite) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputGraphite) GetWriteTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteTimeout
}

func (o *OutputGraphite) GetOnBackpressure() *BackpressureBehaviorGraphite {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputGraphite) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputGraphite) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputGraphite) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputGraphite) GetPqCompress() *CompressionGraphite {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputGraphite) GetPqOnBackpressure() *QueueFullBehaviorGraphite {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputGraphite) GetPqMode() *ModeGraphite {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputGraphite) GetPqControls() *PqControlsGraphite {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputGraphite) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeStatsdExt string

const (
	TypeStatsdExtStatsdExt TypeStatsdExt = "statsd_ext"
)

func (e TypeStatsdExt) ToPointer() *TypeStatsdExt {
	return &e
}
func (e *TypeStatsdExt) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "statsd_ext":
		*e = TypeStatsdExt(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeStatsdExt: %v", v)
	}
}

// DestinationProtocolStatsdExt - Protocol to use when communicating with the destination.
type DestinationProtocolStatsdExt string

const (
	DestinationProtocolStatsdExtUDP DestinationProtocolStatsdExt = "udp"
	DestinationProtocolStatsdExtTCP DestinationProtocolStatsdExt = "tcp"
)

func (e DestinationProtocolStatsdExt) ToPointer() *DestinationProtocolStatsdExt {
	return &e
}
func (e *DestinationProtocolStatsdExt) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "udp":
		fallthrough
	case "tcp":
		*e = DestinationProtocolStatsdExt(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationProtocolStatsdExt: %v", v)
	}
}

// BackpressureBehaviorStatsdExt - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorStatsdExt string

const (
	BackpressureBehaviorStatsdExtBlock BackpressureBehaviorStatsdExt = "block"
	BackpressureBehaviorStatsdExtDrop  BackpressureBehaviorStatsdExt = "drop"
	BackpressureBehaviorStatsdExtQueue BackpressureBehaviorStatsdExt = "queue"
)

func (e BackpressureBehaviorStatsdExt) ToPointer() *BackpressureBehaviorStatsdExt {
	return &e
}
func (e *BackpressureBehaviorStatsdExt) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorStatsdExt(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorStatsdExt: %v", v)
	}
}

// CompressionStatsdExt - Codec to use to compress the persisted data.
type CompressionStatsdExt string

const (
	CompressionStatsdExtNone CompressionStatsdExt = "none"
	CompressionStatsdExtGzip CompressionStatsdExt = "gzip"
)

func (e CompressionStatsdExt) ToPointer() *CompressionStatsdExt {
	return &e
}
func (e *CompressionStatsdExt) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionStatsdExt(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionStatsdExt: %v", v)
	}
}

// QueueFullBehaviorStatsdExt - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorStatsdExt string

const (
	QueueFullBehaviorStatsdExtBlock QueueFullBehaviorStatsdExt = "block"
	QueueFullBehaviorStatsdExtDrop  QueueFullBehaviorStatsdExt = "drop"
)

func (e QueueFullBehaviorStatsdExt) ToPointer() *QueueFullBehaviorStatsdExt {
	return &e
}
func (e *QueueFullBehaviorStatsdExt) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorStatsdExt(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorStatsdExt: %v", v)
	}
}

// ModeStatsdExt - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type ModeStatsdExt string

const (
	ModeStatsdExtError        ModeStatsdExt = "error"
	ModeStatsdExtBackpressure ModeStatsdExt = "backpressure"
	ModeStatsdExtAlways       ModeStatsdExt = "always"
)

func (e ModeStatsdExt) ToPointer() *ModeStatsdExt {
	return &e
}
func (e *ModeStatsdExt) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeStatsdExt(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeStatsdExt: %v", v)
	}
}

type PqControlsStatsdExt struct {
}

type OutputStatsdExt struct {
	// Unique ID for this output
	ID   *string        `json:"id,omitempty"`
	Type *TypeStatsdExt `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Protocol to use when communicating with the destination.
	Protocol *DestinationProtocolStatsdExt `default:"udp" json:"protocol"`
	// The hostname of the destination.
	Host string `json:"host"`
	// Destination port.
	Port *float64 `default:"8125" json:"port"`
	// When protocol is UDP, specifies the maximum size of packets sent to the destination. Also known as the MTU for the network path to the destination system.
	Mtu *float64 `default:"512" json:"mtu"`
	// When protocol is TCP, specifies how often buffers should be flushed, resulting in records sent to the destination.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every batch sent will incur a DNS lookup.
	DNSResolvePeriodSec *float64 `default:"0" json:"dnsResolvePeriodSec"`
	Description         *string  `json:"description,omitempty"`
	// Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
	ThrottleRatePerSec *string `default:"0" json:"throttleRatePerSec"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead
	WriteTimeout *float64 `default:"60000" json:"writeTimeout"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorStatsdExt `default:"block" json:"onBackpressure"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *CompressionStatsdExt `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorStatsdExt `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeStatsdExt       `default:"error" json:"pqMode"`
	PqControls *PqControlsStatsdExt `json:"pqControls,omitempty"`
	Status     *TFStatus            `json:"status,omitempty"`
}

func (o OutputStatsdExt) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputStatsdExt) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputStatsdExt) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputStatsdExt) GetType() *TypeStatsdExt {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputStatsdExt) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputStatsdExt) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputStatsdExt) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputStatsdExt) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputStatsdExt) GetProtocol() *DestinationProtocolStatsdExt {
	if o == nil {
		return nil
	}
	return o.Protocol
}

func (o *OutputStatsdExt) GetHost() string {
	if o == nil {
		return ""
	}
	return o.Host
}

func (o *OutputStatsdExt) GetPort() *float64 {
	if o == nil {
		return nil
	}
	return o.Port
}

func (o *OutputStatsdExt) GetMtu() *float64 {
	if o == nil {
		return nil
	}
	return o.Mtu
}

func (o *OutputStatsdExt) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputStatsdExt) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputStatsdExt) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputStatsdExt) GetThrottleRatePerSec() *string {
	if o == nil {
		return nil
	}
	return o.ThrottleRatePerSec
}

func (o *OutputStatsdExt) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputStatsdExt) GetWriteTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteTimeout
}

func (o *OutputStatsdExt) GetOnBackpressure() *BackpressureBehaviorStatsdExt {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputStatsdExt) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputStatsdExt) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputStatsdExt) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputStatsdExt) GetPqCompress() *CompressionStatsdExt {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputStatsdExt) GetPqOnBackpressure() *QueueFullBehaviorStatsdExt {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputStatsdExt) GetPqMode() *ModeStatsdExt {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputStatsdExt) GetPqControls() *PqControlsStatsdExt {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputStatsdExt) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeStatsd string

const (
	TypeStatsdStatsd TypeStatsd = "statsd"
)

func (e TypeStatsd) ToPointer() *TypeStatsd {
	return &e
}
func (e *TypeStatsd) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "statsd":
		*e = TypeStatsd(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeStatsd: %v", v)
	}
}

// DestinationProtocolStatsd - Protocol to use when communicating with the destination.
type DestinationProtocolStatsd string

const (
	DestinationProtocolStatsdUDP DestinationProtocolStatsd = "udp"
	DestinationProtocolStatsdTCP DestinationProtocolStatsd = "tcp"
)

func (e DestinationProtocolStatsd) ToPointer() *DestinationProtocolStatsd {
	return &e
}
func (e *DestinationProtocolStatsd) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "udp":
		fallthrough
	case "tcp":
		*e = DestinationProtocolStatsd(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationProtocolStatsd: %v", v)
	}
}

// BackpressureBehaviorStatsd - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorStatsd string

const (
	BackpressureBehaviorStatsdBlock BackpressureBehaviorStatsd = "block"
	BackpressureBehaviorStatsdDrop  BackpressureBehaviorStatsd = "drop"
	BackpressureBehaviorStatsdQueue BackpressureBehaviorStatsd = "queue"
)

func (e BackpressureBehaviorStatsd) ToPointer() *BackpressureBehaviorStatsd {
	return &e
}
func (e *BackpressureBehaviorStatsd) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorStatsd(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorStatsd: %v", v)
	}
}

// CompressionStatsd - Codec to use to compress the persisted data.
type CompressionStatsd string

const (
	CompressionStatsdNone CompressionStatsd = "none"
	CompressionStatsdGzip CompressionStatsd = "gzip"
)

func (e CompressionStatsd) ToPointer() *CompressionStatsd {
	return &e
}
func (e *CompressionStatsd) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionStatsd(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionStatsd: %v", v)
	}
}

// QueueFullBehaviorStatsd - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorStatsd string

const (
	QueueFullBehaviorStatsdBlock QueueFullBehaviorStatsd = "block"
	QueueFullBehaviorStatsdDrop  QueueFullBehaviorStatsd = "drop"
)

func (e QueueFullBehaviorStatsd) ToPointer() *QueueFullBehaviorStatsd {
	return &e
}
func (e *QueueFullBehaviorStatsd) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorStatsd(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorStatsd: %v", v)
	}
}

// ModeStatsd - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type ModeStatsd string

const (
	ModeStatsdError        ModeStatsd = "error"
	ModeStatsdBackpressure ModeStatsd = "backpressure"
	ModeStatsdAlways       ModeStatsd = "always"
)

func (e ModeStatsd) ToPointer() *ModeStatsd {
	return &e
}
func (e *ModeStatsd) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeStatsd(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeStatsd: %v", v)
	}
}

type PqControlsStatsd struct {
}

type OutputStatsd struct {
	// Unique ID for this output
	ID   *string     `json:"id,omitempty"`
	Type *TypeStatsd `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Protocol to use when communicating with the destination.
	Protocol *DestinationProtocolStatsd `default:"udp" json:"protocol"`
	// The hostname of the destination.
	Host string `json:"host"`
	// Destination port.
	Port *float64 `default:"8125" json:"port"`
	// When protocol is UDP, specifies the maximum size of packets sent to the destination. Also known as the MTU for the network path to the destination system.
	Mtu *float64 `default:"512" json:"mtu"`
	// When protocol is TCP, specifies how often buffers should be flushed, resulting in records sent to the destination.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every batch sent will incur a DNS lookup.
	DNSResolvePeriodSec *float64 `default:"0" json:"dnsResolvePeriodSec"`
	Description         *string  `json:"description,omitempty"`
	// Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
	ThrottleRatePerSec *string `default:"0" json:"throttleRatePerSec"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead
	WriteTimeout *float64 `default:"60000" json:"writeTimeout"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorStatsd `default:"block" json:"onBackpressure"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *CompressionStatsd `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorStatsd `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeStatsd       `default:"error" json:"pqMode"`
	PqControls *PqControlsStatsd `json:"pqControls,omitempty"`
	Status     *TFStatus         `json:"status,omitempty"`
}

func (o OutputStatsd) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputStatsd) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputStatsd) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputStatsd) GetType() *TypeStatsd {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputStatsd) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputStatsd) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputStatsd) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputStatsd) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputStatsd) GetProtocol() *DestinationProtocolStatsd {
	if o == nil {
		return nil
	}
	return o.Protocol
}

func (o *OutputStatsd) GetHost() string {
	if o == nil {
		return ""
	}
	return o.Host
}

func (o *OutputStatsd) GetPort() *float64 {
	if o == nil {
		return nil
	}
	return o.Port
}

func (o *OutputStatsd) GetMtu() *float64 {
	if o == nil {
		return nil
	}
	return o.Mtu
}

func (o *OutputStatsd) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputStatsd) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputStatsd) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputStatsd) GetThrottleRatePerSec() *string {
	if o == nil {
		return nil
	}
	return o.ThrottleRatePerSec
}

func (o *OutputStatsd) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputStatsd) GetWriteTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteTimeout
}

func (o *OutputStatsd) GetOnBackpressure() *BackpressureBehaviorStatsd {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputStatsd) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputStatsd) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputStatsd) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputStatsd) GetPqCompress() *CompressionStatsd {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputStatsd) GetPqOnBackpressure() *QueueFullBehaviorStatsd {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputStatsd) GetPqMode() *ModeStatsd {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputStatsd) GetPqControls() *PqControlsStatsd {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputStatsd) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type OutputMinioType string

const (
	OutputMinioTypeMinio OutputMinioType = "minio"
)

func (e OutputMinioType) ToPointer() *OutputMinioType {
	return &e
}
func (e *OutputMinioType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "minio":
		*e = OutputMinioType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMinioType: %v", v)
	}
}

// OutputMinioAuthenticationMethod - AWS authentication method. Choose Auto to use IAM roles.
type OutputMinioAuthenticationMethod string

const (
	OutputMinioAuthenticationMethodAuto   OutputMinioAuthenticationMethod = "auto"
	OutputMinioAuthenticationMethodManual OutputMinioAuthenticationMethod = "manual"
	OutputMinioAuthenticationMethodSecret OutputMinioAuthenticationMethod = "secret"
)

func (e OutputMinioAuthenticationMethod) ToPointer() *OutputMinioAuthenticationMethod {
	return &e
}
func (e *OutputMinioAuthenticationMethod) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "auto":
		fallthrough
	case "manual":
		fallthrough
	case "secret":
		*e = OutputMinioAuthenticationMethod(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMinioAuthenticationMethod: %v", v)
	}
}

// OutputMinioSignatureVersion - Signature version to use for signing MinIO requests.
type OutputMinioSignatureVersion string

const (
	OutputMinioSignatureVersionV2 OutputMinioSignatureVersion = "v2"
	OutputMinioSignatureVersionV4 OutputMinioSignatureVersion = "v4"
)

func (e OutputMinioSignatureVersion) ToPointer() *OutputMinioSignatureVersion {
	return &e
}
func (e *OutputMinioSignatureVersion) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "v2":
		fallthrough
	case "v4":
		*e = OutputMinioSignatureVersion(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMinioSignatureVersion: %v", v)
	}
}

// OutputMinioObjectACL - Object ACL to assign to uploaded objects.
type OutputMinioObjectACL string

const (
	OutputMinioObjectACLPrivate                OutputMinioObjectACL = "private"
	OutputMinioObjectACLPublicRead             OutputMinioObjectACL = "public-read"
	OutputMinioObjectACLPublicReadWrite        OutputMinioObjectACL = "public-read-write"
	OutputMinioObjectACLAuthenticatedRead      OutputMinioObjectACL = "authenticated-read"
	OutputMinioObjectACLAwsExecRead            OutputMinioObjectACL = "aws-exec-read"
	OutputMinioObjectACLBucketOwnerRead        OutputMinioObjectACL = "bucket-owner-read"
	OutputMinioObjectACLBucketOwnerFullControl OutputMinioObjectACL = "bucket-owner-full-control"
)

func (e OutputMinioObjectACL) ToPointer() *OutputMinioObjectACL {
	return &e
}
func (e *OutputMinioObjectACL) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "private":
		fallthrough
	case "public-read":
		fallthrough
	case "public-read-write":
		fallthrough
	case "authenticated-read":
		fallthrough
	case "aws-exec-read":
		fallthrough
	case "bucket-owner-read":
		fallthrough
	case "bucket-owner-full-control":
		*e = OutputMinioObjectACL(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMinioObjectACL: %v", v)
	}
}

// OutputMinioStorageClass - Storage class to select for uploaded objects.
type OutputMinioStorageClass string

const (
	OutputMinioStorageClassStandard          OutputMinioStorageClass = "STANDARD"
	OutputMinioStorageClassReducedRedundancy OutputMinioStorageClass = "REDUCED_REDUNDANCY"
)

func (e OutputMinioStorageClass) ToPointer() *OutputMinioStorageClass {
	return &e
}
func (e *OutputMinioStorageClass) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "STANDARD":
		fallthrough
	case "REDUCED_REDUNDANCY":
		*e = OutputMinioStorageClass(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMinioStorageClass: %v", v)
	}
}

// OutputMinioServerSideEncryption - Server-side encryption for uploaded objects.
type OutputMinioServerSideEncryption string

const (
	OutputMinioServerSideEncryptionAes256 OutputMinioServerSideEncryption = "AES256"
)

func (e OutputMinioServerSideEncryption) ToPointer() *OutputMinioServerSideEncryption {
	return &e
}
func (e *OutputMinioServerSideEncryption) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "AES256":
		*e = OutputMinioServerSideEncryption(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMinioServerSideEncryption: %v", v)
	}
}

// OutputMinioDataFormat - Format of the output data
type OutputMinioDataFormat string

const (
	OutputMinioDataFormatJSON    OutputMinioDataFormat = "json"
	OutputMinioDataFormatRaw     OutputMinioDataFormat = "raw"
	OutputMinioDataFormatParquet OutputMinioDataFormat = "parquet"
)

func (e OutputMinioDataFormat) ToPointer() *OutputMinioDataFormat {
	return &e
}
func (e *OutputMinioDataFormat) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "json":
		fallthrough
	case "raw":
		fallthrough
	case "parquet":
		*e = OutputMinioDataFormat(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMinioDataFormat: %v", v)
	}
}

// OutputMinioBackpressureBehavior - Whether to block or drop events when all receivers are exerting backpressure
type OutputMinioBackpressureBehavior string

const (
	OutputMinioBackpressureBehaviorBlock OutputMinioBackpressureBehavior = "block"
	OutputMinioBackpressureBehaviorDrop  OutputMinioBackpressureBehavior = "drop"
)

func (e OutputMinioBackpressureBehavior) ToPointer() *OutputMinioBackpressureBehavior {
	return &e
}
func (e *OutputMinioBackpressureBehavior) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = OutputMinioBackpressureBehavior(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMinioBackpressureBehavior: %v", v)
	}
}

// OutputMinioDiskSpaceProtection - Whether to block or drop events when disk space is below the global 'Min free disk space' limit
type OutputMinioDiskSpaceProtection string

const (
	OutputMinioDiskSpaceProtectionBlock OutputMinioDiskSpaceProtection = "block"
	OutputMinioDiskSpaceProtectionDrop  OutputMinioDiskSpaceProtection = "drop"
)

func (e OutputMinioDiskSpaceProtection) ToPointer() *OutputMinioDiskSpaceProtection {
	return &e
}
func (e *OutputMinioDiskSpaceProtection) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = OutputMinioDiskSpaceProtection(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMinioDiskSpaceProtection: %v", v)
	}
}

// OutputMinioCompress - Choose data compression format to apply before moving files to final destination
type OutputMinioCompress string

const (
	OutputMinioCompressNone OutputMinioCompress = "none"
	OutputMinioCompressGzip OutputMinioCompress = "gzip"
)

func (e OutputMinioCompress) ToPointer() *OutputMinioCompress {
	return &e
}
func (e *OutputMinioCompress) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = OutputMinioCompress(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMinioCompress: %v", v)
	}
}

// OutputMinioCompressionLevel - Compression level to apply before moving files to final destination
type OutputMinioCompressionLevel string

const (
	OutputMinioCompressionLevelBestSpeed       OutputMinioCompressionLevel = "best_speed"
	OutputMinioCompressionLevelNormal          OutputMinioCompressionLevel = "normal"
	OutputMinioCompressionLevelBestCompression OutputMinioCompressionLevel = "best_compression"
)

func (e OutputMinioCompressionLevel) ToPointer() *OutputMinioCompressionLevel {
	return &e
}
func (e *OutputMinioCompressionLevel) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "best_speed":
		fallthrough
	case "normal":
		fallthrough
	case "best_compression":
		*e = OutputMinioCompressionLevel(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMinioCompressionLevel: %v", v)
	}
}

// OutputMinioParquetVersion - Determines which data types are supported and how they are represented
type OutputMinioParquetVersion string

const (
	OutputMinioParquetVersionParquet10 OutputMinioParquetVersion = "PARQUET_1_0"
	OutputMinioParquetVersionParquet24 OutputMinioParquetVersion = "PARQUET_2_4"
	OutputMinioParquetVersionParquet26 OutputMinioParquetVersion = "PARQUET_2_6"
)

func (e OutputMinioParquetVersion) ToPointer() *OutputMinioParquetVersion {
	return &e
}
func (e *OutputMinioParquetVersion) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "PARQUET_1_0":
		fallthrough
	case "PARQUET_2_4":
		fallthrough
	case "PARQUET_2_6":
		*e = OutputMinioParquetVersion(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMinioParquetVersion: %v", v)
	}
}

// OutputMinioDataPageVersion - Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
type OutputMinioDataPageVersion string

const (
	OutputMinioDataPageVersionDataPageV1 OutputMinioDataPageVersion = "DATA_PAGE_V1"
	OutputMinioDataPageVersionDataPageV2 OutputMinioDataPageVersion = "DATA_PAGE_V2"
)

func (e OutputMinioDataPageVersion) ToPointer() *OutputMinioDataPageVersion {
	return &e
}
func (e *OutputMinioDataPageVersion) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "DATA_PAGE_V1":
		fallthrough
	case "DATA_PAGE_V2":
		*e = OutputMinioDataPageVersion(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMinioDataPageVersion: %v", v)
	}
}

type OutputMinioKeyValueMetadatum struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (o OutputMinioKeyValueMetadatum) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputMinioKeyValueMetadatum) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *OutputMinioKeyValueMetadatum) GetKey() *string {
	if o == nil {
		return nil
	}
	return o.Key
}

func (o *OutputMinioKeyValueMetadatum) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OutputMinio struct {
	// Unique ID for this output
	ID   *string          `json:"id,omitempty"`
	Type *OutputMinioType `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// MinIO service url (e.g. http://minioHost:9000)
	Endpoint string `json:"endpoint"`
	// Name of the destination MinIO bucket. This value can be a constant or a JavaScript expression that can only be evaluated at init time. E.g. referencing a Global Variable: `myBucket-${C.vars.myVar}`.
	Bucket string `json:"bucket"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *OutputMinioAuthenticationMethod `default:"auto" json:"awsAuthenticationMethod"`
	// Secret key. This value can be a constant or a JavaScript expression(e.g., `${C.env.SOME_SECRET}`).
	AwsSecretKey *string `json:"awsSecretKey,omitempty"`
	// Region where the MinIO service/cluster is located
	Region *string `json:"region,omitempty"`
	// Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant stable storage.
	StagePath *string `default:"\\$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Append output's ID to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Root directory to prepend to path before uploading. Enter a constant, or a JS expression enclosed in quotes or backticks.
	DestPath *string `json:"destPath,omitempty"`
	// Signature version to use for signing MinIO requests.
	SignatureVersion *OutputMinioSignatureVersion `default:"v4" json:"signatureVersion"`
	// Object ACL to assign to uploaded objects.
	ObjectACL *OutputMinioObjectACL `default:"private" json:"objectACL"`
	// Storage class to select for uploaded objects.
	StorageClass *OutputMinioStorageClass `json:"storageClass,omitempty"`
	// Server-side encryption for uploaded objects.
	ServerSideEncryption *OutputMinioServerSideEncryption `json:"serverSideEncryption,omitempty"`
	// Whether to reuse connections between requests, which can improve performance.
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Whether to reject certificates that cannot be verified against a valid CA (e.g., self-signed certificates).
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Disable if you can access files within the bucket but not the bucket itself.
	VerifyPermissions *bool `default:"true" json:"verifyPermissions"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value  if present  otherwise to each location's root directory.
	PartitionExpr *string `default:"C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')" json:"partitionExpr"`
	// Format of the output data
	Format *OutputMinioDataFormat `default:"json" json:"format"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// Whether to block or drop events when all receivers are exerting backpressure
	OnBackpressure *OutputMinioBackpressureBehavior `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, dead-letter it to prevent further errors.
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// Whether to block or drop events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *OutputMinioDiskSpaceProtection `default:"block" json:"onDiskFullBackpressure"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.
	MaxConcurrentFileParts *float64 `default:"4" json:"maxConcurrentFileParts"`
	Description            *string  `json:"description,omitempty"`
	// This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)
	AwsAPIKey *string `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// Choose data compression format to apply before moving files to final destination
	Compress *OutputMinioCompress `default:"gzip" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *OutputMinioCompressionLevel `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *OutputMinioParquetVersion `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *OutputMinioDataPageVersion `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging, e.g., "key":"OCSF Event Class", "value":"9001".
	KeyValueMetadata []OutputMinioKeyValueMetadatum `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// How frequently, in seconds, to clean up empty directories when 'Remove empty staging dirs' is enabled
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"\\$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64  `default:"20" json:"maxRetryNum"`
	Status      *TFStatus `json:"status,omitempty"`
}

func (o OutputMinio) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputMinio) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputMinio) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputMinio) GetType() *OutputMinioType {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputMinio) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputMinio) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputMinio) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputMinio) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputMinio) GetEndpoint() string {
	if o == nil {
		return ""
	}
	return o.Endpoint
}

func (o *OutputMinio) GetBucket() string {
	if o == nil {
		return ""
	}
	return o.Bucket
}

func (o *OutputMinio) GetAwsAuthenticationMethod() *OutputMinioAuthenticationMethod {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputMinio) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputMinio) GetRegion() *string {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputMinio) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputMinio) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputMinio) GetDestPath() *string {
	if o == nil {
		return nil
	}
	return o.DestPath
}

func (o *OutputMinio) GetSignatureVersion() *OutputMinioSignatureVersion {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputMinio) GetObjectACL() *OutputMinioObjectACL {
	if o == nil {
		return nil
	}
	return o.ObjectACL
}

func (o *OutputMinio) GetStorageClass() *OutputMinioStorageClass {
	if o == nil {
		return nil
	}
	return o.StorageClass
}

func (o *OutputMinio) GetServerSideEncryption() *OutputMinioServerSideEncryption {
	if o == nil {
		return nil
	}
	return o.ServerSideEncryption
}

func (o *OutputMinio) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputMinio) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputMinio) GetVerifyPermissions() *bool {
	if o == nil {
		return nil
	}
	return o.VerifyPermissions
}

func (o *OutputMinio) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputMinio) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputMinio) GetFormat() *OutputMinioDataFormat {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputMinio) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputMinio) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputMinio) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputMinio) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputMinio) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputMinio) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputMinio) GetOnBackpressure() *OutputMinioBackpressureBehavior {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputMinio) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputMinio) GetOnDiskFullBackpressure() *OutputMinioDiskSpaceProtection {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputMinio) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputMinio) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputMinio) GetMaxConcurrentFileParts() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentFileParts
}

func (o *OutputMinio) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputMinio) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputMinio) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputMinio) GetCompress() *OutputMinioCompress {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputMinio) GetCompressionLevel() *OutputMinioCompressionLevel {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputMinio) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputMinio) GetParquetVersion() *OutputMinioParquetVersion {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputMinio) GetParquetDataPageVersion() *OutputMinioDataPageVersion {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputMinio) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputMinio) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputMinio) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputMinio) GetKeyValueMetadata() []OutputMinioKeyValueMetadatum {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputMinio) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputMinio) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputMinio) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputMinio) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputMinio) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputMinio) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

func (o *OutputMinio) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeCloudwatch string

const (
	TypeCloudwatchCloudwatch TypeCloudwatch = "cloudwatch"
)

func (e TypeCloudwatch) ToPointer() *TypeCloudwatch {
	return &e
}
func (e *TypeCloudwatch) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "cloudwatch":
		*e = TypeCloudwatch(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeCloudwatch: %v", v)
	}
}

// AuthenticationMethodCloudwatch - AWS authentication method. Choose Auto to use IAM roles.
type AuthenticationMethodCloudwatch string

const (
	AuthenticationMethodCloudwatchAuto   AuthenticationMethodCloudwatch = "auto"
	AuthenticationMethodCloudwatchManual AuthenticationMethodCloudwatch = "manual"
	AuthenticationMethodCloudwatchSecret AuthenticationMethodCloudwatch = "secret"
)

func (e AuthenticationMethodCloudwatch) ToPointer() *AuthenticationMethodCloudwatch {
	return &e
}
func (e *AuthenticationMethodCloudwatch) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "auto":
		fallthrough
	case "manual":
		fallthrough
	case "secret":
		*e = AuthenticationMethodCloudwatch(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodCloudwatch: %v", v)
	}
}

// BackpressureBehaviorCloudwatch - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorCloudwatch string

const (
	BackpressureBehaviorCloudwatchBlock BackpressureBehaviorCloudwatch = "block"
	BackpressureBehaviorCloudwatchDrop  BackpressureBehaviorCloudwatch = "drop"
	BackpressureBehaviorCloudwatchQueue BackpressureBehaviorCloudwatch = "queue"
)

func (e BackpressureBehaviorCloudwatch) ToPointer() *BackpressureBehaviorCloudwatch {
	return &e
}
func (e *BackpressureBehaviorCloudwatch) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorCloudwatch(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorCloudwatch: %v", v)
	}
}

// CompressionCloudwatch - Codec to use to compress the persisted data.
type CompressionCloudwatch string

const (
	CompressionCloudwatchNone CompressionCloudwatch = "none"
	CompressionCloudwatchGzip CompressionCloudwatch = "gzip"
)

func (e CompressionCloudwatch) ToPointer() *CompressionCloudwatch {
	return &e
}
func (e *CompressionCloudwatch) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionCloudwatch(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionCloudwatch: %v", v)
	}
}

// QueueFullBehaviorCloudwatch - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorCloudwatch string

const (
	QueueFullBehaviorCloudwatchBlock QueueFullBehaviorCloudwatch = "block"
	QueueFullBehaviorCloudwatchDrop  QueueFullBehaviorCloudwatch = "drop"
)

func (e QueueFullBehaviorCloudwatch) ToPointer() *QueueFullBehaviorCloudwatch {
	return &e
}
func (e *QueueFullBehaviorCloudwatch) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorCloudwatch(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorCloudwatch: %v", v)
	}
}

// ModeCloudwatch - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type ModeCloudwatch string

const (
	ModeCloudwatchError        ModeCloudwatch = "error"
	ModeCloudwatchBackpressure ModeCloudwatch = "backpressure"
	ModeCloudwatchAlways       ModeCloudwatch = "always"
)

func (e ModeCloudwatch) ToPointer() *ModeCloudwatch {
	return &e
}
func (e *ModeCloudwatch) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeCloudwatch(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeCloudwatch: %v", v)
	}
}

type PqControlsCloudwatch struct {
}

type OutputCloudwatch struct {
	// Unique ID for this output
	ID   *string         `json:"id,omitempty"`
	Type *TypeCloudwatch `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// CloudWatch log group to associate events with
	LogGroupName string `json:"logGroupName"`
	// Prefix for CloudWatch log stream name. This prefix will be used to generate a unique log stream name per cribl instance, for example: myStream_myHost_myOutputId
	LogStreamName string `json:"logStreamName"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *AuthenticationMethodCloudwatch `default:"auto" json:"awsAuthenticationMethod"`
	AwsSecretKey            *string                         `json:"awsSecretKey,omitempty"`
	// Region where the CloudWatchLogs is located
	Region string `json:"region"`
	// CloudWatchLogs service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to CloudWatchLogs-compatible endpoint.
	Endpoint *string `json:"endpoint,omitempty"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Use Assume Role credentials to access CloudWatchLogs
	EnableAssumeRole *bool `default:"false" json:"enableAssumeRole"`
	// Amazon Resource Name (ARN) of the role to assume
	AssumeRoleArn *string `json:"assumeRoleArn,omitempty"`
	// External ID to use when assuming role
	AssumeRoleExternalID *string `json:"assumeRoleExternalId,omitempty"`
	// Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
	DurationSeconds *float64 `default:"3600" json:"durationSeconds"`
	// Maximum number of queued batches before blocking
	MaxQueueSize *float64 `default:"5" json:"maxQueueSize"`
	// Maximum size (KB) of each individual record before compression. For non compressible data 1MB is the max recommended size
	MaxRecordSizeKB *float64 `default:"1024" json:"maxRecordSizeKB"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorCloudwatch `default:"block" json:"onBackpressure"`
	Description    *string                         `json:"description,omitempty"`
	AwsAPIKey      *string                         `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *CompressionCloudwatch `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorCloudwatch `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeCloudwatch       `default:"error" json:"pqMode"`
	PqControls *PqControlsCloudwatch `json:"pqControls,omitempty"`
	Status     *TFStatus             `json:"status,omitempty"`
}

func (o OutputCloudwatch) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputCloudwatch) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputCloudwatch) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputCloudwatch) GetType() *TypeCloudwatch {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputCloudwatch) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputCloudwatch) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputCloudwatch) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputCloudwatch) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputCloudwatch) GetLogGroupName() string {
	if o == nil {
		return ""
	}
	return o.LogGroupName
}

func (o *OutputCloudwatch) GetLogStreamName() string {
	if o == nil {
		return ""
	}
	return o.LogStreamName
}

func (o *OutputCloudwatch) GetAwsAuthenticationMethod() *AuthenticationMethodCloudwatch {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputCloudwatch) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputCloudwatch) GetRegion() string {
	if o == nil {
		return ""
	}
	return o.Region
}

func (o *OutputCloudwatch) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputCloudwatch) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputCloudwatch) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputCloudwatch) GetEnableAssumeRole() *bool {
	if o == nil {
		return nil
	}
	return o.EnableAssumeRole
}

func (o *OutputCloudwatch) GetAssumeRoleArn() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleArn
}

func (o *OutputCloudwatch) GetAssumeRoleExternalID() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleExternalID
}

func (o *OutputCloudwatch) GetDurationSeconds() *float64 {
	if o == nil {
		return nil
	}
	return o.DurationSeconds
}

func (o *OutputCloudwatch) GetMaxQueueSize() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxQueueSize
}

func (o *OutputCloudwatch) GetMaxRecordSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSizeKB
}

func (o *OutputCloudwatch) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputCloudwatch) GetOnBackpressure() *BackpressureBehaviorCloudwatch {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputCloudwatch) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputCloudwatch) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputCloudwatch) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputCloudwatch) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputCloudwatch) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputCloudwatch) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputCloudwatch) GetPqCompress() *CompressionCloudwatch {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputCloudwatch) GetPqOnBackpressure() *QueueFullBehaviorCloudwatch {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputCloudwatch) GetPqMode() *ModeCloudwatch {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputCloudwatch) GetPqControls() *PqControlsCloudwatch {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputCloudwatch) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeInfluxdb string

const (
	TypeInfluxdbInfluxdb TypeInfluxdb = "influxdb"
)

func (e TypeInfluxdb) ToPointer() *TypeInfluxdb {
	return &e
}
func (e *TypeInfluxdb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "influxdb":
		*e = TypeInfluxdb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeInfluxdb: %v", v)
	}
}

// TimestampPrecision - Sets the precision for the supplied Unix time values. Defaults to milliseconds.
type TimestampPrecision string

const (
	TimestampPrecisionNs TimestampPrecision = "ns"
	TimestampPrecisionU  TimestampPrecision = "u"
	TimestampPrecisionMs TimestampPrecision = "ms"
	TimestampPrecisionS  TimestampPrecision = "s"
	TimestampPrecisionM  TimestampPrecision = "m"
	TimestampPrecisionH  TimestampPrecision = "h"
)

func (e TimestampPrecision) ToPointer() *TimestampPrecision {
	return &e
}
func (e *TimestampPrecision) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "ns":
		fallthrough
	case "u":
		fallthrough
	case "ms":
		fallthrough
	case "s":
		fallthrough
	case "m":
		fallthrough
	case "h":
		*e = TimestampPrecision(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TimestampPrecision: %v", v)
	}
}

type ExtraHTTPHeaderInfluxdb struct {
	// Field name
	Name *string `json:"name,omitempty"`
	// Field value
	Value string `json:"value"`
}

func (o *ExtraHTTPHeaderInfluxdb) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderInfluxdb) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeInfluxdb - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeInfluxdb string

const (
	FailedRequestLoggingModeInfluxdbPayload           FailedRequestLoggingModeInfluxdb = "payload"
	FailedRequestLoggingModeInfluxdbPayloadAndHeaders FailedRequestLoggingModeInfluxdb = "payloadAndHeaders"
	FailedRequestLoggingModeInfluxdbNone              FailedRequestLoggingModeInfluxdb = "none"
)

func (e FailedRequestLoggingModeInfluxdb) ToPointer() *FailedRequestLoggingModeInfluxdb {
	return &e
}
func (e *FailedRequestLoggingModeInfluxdb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeInfluxdb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeInfluxdb: %v", v)
	}
}

type ResponseRetrySettingInfluxdb struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingInfluxdb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingInfluxdb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingInfluxdb) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingInfluxdb) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingInfluxdb) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingInfluxdb) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsInfluxdb struct {
	// Enable to retry on request timeout
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsInfluxdb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsInfluxdb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsInfluxdb) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsInfluxdb) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsInfluxdb) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsInfluxdb) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorInfluxdb - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorInfluxdb string

const (
	BackpressureBehaviorInfluxdbBlock BackpressureBehaviorInfluxdb = "block"
	BackpressureBehaviorInfluxdbDrop  BackpressureBehaviorInfluxdb = "drop"
	BackpressureBehaviorInfluxdbQueue BackpressureBehaviorInfluxdb = "queue"
)

func (e BackpressureBehaviorInfluxdb) ToPointer() *BackpressureBehaviorInfluxdb {
	return &e
}
func (e *BackpressureBehaviorInfluxdb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorInfluxdb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorInfluxdb: %v", v)
	}
}

// AuthenticationTypeInfluxdb - InfluxDB authentication type
type AuthenticationTypeInfluxdb string

const (
	AuthenticationTypeInfluxdbNone              AuthenticationTypeInfluxdb = "none"
	AuthenticationTypeInfluxdbBasic             AuthenticationTypeInfluxdb = "basic"
	AuthenticationTypeInfluxdbCredentialsSecret AuthenticationTypeInfluxdb = "credentialsSecret"
	AuthenticationTypeInfluxdbToken             AuthenticationTypeInfluxdb = "token"
	AuthenticationTypeInfluxdbTextSecret        AuthenticationTypeInfluxdb = "textSecret"
	AuthenticationTypeInfluxdbOauth             AuthenticationTypeInfluxdb = "oauth"
)

func (e AuthenticationTypeInfluxdb) ToPointer() *AuthenticationTypeInfluxdb {
	return &e
}
func (e *AuthenticationTypeInfluxdb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "basic":
		fallthrough
	case "credentialsSecret":
		fallthrough
	case "token":
		fallthrough
	case "textSecret":
		fallthrough
	case "oauth":
		*e = AuthenticationTypeInfluxdb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationTypeInfluxdb: %v", v)
	}
}

// CompressionInfluxdb - Codec to use to compress the persisted data.
type CompressionInfluxdb string

const (
	CompressionInfluxdbNone CompressionInfluxdb = "none"
	CompressionInfluxdbGzip CompressionInfluxdb = "gzip"
)

func (e CompressionInfluxdb) ToPointer() *CompressionInfluxdb {
	return &e
}
func (e *CompressionInfluxdb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionInfluxdb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionInfluxdb: %v", v)
	}
}

// QueueFullBehaviorInfluxdb - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorInfluxdb string

const (
	QueueFullBehaviorInfluxdbBlock QueueFullBehaviorInfluxdb = "block"
	QueueFullBehaviorInfluxdbDrop  QueueFullBehaviorInfluxdb = "drop"
)

func (e QueueFullBehaviorInfluxdb) ToPointer() *QueueFullBehaviorInfluxdb {
	return &e
}
func (e *QueueFullBehaviorInfluxdb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorInfluxdb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorInfluxdb: %v", v)
	}
}

// ModeInfluxdb - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type ModeInfluxdb string

const (
	ModeInfluxdbError        ModeInfluxdb = "error"
	ModeInfluxdbBackpressure ModeInfluxdb = "backpressure"
	ModeInfluxdbAlways       ModeInfluxdb = "always"
)

func (e ModeInfluxdb) ToPointer() *ModeInfluxdb {
	return &e
}
func (e *ModeInfluxdb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeInfluxdb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeInfluxdb: %v", v)
	}
}

type PqControlsInfluxdb struct {
}

type OauthParamInfluxdb struct {
	// OAuth parameter name
	Name string `json:"name"`
	// OAuth parameter value
	Value string `json:"value"`
}

func (o *OauthParamInfluxdb) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OauthParamInfluxdb) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OauthHeaderInfluxdb struct {
	// OAuth header name
	Name string `json:"name"`
	// OAuth header value
	Value string `json:"value"`
}

func (o *OauthHeaderInfluxdb) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OauthHeaderInfluxdb) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OutputInfluxdb struct {
	// Unique ID for this output
	ID   *string      `json:"id,omitempty"`
	Type TypeInfluxdb `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// URL of an InfluxDB cluster to send events to, e.g., http://localhost:8086/write
	URL string `json:"url"`
	// The v2 API can be enabled with InfluxDB versions 1.8 and later.
	UseV2API *bool `default:"false" json:"useV2API"`
	// Sets the precision for the supplied Unix time values. Defaults to milliseconds.
	TimestampPrecision *TimestampPrecision `default:"ms" json:"timestampPrecision"`
	// Enabling this will pull the value field from the metric name. E,g, 'db.query.user' will use 'db.query' as the measurement and 'user' as the value field.
	DynamicValueFieldName *bool `default:"true" json:"dynamicValueFieldName"`
	// Name of the field in which to store the metric when sending to InfluxDB. If dynamic generation is enabled and fails, this will be used as a fallback.
	ValueFieldName *string `default:"value" json:"valueFieldName"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Defaults to Yes. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max body size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events.
	ExtraHTTPHeaders []ExtraHTTPHeaderInfluxdb `json:"extraHttpHeaders,omitempty"`
	// Enables round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeInfluxdb `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable).
	ResponseRetrySettings []ResponseRetrySettingInfluxdb `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsInfluxdb  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorInfluxdb `default:"block" json:"onBackpressure"`
	// InfluxDB authentication type
	AuthType    *AuthenticationTypeInfluxdb `default:"none" json:"authType"`
	Description *string                     `json:"description,omitempty"`
	// Database to write to.
	Database *string `json:"database,omitempty"`
	// Bucket to write to.
	Bucket *string `json:"bucket,omitempty"`
	// Organization ID for this bucket.
	Org *string `json:"org,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *CompressionInfluxdb `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorInfluxdb `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeInfluxdb       `default:"error" json:"pqMode"`
	PqControls *PqControlsInfluxdb `json:"pqControls,omitempty"`
	Username   *string             `json:"username,omitempty"`
	Password   *string             `json:"password,omitempty"`
	// Bearer token to include in the authorization header
	Token *string `json:"token,omitempty"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// URL for OAuth
	LoginURL *string `json:"loginUrl,omitempty"`
	// Secret parameter name to pass in request body
	SecretParamName *string `json:"secretParamName,omitempty"`
	// Secret parameter value to pass in request body
	Secret *string `json:"secret,omitempty"`
	// Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
	TokenAttributeName *string `json:"tokenAttributeName,omitempty"`
	// JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
	AuthHeaderExpr *string `default:"Bearer \\${token}" json:"authHeaderExpr"`
	// How often the OAuth token should be refreshed.
	TokenTimeoutSecs *float64 `default:"3600" json:"tokenTimeoutSecs"`
	// Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthParams []OauthParamInfluxdb `json:"oauthParams,omitempty"`
	// Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthHeaders []OauthHeaderInfluxdb `json:"oauthHeaders,omitempty"`
	Status       *TFStatus             `json:"status,omitempty"`
}

func (o OutputInfluxdb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputInfluxdb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputInfluxdb) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputInfluxdb) GetType() TypeInfluxdb {
	if o == nil {
		return TypeInfluxdb("")
	}
	return o.Type
}

func (o *OutputInfluxdb) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputInfluxdb) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputInfluxdb) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputInfluxdb) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputInfluxdb) GetURL() string {
	if o == nil {
		return ""
	}
	return o.URL
}

func (o *OutputInfluxdb) GetUseV2API() *bool {
	if o == nil {
		return nil
	}
	return o.UseV2API
}

func (o *OutputInfluxdb) GetTimestampPrecision() *TimestampPrecision {
	if o == nil {
		return nil
	}
	return o.TimestampPrecision
}

func (o *OutputInfluxdb) GetDynamicValueFieldName() *bool {
	if o == nil {
		return nil
	}
	return o.DynamicValueFieldName
}

func (o *OutputInfluxdb) GetValueFieldName() *string {
	if o == nil {
		return nil
	}
	return o.ValueFieldName
}

func (o *OutputInfluxdb) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputInfluxdb) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputInfluxdb) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputInfluxdb) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputInfluxdb) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputInfluxdb) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputInfluxdb) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputInfluxdb) GetExtraHTTPHeaders() []ExtraHTTPHeaderInfluxdb {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputInfluxdb) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputInfluxdb) GetFailedRequestLoggingMode() *FailedRequestLoggingModeInfluxdb {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputInfluxdb) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputInfluxdb) GetResponseRetrySettings() []ResponseRetrySettingInfluxdb {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputInfluxdb) GetTimeoutRetrySettings() *TimeoutRetrySettingsInfluxdb {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputInfluxdb) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputInfluxdb) GetOnBackpressure() *BackpressureBehaviorInfluxdb {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputInfluxdb) GetAuthType() *AuthenticationTypeInfluxdb {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputInfluxdb) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputInfluxdb) GetDatabase() *string {
	if o == nil {
		return nil
	}
	return o.Database
}

func (o *OutputInfluxdb) GetBucket() *string {
	if o == nil {
		return nil
	}
	return o.Bucket
}

func (o *OutputInfluxdb) GetOrg() *string {
	if o == nil {
		return nil
	}
	return o.Org
}

func (o *OutputInfluxdb) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputInfluxdb) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputInfluxdb) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputInfluxdb) GetPqCompress() *CompressionInfluxdb {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputInfluxdb) GetPqOnBackpressure() *QueueFullBehaviorInfluxdb {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputInfluxdb) GetPqMode() *ModeInfluxdb {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputInfluxdb) GetPqControls() *PqControlsInfluxdb {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputInfluxdb) GetUsername() *string {
	if o == nil {
		return nil
	}
	return o.Username
}

func (o *OutputInfluxdb) GetPassword() *string {
	if o == nil {
		return nil
	}
	return o.Password
}

func (o *OutputInfluxdb) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputInfluxdb) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

func (o *OutputInfluxdb) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputInfluxdb) GetLoginURL() *string {
	if o == nil {
		return nil
	}
	return o.LoginURL
}

func (o *OutputInfluxdb) GetSecretParamName() *string {
	if o == nil {
		return nil
	}
	return o.SecretParamName
}

func (o *OutputInfluxdb) GetSecret() *string {
	if o == nil {
		return nil
	}
	return o.Secret
}

func (o *OutputInfluxdb) GetTokenAttributeName() *string {
	if o == nil {
		return nil
	}
	return o.TokenAttributeName
}

func (o *OutputInfluxdb) GetAuthHeaderExpr() *string {
	if o == nil {
		return nil
	}
	return o.AuthHeaderExpr
}

func (o *OutputInfluxdb) GetTokenTimeoutSecs() *float64 {
	if o == nil {
		return nil
	}
	return o.TokenTimeoutSecs
}

func (o *OutputInfluxdb) GetOauthParams() []OauthParamInfluxdb {
	if o == nil {
		return nil
	}
	return o.OauthParams
}

func (o *OutputInfluxdb) GetOauthHeaders() []OauthHeaderInfluxdb {
	if o == nil {
		return nil
	}
	return o.OauthHeaders
}

func (o *OutputInfluxdb) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeNewrelicEvents string

const (
	TypeNewrelicEventsNewrelicEvents TypeNewrelicEvents = "newrelic_events"
)

func (e TypeNewrelicEvents) ToPointer() *TypeNewrelicEvents {
	return &e
}
func (e *TypeNewrelicEvents) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "newrelic_events":
		*e = TypeNewrelicEvents(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeNewrelicEvents: %v", v)
	}
}

// RegionNewrelicEvents - Which New Relic region endpoint to use.
type RegionNewrelicEvents string

const (
	RegionNewrelicEventsUs     RegionNewrelicEvents = "US"
	RegionNewrelicEventsEu     RegionNewrelicEvents = "EU"
	RegionNewrelicEventsCustom RegionNewrelicEvents = "Custom"
)

func (e RegionNewrelicEvents) ToPointer() *RegionNewrelicEvents {
	return &e
}
func (e *RegionNewrelicEvents) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "US":
		fallthrough
	case "EU":
		fallthrough
	case "Custom":
		*e = RegionNewrelicEvents(v)
		return nil
	default:
		return fmt.Errorf("invalid value for RegionNewrelicEvents: %v", v)
	}
}

type ExtraHTTPHeaderNewrelicEvents struct {
	// Field name
	Name *string `json:"name,omitempty"`
	// Field value
	Value string `json:"value"`
}

func (o *ExtraHTTPHeaderNewrelicEvents) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderNewrelicEvents) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeNewrelicEvents - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeNewrelicEvents string

const (
	FailedRequestLoggingModeNewrelicEventsPayload           FailedRequestLoggingModeNewrelicEvents = "payload"
	FailedRequestLoggingModeNewrelicEventsPayloadAndHeaders FailedRequestLoggingModeNewrelicEvents = "payloadAndHeaders"
	FailedRequestLoggingModeNewrelicEventsNone              FailedRequestLoggingModeNewrelicEvents = "none"
)

func (e FailedRequestLoggingModeNewrelicEvents) ToPointer() *FailedRequestLoggingModeNewrelicEvents {
	return &e
}
func (e *FailedRequestLoggingModeNewrelicEvents) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeNewrelicEvents(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeNewrelicEvents: %v", v)
	}
}

type ResponseRetrySettingNewrelicEvents struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingNewrelicEvents) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingNewrelicEvents) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingNewrelicEvents) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingNewrelicEvents) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingNewrelicEvents) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingNewrelicEvents) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsNewrelicEvents struct {
	// Enable to retry on request timeout
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsNewrelicEvents) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsNewrelicEvents) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsNewrelicEvents) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsNewrelicEvents) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsNewrelicEvents) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsNewrelicEvents) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorNewrelicEvents - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorNewrelicEvents string

const (
	BackpressureBehaviorNewrelicEventsBlock BackpressureBehaviorNewrelicEvents = "block"
	BackpressureBehaviorNewrelicEventsDrop  BackpressureBehaviorNewrelicEvents = "drop"
	BackpressureBehaviorNewrelicEventsQueue BackpressureBehaviorNewrelicEvents = "queue"
)

func (e BackpressureBehaviorNewrelicEvents) ToPointer() *BackpressureBehaviorNewrelicEvents {
	return &e
}
func (e *BackpressureBehaviorNewrelicEvents) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorNewrelicEvents(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorNewrelicEvents: %v", v)
	}
}

// AuthenticationMethodNewrelicEvents - Enter API key directly, or select a stored secret
type AuthenticationMethodNewrelicEvents string

const (
	AuthenticationMethodNewrelicEventsManual AuthenticationMethodNewrelicEvents = "manual"
	AuthenticationMethodNewrelicEventsSecret AuthenticationMethodNewrelicEvents = "secret"
)

func (e AuthenticationMethodNewrelicEvents) ToPointer() *AuthenticationMethodNewrelicEvents {
	return &e
}
func (e *AuthenticationMethodNewrelicEvents) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		*e = AuthenticationMethodNewrelicEvents(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodNewrelicEvents: %v", v)
	}
}

// CompressionNewrelicEvents - Codec to use to compress the persisted data.
type CompressionNewrelicEvents string

const (
	CompressionNewrelicEventsNone CompressionNewrelicEvents = "none"
	CompressionNewrelicEventsGzip CompressionNewrelicEvents = "gzip"
)

func (e CompressionNewrelicEvents) ToPointer() *CompressionNewrelicEvents {
	return &e
}
func (e *CompressionNewrelicEvents) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionNewrelicEvents(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionNewrelicEvents: %v", v)
	}
}

// QueueFullBehaviorNewrelicEvents - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorNewrelicEvents string

const (
	QueueFullBehaviorNewrelicEventsBlock QueueFullBehaviorNewrelicEvents = "block"
	QueueFullBehaviorNewrelicEventsDrop  QueueFullBehaviorNewrelicEvents = "drop"
)

func (e QueueFullBehaviorNewrelicEvents) ToPointer() *QueueFullBehaviorNewrelicEvents {
	return &e
}
func (e *QueueFullBehaviorNewrelicEvents) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorNewrelicEvents(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorNewrelicEvents: %v", v)
	}
}

// ModeNewrelicEvents - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type ModeNewrelicEvents string

const (
	ModeNewrelicEventsError        ModeNewrelicEvents = "error"
	ModeNewrelicEventsBackpressure ModeNewrelicEvents = "backpressure"
	ModeNewrelicEventsAlways       ModeNewrelicEvents = "always"
)

func (e ModeNewrelicEvents) ToPointer() *ModeNewrelicEvents {
	return &e
}
func (e *ModeNewrelicEvents) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeNewrelicEvents(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeNewrelicEvents: %v", v)
	}
}

type PqControlsNewrelicEvents struct {
}

type OutputNewrelicEvents struct {
	// Unique ID for this output
	ID   *string             `json:"id,omitempty"`
	Type *TypeNewrelicEvents `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Which New Relic region endpoint to use.
	Region *RegionNewrelicEvents `default:"US" json:"region"`
	// New Relic account ID
	AccountID string `json:"accountId"`
	// Default eventType to use when not present in an event. For more information, see [here](https://docs.newrelic.com/docs/telemetry-data-platform/custom-data/custom-events/data-requirements-limits-custom-event-data/#reserved-words).
	EventType string `json:"eventType"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"1024" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Defaults to Yes. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max body size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events.
	ExtraHTTPHeaders []ExtraHTTPHeaderNewrelicEvents `json:"extraHttpHeaders,omitempty"`
	// Enables round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeNewrelicEvents `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable).
	ResponseRetrySettings []ResponseRetrySettingNewrelicEvents `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsNewrelicEvents  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorNewrelicEvents `default:"block" json:"onBackpressure"`
	// Enter API key directly, or select a stored secret
	AuthType    *AuthenticationMethodNewrelicEvents `default:"manual" json:"authType"`
	Description *string                             `json:"description,omitempty"`
	CustomURL   *string                             `json:"customUrl,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *CompressionNewrelicEvents `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorNewrelicEvents `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeNewrelicEvents       `default:"error" json:"pqMode"`
	PqControls *PqControlsNewrelicEvents `json:"pqControls,omitempty"`
	// New Relic API key. Can be overridden using __newRelic_apiKey field.
	APIKey *string `json:"apiKey,omitempty"`
	// Select or create a stored text secret
	TextSecret *string   `json:"textSecret,omitempty"`
	Status     *TFStatus `json:"status,omitempty"`
}

func (o OutputNewrelicEvents) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputNewrelicEvents) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputNewrelicEvents) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputNewrelicEvents) GetType() *TypeNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputNewrelicEvents) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputNewrelicEvents) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputNewrelicEvents) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputNewrelicEvents) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputNewrelicEvents) GetRegion() *RegionNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputNewrelicEvents) GetAccountID() string {
	if o == nil {
		return ""
	}
	return o.AccountID
}

func (o *OutputNewrelicEvents) GetEventType() string {
	if o == nil {
		return ""
	}
	return o.EventType
}

func (o *OutputNewrelicEvents) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputNewrelicEvents) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputNewrelicEvents) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputNewrelicEvents) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputNewrelicEvents) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputNewrelicEvents) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputNewrelicEvents) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputNewrelicEvents) GetExtraHTTPHeaders() []ExtraHTTPHeaderNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputNewrelicEvents) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputNewrelicEvents) GetFailedRequestLoggingMode() *FailedRequestLoggingModeNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputNewrelicEvents) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputNewrelicEvents) GetResponseRetrySettings() []ResponseRetrySettingNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputNewrelicEvents) GetTimeoutRetrySettings() *TimeoutRetrySettingsNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputNewrelicEvents) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputNewrelicEvents) GetOnBackpressure() *BackpressureBehaviorNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputNewrelicEvents) GetAuthType() *AuthenticationMethodNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputNewrelicEvents) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputNewrelicEvents) GetCustomURL() *string {
	if o == nil {
		return nil
	}
	return o.CustomURL
}

func (o *OutputNewrelicEvents) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputNewrelicEvents) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputNewrelicEvents) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputNewrelicEvents) GetPqCompress() *CompressionNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputNewrelicEvents) GetPqOnBackpressure() *QueueFullBehaviorNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputNewrelicEvents) GetPqMode() *ModeNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputNewrelicEvents) GetPqControls() *PqControlsNewrelicEvents {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputNewrelicEvents) GetAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.APIKey
}

func (o *OutputNewrelicEvents) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputNewrelicEvents) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeNewrelic string

const (
	TypeNewrelicNewrelic TypeNewrelic = "newrelic"
)

func (e TypeNewrelic) ToPointer() *TypeNewrelic {
	return &e
}
func (e *TypeNewrelic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "newrelic":
		*e = TypeNewrelic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeNewrelic: %v", v)
	}
}

// RegionNewrelic - Which New Relic region endpoint to use.
type RegionNewrelic string

const (
	RegionNewrelicUs     RegionNewrelic = "US"
	RegionNewrelicEu     RegionNewrelic = "EU"
	RegionNewrelicCustom RegionNewrelic = "Custom"
)

func (e RegionNewrelic) ToPointer() *RegionNewrelic {
	return &e
}
func (e *RegionNewrelic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "US":
		fallthrough
	case "EU":
		fallthrough
	case "Custom":
		*e = RegionNewrelic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for RegionNewrelic: %v", v)
	}
}

type FieldName string

const (
	FieldNameService   FieldName = "service"
	FieldNameHostname  FieldName = "hostname"
	FieldNameTimestamp FieldName = "timestamp"
	FieldNameAuditID   FieldName = "auditId"
)

func (e FieldName) ToPointer() *FieldName {
	return &e
}
func (e *FieldName) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "service":
		fallthrough
	case "hostname":
		fallthrough
	case "timestamp":
		fallthrough
	case "auditId":
		*e = FieldName(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FieldName: %v", v)
	}
}

type MetadatumNewrelic struct {
	Name FieldName `json:"name"`
	// JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)
	Value string `json:"value"`
}

func (o *MetadatumNewrelic) GetName() FieldName {
	if o == nil {
		return FieldName("")
	}
	return o.Name
}

func (o *MetadatumNewrelic) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type ExtraHTTPHeaderNewrelic struct {
	// Field name
	Name *string `json:"name,omitempty"`
	// Field value
	Value string `json:"value"`
}

func (o *ExtraHTTPHeaderNewrelic) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderNewrelic) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeNewrelic - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeNewrelic string

const (
	FailedRequestLoggingModeNewrelicPayload           FailedRequestLoggingModeNewrelic = "payload"
	FailedRequestLoggingModeNewrelicPayloadAndHeaders FailedRequestLoggingModeNewrelic = "payloadAndHeaders"
	FailedRequestLoggingModeNewrelicNone              FailedRequestLoggingModeNewrelic = "none"
)

func (e FailedRequestLoggingModeNewrelic) ToPointer() *FailedRequestLoggingModeNewrelic {
	return &e
}
func (e *FailedRequestLoggingModeNewrelic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeNewrelic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeNewrelic: %v", v)
	}
}

type ResponseRetrySettingNewrelic struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingNewrelic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingNewrelic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingNewrelic) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingNewrelic) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingNewrelic) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingNewrelic) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsNewrelic struct {
	// Enable to retry on request timeout
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsNewrelic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsNewrelic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsNewrelic) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsNewrelic) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsNewrelic) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsNewrelic) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorNewrelic - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorNewrelic string

const (
	BackpressureBehaviorNewrelicBlock BackpressureBehaviorNewrelic = "block"
	BackpressureBehaviorNewrelicDrop  BackpressureBehaviorNewrelic = "drop"
	BackpressureBehaviorNewrelicQueue BackpressureBehaviorNewrelic = "queue"
)

func (e BackpressureBehaviorNewrelic) ToPointer() *BackpressureBehaviorNewrelic {
	return &e
}
func (e *BackpressureBehaviorNewrelic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorNewrelic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorNewrelic: %v", v)
	}
}

// AuthenticationMethodNewrelic - Enter API key directly, or select a stored secret
type AuthenticationMethodNewrelic string

const (
	AuthenticationMethodNewrelicManual AuthenticationMethodNewrelic = "manual"
	AuthenticationMethodNewrelicSecret AuthenticationMethodNewrelic = "secret"
)

func (e AuthenticationMethodNewrelic) ToPointer() *AuthenticationMethodNewrelic {
	return &e
}
func (e *AuthenticationMethodNewrelic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		*e = AuthenticationMethodNewrelic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodNewrelic: %v", v)
	}
}

// CompressionNewrelic - Codec to use to compress the persisted data.
type CompressionNewrelic string

const (
	CompressionNewrelicNone CompressionNewrelic = "none"
	CompressionNewrelicGzip CompressionNewrelic = "gzip"
)

func (e CompressionNewrelic) ToPointer() *CompressionNewrelic {
	return &e
}
func (e *CompressionNewrelic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionNewrelic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionNewrelic: %v", v)
	}
}

// QueueFullBehaviorNewrelic - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorNewrelic string

const (
	QueueFullBehaviorNewrelicBlock QueueFullBehaviorNewrelic = "block"
	QueueFullBehaviorNewrelicDrop  QueueFullBehaviorNewrelic = "drop"
)

func (e QueueFullBehaviorNewrelic) ToPointer() *QueueFullBehaviorNewrelic {
	return &e
}
func (e *QueueFullBehaviorNewrelic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorNewrelic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorNewrelic: %v", v)
	}
}

// ModeNewrelic - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type ModeNewrelic string

const (
	ModeNewrelicError        ModeNewrelic = "error"
	ModeNewrelicBackpressure ModeNewrelic = "backpressure"
	ModeNewrelicAlways       ModeNewrelic = "always"
)

func (e ModeNewrelic) ToPointer() *ModeNewrelic {
	return &e
}
func (e *ModeNewrelic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeNewrelic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeNewrelic: %v", v)
	}
}

type PqControlsNewrelic struct {
}

type OutputNewrelic struct {
	// Unique ID for this output
	ID   string       `json:"id"`
	Type TypeNewrelic `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Which New Relic region endpoint to use.
	Region *RegionNewrelic `default:"US" json:"region"`
	// Name of the logtype to send with events, e.g.: observability, access_log. The event's 'sourcetype' field (if set) will override this value.
	LogType *string `default:"" json:"logType"`
	// Name of field to send as log message value. If not present, event will be serialized and sent as JSON.
	MessageField *string `default:"" json:"messageField"`
	// Fields to add to events from this input
	Metadata []MetadatumNewrelic `json:"metadata,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"1024" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Defaults to Yes. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max body size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events.
	ExtraHTTPHeaders []ExtraHTTPHeaderNewrelic `json:"extraHttpHeaders,omitempty"`
	// Enables round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeNewrelic `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable).
	ResponseRetrySettings []ResponseRetrySettingNewrelic `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsNewrelic  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorNewrelic `default:"block" json:"onBackpressure"`
	// Enter API key directly, or select a stored secret
	AuthType *AuthenticationMethodNewrelic `default:"manual" json:"authType"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	Description        *string  `json:"description,omitempty"`
	CustomURL          *string  `json:"customUrl,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *CompressionNewrelic `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorNewrelic `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeNewrelic       `default:"error" json:"pqMode"`
	PqControls *PqControlsNewrelic `json:"pqControls,omitempty"`
	// New Relic API key. Can be overridden using __newRelic_apiKey field.
	APIKey *string `json:"apiKey,omitempty"`
	// Select or create a stored text secret
	TextSecret *string   `json:"textSecret,omitempty"`
	Status     *TFStatus `json:"status,omitempty"`
}

func (o OutputNewrelic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputNewrelic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputNewrelic) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputNewrelic) GetType() TypeNewrelic {
	if o == nil {
		return TypeNewrelic("")
	}
	return o.Type
}

func (o *OutputNewrelic) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputNewrelic) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputNewrelic) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputNewrelic) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputNewrelic) GetRegion() *RegionNewrelic {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputNewrelic) GetLogType() *string {
	if o == nil {
		return nil
	}
	return o.LogType
}

func (o *OutputNewrelic) GetMessageField() *string {
	if o == nil {
		return nil
	}
	return o.MessageField
}

func (o *OutputNewrelic) GetMetadata() []MetadatumNewrelic {
	if o == nil {
		return nil
	}
	return o.Metadata
}

func (o *OutputNewrelic) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputNewrelic) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputNewrelic) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputNewrelic) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputNewrelic) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputNewrelic) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputNewrelic) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputNewrelic) GetExtraHTTPHeaders() []ExtraHTTPHeaderNewrelic {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputNewrelic) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputNewrelic) GetFailedRequestLoggingMode() *FailedRequestLoggingModeNewrelic {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputNewrelic) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputNewrelic) GetResponseRetrySettings() []ResponseRetrySettingNewrelic {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputNewrelic) GetTimeoutRetrySettings() *TimeoutRetrySettingsNewrelic {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputNewrelic) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputNewrelic) GetOnBackpressure() *BackpressureBehaviorNewrelic {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputNewrelic) GetAuthType() *AuthenticationMethodNewrelic {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputNewrelic) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputNewrelic) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputNewrelic) GetCustomURL() *string {
	if o == nil {
		return nil
	}
	return o.CustomURL
}

func (o *OutputNewrelic) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputNewrelic) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputNewrelic) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputNewrelic) GetPqCompress() *CompressionNewrelic {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputNewrelic) GetPqOnBackpressure() *QueueFullBehaviorNewrelic {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputNewrelic) GetPqMode() *ModeNewrelic {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputNewrelic) GetPqControls() *PqControlsNewrelic {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputNewrelic) GetAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.APIKey
}

func (o *OutputNewrelic) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputNewrelic) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeElasticCloud string

const (
	TypeElasticCloudElasticCloud TypeElasticCloud = "elastic_cloud"
)

func (e TypeElasticCloud) ToPointer() *TypeElasticCloud {
	return &e
}
func (e *TypeElasticCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "elastic_cloud":
		*e = TypeElasticCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeElasticCloud: %v", v)
	}
}

type ExtraHTTPHeaderElasticCloud struct {
	// Field name
	Name *string `json:"name,omitempty"`
	// Field value
	Value string `json:"value"`
}

func (o *ExtraHTTPHeaderElasticCloud) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderElasticCloud) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeElasticCloud - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeElasticCloud string

const (
	FailedRequestLoggingModeElasticCloudPayload           FailedRequestLoggingModeElasticCloud = "payload"
	FailedRequestLoggingModeElasticCloudPayloadAndHeaders FailedRequestLoggingModeElasticCloud = "payloadAndHeaders"
	FailedRequestLoggingModeElasticCloudNone              FailedRequestLoggingModeElasticCloud = "none"
)

func (e FailedRequestLoggingModeElasticCloud) ToPointer() *FailedRequestLoggingModeElasticCloud {
	return &e
}
func (e *FailedRequestLoggingModeElasticCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeElasticCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeElasticCloud: %v", v)
	}
}

type ExtraParamElasticCloud struct {
	// Field name
	Name string `json:"name"`
	// Field value
	Value string `json:"value"`
}

func (o *ExtraParamElasticCloud) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *ExtraParamElasticCloud) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// AuthenticationMethodElasticCloud - Enter credentials directly, or select a stored secret
type AuthenticationMethodElasticCloud string

const (
	AuthenticationMethodElasticCloudManual       AuthenticationMethodElasticCloud = "manual"
	AuthenticationMethodElasticCloudSecret       AuthenticationMethodElasticCloud = "secret"
	AuthenticationMethodElasticCloudManualAPIKey AuthenticationMethodElasticCloud = "manualAPIKey"
	AuthenticationMethodElasticCloudTextSecret   AuthenticationMethodElasticCloud = "textSecret"
)

func (e AuthenticationMethodElasticCloud) ToPointer() *AuthenticationMethodElasticCloud {
	return &e
}
func (e *AuthenticationMethodElasticCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		fallthrough
	case "manualAPIKey":
		fallthrough
	case "textSecret":
		*e = AuthenticationMethodElasticCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodElasticCloud: %v", v)
	}
}

type AuthElasticCloud struct {
	Disabled *bool `default:"false" json:"disabled"`
	// Enter credentials directly, or select a stored secret
	AuthType *AuthenticationMethodElasticCloud `default:"manual" json:"authType"`
}

func (a AuthElasticCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(a, "", false)
}

func (a *AuthElasticCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &a, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *AuthElasticCloud) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *AuthElasticCloud) GetAuthType() *AuthenticationMethodElasticCloud {
	if o == nil {
		return nil
	}
	return o.AuthType
}

type ResponseRetrySettingElasticCloud struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingElasticCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingElasticCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingElasticCloud) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingElasticCloud) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingElasticCloud) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingElasticCloud) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsElasticCloud struct {
	// Enable to retry on request timeout
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsElasticCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsElasticCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsElasticCloud) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsElasticCloud) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsElasticCloud) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsElasticCloud) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorElasticCloud - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorElasticCloud string

const (
	BackpressureBehaviorElasticCloudBlock BackpressureBehaviorElasticCloud = "block"
	BackpressureBehaviorElasticCloudDrop  BackpressureBehaviorElasticCloud = "drop"
	BackpressureBehaviorElasticCloudQueue BackpressureBehaviorElasticCloud = "queue"
)

func (e BackpressureBehaviorElasticCloud) ToPointer() *BackpressureBehaviorElasticCloud {
	return &e
}
func (e *BackpressureBehaviorElasticCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorElasticCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorElasticCloud: %v", v)
	}
}

// CompressionElasticCloud - Codec to use to compress the persisted data.
type CompressionElasticCloud string

const (
	CompressionElasticCloudNone CompressionElasticCloud = "none"
	CompressionElasticCloudGzip CompressionElasticCloud = "gzip"
)

func (e CompressionElasticCloud) ToPointer() *CompressionElasticCloud {
	return &e
}
func (e *CompressionElasticCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionElasticCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionElasticCloud: %v", v)
	}
}

// QueueFullBehaviorElasticCloud - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorElasticCloud string

const (
	QueueFullBehaviorElasticCloudBlock QueueFullBehaviorElasticCloud = "block"
	QueueFullBehaviorElasticCloudDrop  QueueFullBehaviorElasticCloud = "drop"
)

func (e QueueFullBehaviorElasticCloud) ToPointer() *QueueFullBehaviorElasticCloud {
	return &e
}
func (e *QueueFullBehaviorElasticCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorElasticCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorElasticCloud: %v", v)
	}
}

// ModeElasticCloud - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type ModeElasticCloud string

const (
	ModeElasticCloudError        ModeElasticCloud = "error"
	ModeElasticCloudBackpressure ModeElasticCloud = "backpressure"
	ModeElasticCloudAlways       ModeElasticCloud = "always"
)

func (e ModeElasticCloud) ToPointer() *ModeElasticCloud {
	return &e
}
func (e *ModeElasticCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeElasticCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeElasticCloud: %v", v)
	}
}

type PqControlsElasticCloud struct {
}

type OutputElasticCloud struct {
	// Unique ID for this output
	ID   *string           `json:"id,omitempty"`
	Type *TypeElasticCloud `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Enter Cloud ID of the Elastic Cloud environment to send events to
	URL string `json:"url"`
	// Data stream or index to send events to. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be overwritten by an event's __index field
	Index string `json:"index"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Defaults to Yes. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max body size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events.
	ExtraHTTPHeaders []ExtraHTTPHeaderElasticCloud `json:"extraHttpHeaders,omitempty"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeElasticCloud `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Extra parameters to use in HTTP requests
	ExtraParams []ExtraParamElasticCloud `json:"extraParams,omitempty"`
	Auth        *AuthElasticCloud        `json:"auth,omitempty"`
	// Optional Elastic Cloud Destination pipeline
	ElasticPipeline *string `json:"elasticPipeline,omitempty"`
	// Toggle to No when sending events to an Elastic TSDS (time series data stream)
	IncludeDocID *bool `default:"true" json:"includeDocId"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable).
	ResponseRetrySettings []ResponseRetrySettingElasticCloud `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsElasticCloud  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorElasticCloud `default:"block" json:"onBackpressure"`
	Description    *string                           `json:"description,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *CompressionElasticCloud `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorElasticCloud `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeElasticCloud       `default:"error" json:"pqMode"`
	PqControls *PqControlsElasticCloud `json:"pqControls,omitempty"`
	Status     *TFStatus               `json:"status,omitempty"`
}

func (o OutputElasticCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputElasticCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputElasticCloud) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputElasticCloud) GetType() *TypeElasticCloud {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputElasticCloud) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputElasticCloud) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputElasticCloud) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputElasticCloud) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputElasticCloud) GetURL() string {
	if o == nil {
		return ""
	}
	return o.URL
}

func (o *OutputElasticCloud) GetIndex() string {
	if o == nil {
		return ""
	}
	return o.Index
}

func (o *OutputElasticCloud) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputElasticCloud) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputElasticCloud) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputElasticCloud) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputElasticCloud) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputElasticCloud) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputElasticCloud) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputElasticCloud) GetExtraHTTPHeaders() []ExtraHTTPHeaderElasticCloud {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputElasticCloud) GetFailedRequestLoggingMode() *FailedRequestLoggingModeElasticCloud {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputElasticCloud) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputElasticCloud) GetExtraParams() []ExtraParamElasticCloud {
	if o == nil {
		return nil
	}
	return o.ExtraParams
}

func (o *OutputElasticCloud) GetAuth() *AuthElasticCloud {
	if o == nil {
		return nil
	}
	return o.Auth
}

func (o *OutputElasticCloud) GetElasticPipeline() *string {
	if o == nil {
		return nil
	}
	return o.ElasticPipeline
}

func (o *OutputElasticCloud) GetIncludeDocID() *bool {
	if o == nil {
		return nil
	}
	return o.IncludeDocID
}

func (o *OutputElasticCloud) GetResponseRetrySettings() []ResponseRetrySettingElasticCloud {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputElasticCloud) GetTimeoutRetrySettings() *TimeoutRetrySettingsElasticCloud {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputElasticCloud) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputElasticCloud) GetOnBackpressure() *BackpressureBehaviorElasticCloud {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputElasticCloud) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputElasticCloud) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputElasticCloud) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputElasticCloud) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputElasticCloud) GetPqCompress() *CompressionElasticCloud {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputElasticCloud) GetPqOnBackpressure() *QueueFullBehaviorElasticCloud {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputElasticCloud) GetPqMode() *ModeElasticCloud {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputElasticCloud) GetPqControls() *PqControlsElasticCloud {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputElasticCloud) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type OutputTypeElastic string

const (
	OutputTypeElasticElastic OutputTypeElastic = "elastic"
)

func (e OutputTypeElastic) ToPointer() *OutputTypeElastic {
	return &e
}
func (e *OutputTypeElastic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "elastic":
		*e = OutputTypeElastic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeElastic: %v", v)
	}
}

type OutputExtraHTTPHeaderElastic struct {
	// Field name
	Name *string `json:"name,omitempty"`
	// Field value
	Value string `json:"value"`
}

func (o *OutputExtraHTTPHeaderElastic) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *OutputExtraHTTPHeaderElastic) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeElastic - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeElastic string

const (
	FailedRequestLoggingModeElasticPayload           FailedRequestLoggingModeElastic = "payload"
	FailedRequestLoggingModeElasticPayloadAndHeaders FailedRequestLoggingModeElastic = "payloadAndHeaders"
	FailedRequestLoggingModeElasticNone              FailedRequestLoggingModeElastic = "none"
)

func (e FailedRequestLoggingModeElastic) ToPointer() *FailedRequestLoggingModeElastic {
	return &e
}
func (e *FailedRequestLoggingModeElastic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeElastic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeElastic: %v", v)
	}
}

type ResponseRetrySettingElastic struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingElastic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingElastic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingElastic) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingElastic) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingElastic) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingElastic) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsElastic struct {
	// Enable to retry on request timeout
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsElastic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsElastic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsElastic) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsElastic) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsElastic) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsElastic) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type ExtraParamElastic struct {
	// Field name
	Name string `json:"name"`
	// Field value
	Value string `json:"value"`
}

func (o *ExtraParamElastic) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *ExtraParamElastic) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// AuthAuthenticationMethodElastic - Enter credentials directly, or select a stored secret
type AuthAuthenticationMethodElastic string

const (
	AuthAuthenticationMethodElasticManual       AuthAuthenticationMethodElastic = "manual"
	AuthAuthenticationMethodElasticSecret       AuthAuthenticationMethodElastic = "secret"
	AuthAuthenticationMethodElasticManualAPIKey AuthAuthenticationMethodElastic = "manualAPIKey"
	AuthAuthenticationMethodElasticTextSecret   AuthAuthenticationMethodElastic = "textSecret"
)

func (e AuthAuthenticationMethodElastic) ToPointer() *AuthAuthenticationMethodElastic {
	return &e
}
func (e *AuthAuthenticationMethodElastic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		fallthrough
	case "manualAPIKey":
		fallthrough
	case "textSecret":
		*e = AuthAuthenticationMethodElastic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthAuthenticationMethodElastic: %v", v)
	}
}

type AuthElastic struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Enter credentials directly, or select a stored secret
	AuthType *AuthAuthenticationMethodElastic `default:"manual" json:"authType"`
}

func (a AuthElastic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(a, "", false)
}

func (a *AuthElastic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &a, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *AuthElastic) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *AuthElastic) GetAuthType() *AuthAuthenticationMethodElastic {
	if o == nil {
		return nil
	}
	return o.AuthType
}

// ElasticVersion - Optional Elasticsearch version, used to format events. If not specified, will auto-discover version.
type ElasticVersion string

const (
	ElasticVersionAuto  ElasticVersion = "auto"
	ElasticVersionSix   ElasticVersion = "6"
	ElasticVersionSeven ElasticVersion = "7"
)

func (e ElasticVersion) ToPointer() *ElasticVersion {
	return &e
}
func (e *ElasticVersion) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "auto":
		fallthrough
	case "6":
		fallthrough
	case "7":
		*e = ElasticVersion(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ElasticVersion: %v", v)
	}
}

// WriteAction - Action to use when writing events. Must be set to `Create` when writing to a data stream.
type WriteAction string

const (
	WriteActionIndex  WriteAction = "index"
	WriteActionCreate WriteAction = "create"
)

func (e WriteAction) ToPointer() *WriteAction {
	return &e
}
func (e *WriteAction) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "index":
		fallthrough
	case "create":
		*e = WriteAction(v)
		return nil
	default:
		return fmt.Errorf("invalid value for WriteAction: %v", v)
	}
}

// BackpressureBehaviorElastic - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorElastic string

const (
	BackpressureBehaviorElasticBlock BackpressureBehaviorElastic = "block"
	BackpressureBehaviorElasticDrop  BackpressureBehaviorElastic = "drop"
	BackpressureBehaviorElasticQueue BackpressureBehaviorElastic = "queue"
)

func (e BackpressureBehaviorElastic) ToPointer() *BackpressureBehaviorElastic {
	return &e
}
func (e *BackpressureBehaviorElastic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorElastic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorElastic: %v", v)
	}
}

type URLElastic struct {
	// The URL to an Elastic node to send events to. Example: http://elastic:9200/_bulk
	URL string `json:"url"`
	// Assign a weight (>0) to each endpoint to indicate its traffic-handling capability
	Weight *float64 `default:"1" json:"weight"`
}

func (u URLElastic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(u, "", false)
}

func (u *URLElastic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &u, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *URLElastic) GetURL() string {
	if o == nil {
		return ""
	}
	return o.URL
}

func (o *URLElastic) GetWeight() *float64 {
	if o == nil {
		return nil
	}
	return o.Weight
}

// PqCompressCompressionElastic - Codec to use to compress the persisted data.
type PqCompressCompressionElastic string

const (
	PqCompressCompressionElasticNone PqCompressCompressionElastic = "none"
	PqCompressCompressionElasticGzip PqCompressCompressionElastic = "gzip"
)

func (e PqCompressCompressionElastic) ToPointer() *PqCompressCompressionElastic {
	return &e
}
func (e *PqCompressCompressionElastic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = PqCompressCompressionElastic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PqCompressCompressionElastic: %v", v)
	}
}

// QueueFullBehaviorElastic - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorElastic string

const (
	QueueFullBehaviorElasticBlock QueueFullBehaviorElastic = "block"
	QueueFullBehaviorElasticDrop  QueueFullBehaviorElastic = "drop"
)

func (e QueueFullBehaviorElastic) ToPointer() *QueueFullBehaviorElastic {
	return &e
}
func (e *QueueFullBehaviorElastic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorElastic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorElastic: %v", v)
	}
}

// OutputModeElastic - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type OutputModeElastic string

const (
	OutputModeElasticError        OutputModeElastic = "error"
	OutputModeElasticBackpressure OutputModeElastic = "backpressure"
	OutputModeElasticAlways       OutputModeElastic = "always"
)

func (e OutputModeElastic) ToPointer() *OutputModeElastic {
	return &e
}
func (e *OutputModeElastic) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = OutputModeElastic(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputModeElastic: %v", v)
	}
}

type PqControlsElastic struct {
}

type OutputElastic struct {
	// Unique ID for this output
	ID   *string           `json:"id,omitempty"`
	Type OutputTypeElastic `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.
	LoadBalanced *bool `default:"true" json:"loadBalanced"`
	// Index or data stream to send events to. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be overwritten by an event's __index field.
	Index string `json:"index"`
	// Document type to use for events. Can be overwritten by an event's __type field
	DocType *string `json:"docType,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Defaults to Yes. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max body size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events.
	ExtraHTTPHeaders []OutputExtraHTTPHeaderElastic `json:"extraHttpHeaders,omitempty"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeElastic `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable).
	ResponseRetrySettings []ResponseRetrySettingElastic `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsElastic  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// Extra Parameters.
	ExtraParams []ExtraParamElastic `json:"extraParams,omitempty"`
	Auth        *AuthElastic        `json:"auth,omitempty"`
	// Optional Elasticsearch version, used to format events. If not specified, will auto-discover version.
	ElasticVersion *ElasticVersion `default:"auto" json:"elasticVersion"`
	// Optional Elasticsearch destination pipeline
	ElasticPipeline *string `json:"elasticPipeline,omitempty"`
	// Toggle this off when sending events to an Elastic TSDS (time series data stream) or to allow Elastic to generate document IDs
	IncludeDocID *bool `default:"false" json:"includeDocId"`
	// Action to use when writing events. Must be set to `Create` when writing to a data stream.
	WriteAction *WriteAction `default:"create" json:"writeAction"`
	// Retry failed events when a bulk request to Elastic is successful, but the response body returns an error for one or more events in the batch
	RetryPartialErrors *bool `default:"false" json:"retryPartialErrors"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorElastic `default:"block" json:"onBackpressure"`
	Description    *string                      `json:"description,omitempty"`
	// The Cloud ID or URL to an Elastic cluster to send events to. Example: http://elastic:9200/_bulk
	URL *string `json:"url,omitempty"`
	// Enables round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Exclude all IPs of the current host from the list of any resolved hostnames.
	ExcludeSelf *bool        `default:"false" json:"excludeSelf"`
	Urls        []URLElastic `json:"urls,omitempty"`
	// Re-resolve any hostnames every this many seconds and pick up destinations from A records.
	DNSResolvePeriodSec *float64 `default:"600" json:"dnsResolvePeriodSec"`
	// How far back in time to keep traffic stats for load balancing purposes.
	LoadBalanceStatsPeriodSec *float64 `default:"300" json:"loadBalanceStatsPeriodSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *PqCompressCompressionElastic `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorElastic `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *OutputModeElastic `default:"error" json:"pqMode"`
	PqControls *PqControlsElastic `json:"pqControls,omitempty"`
	Status     *TFStatus          `json:"status,omitempty"`
}

func (o OutputElastic) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputElastic) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputElastic) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputElastic) GetType() OutputTypeElastic {
	if o == nil {
		return OutputTypeElastic("")
	}
	return o.Type
}

func (o *OutputElastic) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputElastic) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputElastic) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputElastic) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputElastic) GetLoadBalanced() *bool {
	if o == nil {
		return nil
	}
	return o.LoadBalanced
}

func (o *OutputElastic) GetIndex() string {
	if o == nil {
		return ""
	}
	return o.Index
}

func (o *OutputElastic) GetDocType() *string {
	if o == nil {
		return nil
	}
	return o.DocType
}

func (o *OutputElastic) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputElastic) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputElastic) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputElastic) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputElastic) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputElastic) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputElastic) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputElastic) GetExtraHTTPHeaders() []OutputExtraHTTPHeaderElastic {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputElastic) GetFailedRequestLoggingMode() *FailedRequestLoggingModeElastic {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputElastic) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputElastic) GetResponseRetrySettings() []ResponseRetrySettingElastic {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputElastic) GetTimeoutRetrySettings() *TimeoutRetrySettingsElastic {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputElastic) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputElastic) GetExtraParams() []ExtraParamElastic {
	if o == nil {
		return nil
	}
	return o.ExtraParams
}

func (o *OutputElastic) GetAuth() *AuthElastic {
	if o == nil {
		return nil
	}
	return o.Auth
}

func (o *OutputElastic) GetElasticVersion() *ElasticVersion {
	if o == nil {
		return nil
	}
	return o.ElasticVersion
}

func (o *OutputElastic) GetElasticPipeline() *string {
	if o == nil {
		return nil
	}
	return o.ElasticPipeline
}

func (o *OutputElastic) GetIncludeDocID() *bool {
	if o == nil {
		return nil
	}
	return o.IncludeDocID
}

func (o *OutputElastic) GetWriteAction() *WriteAction {
	if o == nil {
		return nil
	}
	return o.WriteAction
}

func (o *OutputElastic) GetRetryPartialErrors() *bool {
	if o == nil {
		return nil
	}
	return o.RetryPartialErrors
}

func (o *OutputElastic) GetOnBackpressure() *BackpressureBehaviorElastic {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputElastic) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputElastic) GetURL() *string {
	if o == nil {
		return nil
	}
	return o.URL
}

func (o *OutputElastic) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputElastic) GetExcludeSelf() *bool {
	if o == nil {
		return nil
	}
	return o.ExcludeSelf
}

func (o *OutputElastic) GetUrls() []URLElastic {
	if o == nil {
		return nil
	}
	return o.Urls
}

func (o *OutputElastic) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputElastic) GetLoadBalanceStatsPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.LoadBalanceStatsPeriodSec
}

func (o *OutputElastic) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputElastic) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputElastic) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputElastic) GetPqCompress() *PqCompressCompressionElastic {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputElastic) GetPqOnBackpressure() *QueueFullBehaviorElastic {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputElastic) GetPqMode() *OutputModeElastic {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputElastic) GetPqControls() *PqControlsElastic {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputElastic) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type OutputTypeMsk string

const (
	OutputTypeMskMsk OutputTypeMsk = "msk"
)

func (e OutputTypeMsk) ToPointer() *OutputTypeMsk {
	return &e
}
func (e *OutputTypeMsk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "msk":
		*e = OutputTypeMsk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeMsk: %v", v)
	}
}

// AcknowledgmentsMsk - Control the number of required acknowledgments.
type AcknowledgmentsMsk int64

const (
	AcknowledgmentsMskOne    AcknowledgmentsMsk = 1
	AcknowledgmentsMskZero   AcknowledgmentsMsk = 0
	AcknowledgmentsMskMinus1 AcknowledgmentsMsk = -1
)

func (e AcknowledgmentsMsk) ToPointer() *AcknowledgmentsMsk {
	return &e
}
func (e *AcknowledgmentsMsk) UnmarshalJSON(data []byte) error {
	var v int64
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case 1:
		fallthrough
	case 0:
		fallthrough
	case -1:
		*e = AcknowledgmentsMsk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AcknowledgmentsMsk: %v", v)
	}
}

// RecordDataFormatMsk - Format to use to serialize events before writing to Kafka.
type RecordDataFormatMsk string

const (
	RecordDataFormatMskJSON     RecordDataFormatMsk = "json"
	RecordDataFormatMskRaw      RecordDataFormatMsk = "raw"
	RecordDataFormatMskProtobuf RecordDataFormatMsk = "protobuf"
)

func (e RecordDataFormatMsk) ToPointer() *RecordDataFormatMsk {
	return &e
}
func (e *RecordDataFormatMsk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "json":
		fallthrough
	case "raw":
		fallthrough
	case "protobuf":
		*e = RecordDataFormatMsk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for RecordDataFormatMsk: %v", v)
	}
}

// OutputCompressionMsk - Codec to use to compress the data before sending to Kafka
type OutputCompressionMsk string

const (
	OutputCompressionMskNone   OutputCompressionMsk = "none"
	OutputCompressionMskGzip   OutputCompressionMsk = "gzip"
	OutputCompressionMskSnappy OutputCompressionMsk = "snappy"
	OutputCompressionMskLz4    OutputCompressionMsk = "lz4"
)

func (e OutputCompressionMsk) ToPointer() *OutputCompressionMsk {
	return &e
}
func (e *OutputCompressionMsk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		fallthrough
	case "snappy":
		fallthrough
	case "lz4":
		*e = OutputCompressionMsk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputCompressionMsk: %v", v)
	}
}

// OutputAuthMsk - Credentials to use when authenticating with the schema registry using basic HTTP authentication
type OutputAuthMsk struct {
	// Enable authentication
	Disabled *bool `default:"true" json:"disabled"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
}

func (o OutputAuthMsk) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputAuthMsk) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *OutputAuthMsk) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *OutputAuthMsk) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

// OutputKafkaSchemaRegistryMinimumTLSVersionMsk - Minimum TLS version to use when connecting
type OutputKafkaSchemaRegistryMinimumTLSVersionMsk string

const (
	OutputKafkaSchemaRegistryMinimumTLSVersionMskTlSv1  OutputKafkaSchemaRegistryMinimumTLSVersionMsk = "TLSv1"
	OutputKafkaSchemaRegistryMinimumTLSVersionMskTlSv11 OutputKafkaSchemaRegistryMinimumTLSVersionMsk = "TLSv1.1"
	OutputKafkaSchemaRegistryMinimumTLSVersionMskTlSv12 OutputKafkaSchemaRegistryMinimumTLSVersionMsk = "TLSv1.2"
	OutputKafkaSchemaRegistryMinimumTLSVersionMskTlSv13 OutputKafkaSchemaRegistryMinimumTLSVersionMsk = "TLSv1.3"
)

func (e OutputKafkaSchemaRegistryMinimumTLSVersionMsk) ToPointer() *OutputKafkaSchemaRegistryMinimumTLSVersionMsk {
	return &e
}
func (e *OutputKafkaSchemaRegistryMinimumTLSVersionMsk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = OutputKafkaSchemaRegistryMinimumTLSVersionMsk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputKafkaSchemaRegistryMinimumTLSVersionMsk: %v", v)
	}
}

// OutputKafkaSchemaRegistryMaximumTLSVersionMsk - Maximum TLS version to use when connecting
type OutputKafkaSchemaRegistryMaximumTLSVersionMsk string

const (
	OutputKafkaSchemaRegistryMaximumTLSVersionMskTlSv1  OutputKafkaSchemaRegistryMaximumTLSVersionMsk = "TLSv1"
	OutputKafkaSchemaRegistryMaximumTLSVersionMskTlSv11 OutputKafkaSchemaRegistryMaximumTLSVersionMsk = "TLSv1.1"
	OutputKafkaSchemaRegistryMaximumTLSVersionMskTlSv12 OutputKafkaSchemaRegistryMaximumTLSVersionMsk = "TLSv1.2"
	OutputKafkaSchemaRegistryMaximumTLSVersionMskTlSv13 OutputKafkaSchemaRegistryMaximumTLSVersionMsk = "TLSv1.3"
)

func (e OutputKafkaSchemaRegistryMaximumTLSVersionMsk) ToPointer() *OutputKafkaSchemaRegistryMaximumTLSVersionMsk {
	return &e
}
func (e *OutputKafkaSchemaRegistryMaximumTLSVersionMsk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = OutputKafkaSchemaRegistryMaximumTLSVersionMsk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputKafkaSchemaRegistryMaximumTLSVersionMsk: %v", v)
	}
}

type OutputKafkaSchemaRegistryTLSSettingsClientSideMsk struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certs that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (e.g., the system's CA). Defaults to Yes. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate.
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key.
	Passphrase *string `json:"passphrase,omitempty"`
	// Minimum TLS version to use when connecting
	MinVersion *OutputKafkaSchemaRegistryMinimumTLSVersionMsk `json:"minVersion,omitempty"`
	// Maximum TLS version to use when connecting
	MaxVersion *OutputKafkaSchemaRegistryMaximumTLSVersionMsk `json:"maxVersion,omitempty"`
}

func (o OutputKafkaSchemaRegistryTLSSettingsClientSideMsk) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideMsk) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetServername() *string {
	if o == nil {
		return nil
	}
	return o.Servername
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetCaPath() *string {
	if o == nil {
		return nil
	}
	return o.CaPath
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetPrivKeyPath() *string {
	if o == nil {
		return nil
	}
	return o.PrivKeyPath
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetCertPath() *string {
	if o == nil {
		return nil
	}
	return o.CertPath
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetPassphrase() *string {
	if o == nil {
		return nil
	}
	return o.Passphrase
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetMinVersion() *OutputKafkaSchemaRegistryMinimumTLSVersionMsk {
	if o == nil {
		return nil
	}
	return o.MinVersion
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideMsk) GetMaxVersion() *OutputKafkaSchemaRegistryMaximumTLSVersionMsk {
	if o == nil {
		return nil
	}
	return o.MaxVersion
}

type OutputKafkaSchemaRegistryAuthenticationMsk struct {
	// Enable Schema Registry
	Disabled *bool `default:"true" json:"disabled"`
	// URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.
	SchemaRegistryURL *string `default:"http://localhost:8081" json:"schemaRegistryURL"`
	// Maximum time to wait for a Schema Registry connection to complete successfully
	ConnectionTimeout *float64 `default:"30000" json:"connectionTimeout"`
	// Maximum time to wait for the Schema Registry to respond to a request
	RequestTimeout *float64 `default:"30000" json:"requestTimeout"`
	// Maximum number of times to try fetching schemas from the Schema Registry
	MaxRetries *float64 `default:"1" json:"maxRetries"`
	// Credentials to use when authenticating with the schema registry using basic HTTP authentication
	Auth *OutputAuthMsk                                     `json:"auth,omitempty"`
	TLS  *OutputKafkaSchemaRegistryTLSSettingsClientSideMsk `json:"tls,omitempty"`
	// Used when __keySchemaIdOut is not present, to transform key values, leave blank if key transformation is not required by default.
	DefaultKeySchemaID *float64 `json:"defaultKeySchemaId,omitempty"`
	// Used when __valueSchemaIdOut is not present, to transform _raw, leave blank if value transformation is not required by default.
	DefaultValueSchemaID *float64 `json:"defaultValueSchemaId,omitempty"`
}

func (o OutputKafkaSchemaRegistryAuthenticationMsk) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputKafkaSchemaRegistryAuthenticationMsk) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *OutputKafkaSchemaRegistryAuthenticationMsk) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *OutputKafkaSchemaRegistryAuthenticationMsk) GetSchemaRegistryURL() *string {
	if o == nil {
		return nil
	}
	return o.SchemaRegistryURL
}

func (o *OutputKafkaSchemaRegistryAuthenticationMsk) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputKafkaSchemaRegistryAuthenticationMsk) GetRequestTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.RequestTimeout
}

func (o *OutputKafkaSchemaRegistryAuthenticationMsk) GetMaxRetries() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetries
}

func (o *OutputKafkaSchemaRegistryAuthenticationMsk) GetAuth() *OutputAuthMsk {
	if o == nil {
		return nil
	}
	return o.Auth
}

func (o *OutputKafkaSchemaRegistryAuthenticationMsk) GetTLS() *OutputKafkaSchemaRegistryTLSSettingsClientSideMsk {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputKafkaSchemaRegistryAuthenticationMsk) GetDefaultKeySchemaID() *float64 {
	if o == nil {
		return nil
	}
	return o.DefaultKeySchemaID
}

func (o *OutputKafkaSchemaRegistryAuthenticationMsk) GetDefaultValueSchemaID() *float64 {
	if o == nil {
		return nil
	}
	return o.DefaultValueSchemaID
}

// OutputAuthenticationMethodMsk - AWS authentication method. Choose Auto to use IAM roles.
type OutputAuthenticationMethodMsk string

const (
	OutputAuthenticationMethodMskAuto   OutputAuthenticationMethodMsk = "auto"
	OutputAuthenticationMethodMskManual OutputAuthenticationMethodMsk = "manual"
	OutputAuthenticationMethodMskSecret OutputAuthenticationMethodMsk = "secret"
)

func (e OutputAuthenticationMethodMsk) ToPointer() *OutputAuthenticationMethodMsk {
	return &e
}
func (e *OutputAuthenticationMethodMsk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "auto":
		fallthrough
	case "manual":
		fallthrough
	case "secret":
		*e = OutputAuthenticationMethodMsk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputAuthenticationMethodMsk: %v", v)
	}
}

// OutputSignatureVersionMsk - Signature version to use for signing MSK cluster requests
type OutputSignatureVersionMsk string

const (
	OutputSignatureVersionMskV2 OutputSignatureVersionMsk = "v2"
	OutputSignatureVersionMskV4 OutputSignatureVersionMsk = "v4"
)

func (e OutputSignatureVersionMsk) ToPointer() *OutputSignatureVersionMsk {
	return &e
}
func (e *OutputSignatureVersionMsk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "v2":
		fallthrough
	case "v4":
		*e = OutputSignatureVersionMsk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputSignatureVersionMsk: %v", v)
	}
}

// OutputMinimumTLSVersionMsk - Minimum TLS version to use when connecting
type OutputMinimumTLSVersionMsk string

const (
	OutputMinimumTLSVersionMskTlSv1  OutputMinimumTLSVersionMsk = "TLSv1"
	OutputMinimumTLSVersionMskTlSv11 OutputMinimumTLSVersionMsk = "TLSv1.1"
	OutputMinimumTLSVersionMskTlSv12 OutputMinimumTLSVersionMsk = "TLSv1.2"
	OutputMinimumTLSVersionMskTlSv13 OutputMinimumTLSVersionMsk = "TLSv1.3"
)

func (e OutputMinimumTLSVersionMsk) ToPointer() *OutputMinimumTLSVersionMsk {
	return &e
}
func (e *OutputMinimumTLSVersionMsk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = OutputMinimumTLSVersionMsk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMinimumTLSVersionMsk: %v", v)
	}
}

// OutputMaximumTLSVersionMsk - Maximum TLS version to use when connecting
type OutputMaximumTLSVersionMsk string

const (
	OutputMaximumTLSVersionMskTlSv1  OutputMaximumTLSVersionMsk = "TLSv1"
	OutputMaximumTLSVersionMskTlSv11 OutputMaximumTLSVersionMsk = "TLSv1.1"
	OutputMaximumTLSVersionMskTlSv12 OutputMaximumTLSVersionMsk = "TLSv1.2"
	OutputMaximumTLSVersionMskTlSv13 OutputMaximumTLSVersionMsk = "TLSv1.3"
)

func (e OutputMaximumTLSVersionMsk) ToPointer() *OutputMaximumTLSVersionMsk {
	return &e
}
func (e *OutputMaximumTLSVersionMsk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = OutputMaximumTLSVersionMsk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMaximumTLSVersionMsk: %v", v)
	}
}

type OutputTLSSettingsClientSideMsk struct {
	Disabled *bool `default:"false" json:"disabled"`
	// Reject certs that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (e.g., the system's CA). Defaults to Yes. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate.
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key.
	Passphrase *string `json:"passphrase,omitempty"`
	// Minimum TLS version to use when connecting
	MinVersion *OutputMinimumTLSVersionMsk `json:"minVersion,omitempty"`
	// Maximum TLS version to use when connecting
	MaxVersion *OutputMaximumTLSVersionMsk `json:"maxVersion,omitempty"`
}

func (o OutputTLSSettingsClientSideMsk) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputTLSSettingsClientSideMsk) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *OutputTLSSettingsClientSideMsk) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *OutputTLSSettingsClientSideMsk) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputTLSSettingsClientSideMsk) GetServername() *string {
	if o == nil {
		return nil
	}
	return o.Servername
}

func (o *OutputTLSSettingsClientSideMsk) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

func (o *OutputTLSSettingsClientSideMsk) GetCaPath() *string {
	if o == nil {
		return nil
	}
	return o.CaPath
}

func (o *OutputTLSSettingsClientSideMsk) GetPrivKeyPath() *string {
	if o == nil {
		return nil
	}
	return o.PrivKeyPath
}

func (o *OutputTLSSettingsClientSideMsk) GetCertPath() *string {
	if o == nil {
		return nil
	}
	return o.CertPath
}

func (o *OutputTLSSettingsClientSideMsk) GetPassphrase() *string {
	if o == nil {
		return nil
	}
	return o.Passphrase
}

func (o *OutputTLSSettingsClientSideMsk) GetMinVersion() *OutputMinimumTLSVersionMsk {
	if o == nil {
		return nil
	}
	return o.MinVersion
}

func (o *OutputTLSSettingsClientSideMsk) GetMaxVersion() *OutputMaximumTLSVersionMsk {
	if o == nil {
		return nil
	}
	return o.MaxVersion
}

// BackpressureBehaviorMsk - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorMsk string

const (
	BackpressureBehaviorMskBlock BackpressureBehaviorMsk = "block"
	BackpressureBehaviorMskDrop  BackpressureBehaviorMsk = "drop"
	BackpressureBehaviorMskQueue BackpressureBehaviorMsk = "queue"
)

func (e BackpressureBehaviorMsk) ToPointer() *BackpressureBehaviorMsk {
	return &e
}
func (e *BackpressureBehaviorMsk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorMsk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorMsk: %v", v)
	}
}

// PqCompressCompressionMsk - Codec to use to compress the persisted data.
type PqCompressCompressionMsk string

const (
	PqCompressCompressionMskNone PqCompressCompressionMsk = "none"
	PqCompressCompressionMskGzip PqCompressCompressionMsk = "gzip"
)

func (e PqCompressCompressionMsk) ToPointer() *PqCompressCompressionMsk {
	return &e
}
func (e *PqCompressCompressionMsk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = PqCompressCompressionMsk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PqCompressCompressionMsk: %v", v)
	}
}

// QueueFullBehaviorMsk - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorMsk string

const (
	QueueFullBehaviorMskBlock QueueFullBehaviorMsk = "block"
	QueueFullBehaviorMskDrop  QueueFullBehaviorMsk = "drop"
)

func (e QueueFullBehaviorMsk) ToPointer() *QueueFullBehaviorMsk {
	return &e
}
func (e *QueueFullBehaviorMsk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorMsk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorMsk: %v", v)
	}
}

// OutputModeMsk - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type OutputModeMsk string

const (
	OutputModeMskError        OutputModeMsk = "error"
	OutputModeMskBackpressure OutputModeMsk = "backpressure"
	OutputModeMskAlways       OutputModeMsk = "always"
)

func (e OutputModeMsk) ToPointer() *OutputModeMsk {
	return &e
}
func (e *OutputModeMsk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = OutputModeMsk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputModeMsk: %v", v)
	}
}

type PqControlsMsk struct {
}

type OutputMsk struct {
	// Unique ID for this output
	ID   *string        `json:"id,omitempty"`
	Type *OutputTypeMsk `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Enter each Kafka bootstrap server you want to use. Specify hostname and port, e.g., mykafkabroker:9092, or just hostname, in which case @{product} will assign port 9092.
	Brokers []string `json:"brokers"`
	// The topic to publish events to. Can be overridden using the __topicOut field.
	Topic string `json:"topic"`
	// Control the number of required acknowledgments.
	Ack *AcknowledgmentsMsk `default:"1" json:"ack"`
	// Format to use to serialize events before writing to Kafka.
	Format *RecordDataFormatMsk `default:"json" json:"format"`
	// Codec to use to compress the data before sending to Kafka
	Compression *OutputCompressionMsk `default:"gzip" json:"compression"`
	// Maximum size of each record batch before compression. The value must not exceed the Kafka brokers' message.max.bytes setting.
	MaxRecordSizeKB *float64 `default:"768" json:"maxRecordSizeKB"`
	// The maximum number of events you want the Destination to allow in a batch before forcing a flush
	FlushEventCount *float64 `default:"1000" json:"flushEventCount"`
	// The maximum amount of time you want the Destination to wait before forcing a flush. Shorter intervals tend to result in smaller batches being sent.
	FlushPeriodSec      *float64                                    `default:"1" json:"flushPeriodSec"`
	KafkaSchemaRegistry *OutputKafkaSchemaRegistryAuthenticationMsk `json:"kafkaSchemaRegistry,omitempty"`
	// Maximum time to wait for a connection to complete successfully
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Maximum time to wait for Kafka to respond to a request
	RequestTimeout *float64 `default:"60000" json:"requestTimeout"`
	// If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
	MaxRetries *float64 `default:"5" json:"maxRetries"`
	// The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackOff *float64 `default:"30000" json:"maxBackOff"`
	// Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"300" json:"initialBackoff"`
	// Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// Maximum time to wait for Kafka to respond to an authentication request
	AuthenticationTimeout *float64 `default:"10000" json:"authenticationTimeout"`
	// Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backwards from the moment when credentials are set to expire.
	ReauthenticationThreshold *float64 `default:"10000" json:"reauthenticationThreshold"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *OutputAuthenticationMethodMsk `default:"auto" json:"awsAuthenticationMethod"`
	AwsSecretKey            *string                        `json:"awsSecretKey,omitempty"`
	// Region where the MSK cluster is located
	Region string `json:"region"`
	// MSK cluster service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to MSK cluster-compatible endpoint.
	Endpoint *string `json:"endpoint,omitempty"`
	// Signature version to use for signing MSK cluster requests
	SignatureVersion *OutputSignatureVersionMsk `default:"v4" json:"signatureVersion"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Use Assume Role credentials to access MSK
	EnableAssumeRole *bool `default:"false" json:"enableAssumeRole"`
	// Amazon Resource Name (ARN) of the role to assume
	AssumeRoleArn *string `json:"assumeRoleArn,omitempty"`
	// External ID to use when assuming role
	AssumeRoleExternalID *string `json:"assumeRoleExternalId,omitempty"`
	// Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
	DurationSeconds *float64                        `default:"3600" json:"durationSeconds"`
	TLS             *OutputTLSSettingsClientSideMsk `json:"tls,omitempty"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorMsk `default:"block" json:"onBackpressure"`
	Description    *string                  `json:"description,omitempty"`
	AwsAPIKey      *string                  `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// Select a set of Protobuf definitions for the events you want to send
	ProtobufLibraryID *string `json:"protobufLibraryId,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *PqCompressCompressionMsk `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorMsk `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *OutputModeMsk `default:"error" json:"pqMode"`
	PqControls *PqControlsMsk `json:"pqControls,omitempty"`
	Status     *TFStatus      `json:"status,omitempty"`
}

func (o OutputMsk) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputMsk) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputMsk) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputMsk) GetType() *OutputTypeMsk {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputMsk) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputMsk) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputMsk) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputMsk) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputMsk) GetBrokers() []string {
	if o == nil {
		return []string{}
	}
	return o.Brokers
}

func (o *OutputMsk) GetTopic() string {
	if o == nil {
		return ""
	}
	return o.Topic
}

func (o *OutputMsk) GetAck() *AcknowledgmentsMsk {
	if o == nil {
		return nil
	}
	return o.Ack
}

func (o *OutputMsk) GetFormat() *RecordDataFormatMsk {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputMsk) GetCompression() *OutputCompressionMsk {
	if o == nil {
		return nil
	}
	return o.Compression
}

func (o *OutputMsk) GetMaxRecordSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSizeKB
}

func (o *OutputMsk) GetFlushEventCount() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushEventCount
}

func (o *OutputMsk) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputMsk) GetKafkaSchemaRegistry() *OutputKafkaSchemaRegistryAuthenticationMsk {
	if o == nil {
		return nil
	}
	return o.KafkaSchemaRegistry
}

func (o *OutputMsk) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputMsk) GetRequestTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.RequestTimeout
}

func (o *OutputMsk) GetMaxRetries() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetries
}

func (o *OutputMsk) GetMaxBackOff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackOff
}

func (o *OutputMsk) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *OutputMsk) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *OutputMsk) GetAuthenticationTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.AuthenticationTimeout
}

func (o *OutputMsk) GetReauthenticationThreshold() *float64 {
	if o == nil {
		return nil
	}
	return o.ReauthenticationThreshold
}

func (o *OutputMsk) GetAwsAuthenticationMethod() *OutputAuthenticationMethodMsk {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputMsk) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputMsk) GetRegion() string {
	if o == nil {
		return ""
	}
	return o.Region
}

func (o *OutputMsk) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputMsk) GetSignatureVersion() *OutputSignatureVersionMsk {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputMsk) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputMsk) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputMsk) GetEnableAssumeRole() *bool {
	if o == nil {
		return nil
	}
	return o.EnableAssumeRole
}

func (o *OutputMsk) GetAssumeRoleArn() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleArn
}

func (o *OutputMsk) GetAssumeRoleExternalID() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleExternalID
}

func (o *OutputMsk) GetDurationSeconds() *float64 {
	if o == nil {
		return nil
	}
	return o.DurationSeconds
}

func (o *OutputMsk) GetTLS() *OutputTLSSettingsClientSideMsk {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputMsk) GetOnBackpressure() *BackpressureBehaviorMsk {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputMsk) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputMsk) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputMsk) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputMsk) GetProtobufLibraryID() *string {
	if o == nil {
		return nil
	}
	return o.ProtobufLibraryID
}

func (o *OutputMsk) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputMsk) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputMsk) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputMsk) GetPqCompress() *PqCompressCompressionMsk {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputMsk) GetPqOnBackpressure() *QueueFullBehaviorMsk {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputMsk) GetPqMode() *OutputModeMsk {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputMsk) GetPqControls() *PqControlsMsk {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputMsk) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type OutputTypeConfluentCloud string

const (
	OutputTypeConfluentCloudConfluentCloud OutputTypeConfluentCloud = "confluent_cloud"
)

func (e OutputTypeConfluentCloud) ToPointer() *OutputTypeConfluentCloud {
	return &e
}
func (e *OutputTypeConfluentCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "confluent_cloud":
		*e = OutputTypeConfluentCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeConfluentCloud: %v", v)
	}
}

// OutputMinimumTLSVersionConfluentCloud - Minimum TLS version to use when connecting
type OutputMinimumTLSVersionConfluentCloud string

const (
	OutputMinimumTLSVersionConfluentCloudTlSv1  OutputMinimumTLSVersionConfluentCloud = "TLSv1"
	OutputMinimumTLSVersionConfluentCloudTlSv11 OutputMinimumTLSVersionConfluentCloud = "TLSv1.1"
	OutputMinimumTLSVersionConfluentCloudTlSv12 OutputMinimumTLSVersionConfluentCloud = "TLSv1.2"
	OutputMinimumTLSVersionConfluentCloudTlSv13 OutputMinimumTLSVersionConfluentCloud = "TLSv1.3"
)

func (e OutputMinimumTLSVersionConfluentCloud) ToPointer() *OutputMinimumTLSVersionConfluentCloud {
	return &e
}
func (e *OutputMinimumTLSVersionConfluentCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = OutputMinimumTLSVersionConfluentCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMinimumTLSVersionConfluentCloud: %v", v)
	}
}

// OutputMaximumTLSVersionConfluentCloud - Maximum TLS version to use when connecting
type OutputMaximumTLSVersionConfluentCloud string

const (
	OutputMaximumTLSVersionConfluentCloudTlSv1  OutputMaximumTLSVersionConfluentCloud = "TLSv1"
	OutputMaximumTLSVersionConfluentCloudTlSv11 OutputMaximumTLSVersionConfluentCloud = "TLSv1.1"
	OutputMaximumTLSVersionConfluentCloudTlSv12 OutputMaximumTLSVersionConfluentCloud = "TLSv1.2"
	OutputMaximumTLSVersionConfluentCloudTlSv13 OutputMaximumTLSVersionConfluentCloud = "TLSv1.3"
)

func (e OutputMaximumTLSVersionConfluentCloud) ToPointer() *OutputMaximumTLSVersionConfluentCloud {
	return &e
}
func (e *OutputMaximumTLSVersionConfluentCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = OutputMaximumTLSVersionConfluentCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMaximumTLSVersionConfluentCloud: %v", v)
	}
}

type OutputTLSSettingsClientSideConfluentCloud struct {
	Disabled *bool `default:"false" json:"disabled"`
	// Reject certs that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (e.g., the system's CA). Defaults to Yes. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate.
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key.
	Passphrase *string `json:"passphrase,omitempty"`
	// Minimum TLS version to use when connecting
	MinVersion *OutputMinimumTLSVersionConfluentCloud `json:"minVersion,omitempty"`
	// Maximum TLS version to use when connecting
	MaxVersion *OutputMaximumTLSVersionConfluentCloud `json:"maxVersion,omitempty"`
}

func (o OutputTLSSettingsClientSideConfluentCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputTLSSettingsClientSideConfluentCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *OutputTLSSettingsClientSideConfluentCloud) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *OutputTLSSettingsClientSideConfluentCloud) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputTLSSettingsClientSideConfluentCloud) GetServername() *string {
	if o == nil {
		return nil
	}
	return o.Servername
}

func (o *OutputTLSSettingsClientSideConfluentCloud) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

func (o *OutputTLSSettingsClientSideConfluentCloud) GetCaPath() *string {
	if o == nil {
		return nil
	}
	return o.CaPath
}

func (o *OutputTLSSettingsClientSideConfluentCloud) GetPrivKeyPath() *string {
	if o == nil {
		return nil
	}
	return o.PrivKeyPath
}

func (o *OutputTLSSettingsClientSideConfluentCloud) GetCertPath() *string {
	if o == nil {
		return nil
	}
	return o.CertPath
}

func (o *OutputTLSSettingsClientSideConfluentCloud) GetPassphrase() *string {
	if o == nil {
		return nil
	}
	return o.Passphrase
}

func (o *OutputTLSSettingsClientSideConfluentCloud) GetMinVersion() *OutputMinimumTLSVersionConfluentCloud {
	if o == nil {
		return nil
	}
	return o.MinVersion
}

func (o *OutputTLSSettingsClientSideConfluentCloud) GetMaxVersion() *OutputMaximumTLSVersionConfluentCloud {
	if o == nil {
		return nil
	}
	return o.MaxVersion
}

// AcknowledgmentsConfluentCloud - Control the number of required acknowledgments.
type AcknowledgmentsConfluentCloud int64

const (
	AcknowledgmentsConfluentCloudOne    AcknowledgmentsConfluentCloud = 1
	AcknowledgmentsConfluentCloudZero   AcknowledgmentsConfluentCloud = 0
	AcknowledgmentsConfluentCloudMinus1 AcknowledgmentsConfluentCloud = -1
)

func (e AcknowledgmentsConfluentCloud) ToPointer() *AcknowledgmentsConfluentCloud {
	return &e
}
func (e *AcknowledgmentsConfluentCloud) UnmarshalJSON(data []byte) error {
	var v int64
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case 1:
		fallthrough
	case 0:
		fallthrough
	case -1:
		*e = AcknowledgmentsConfluentCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AcknowledgmentsConfluentCloud: %v", v)
	}
}

// RecordDataFormatConfluentCloud - Format to use to serialize events before writing to Kafka.
type RecordDataFormatConfluentCloud string

const (
	RecordDataFormatConfluentCloudJSON     RecordDataFormatConfluentCloud = "json"
	RecordDataFormatConfluentCloudRaw      RecordDataFormatConfluentCloud = "raw"
	RecordDataFormatConfluentCloudProtobuf RecordDataFormatConfluentCloud = "protobuf"
)

func (e RecordDataFormatConfluentCloud) ToPointer() *RecordDataFormatConfluentCloud {
	return &e
}
func (e *RecordDataFormatConfluentCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "json":
		fallthrough
	case "raw":
		fallthrough
	case "protobuf":
		*e = RecordDataFormatConfluentCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for RecordDataFormatConfluentCloud: %v", v)
	}
}

// OutputCompressionConfluentCloud - Codec to use to compress the data before sending to Kafka
type OutputCompressionConfluentCloud string

const (
	OutputCompressionConfluentCloudNone   OutputCompressionConfluentCloud = "none"
	OutputCompressionConfluentCloudGzip   OutputCompressionConfluentCloud = "gzip"
	OutputCompressionConfluentCloudSnappy OutputCompressionConfluentCloud = "snappy"
	OutputCompressionConfluentCloudLz4    OutputCompressionConfluentCloud = "lz4"
)

func (e OutputCompressionConfluentCloud) ToPointer() *OutputCompressionConfluentCloud {
	return &e
}
func (e *OutputCompressionConfluentCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		fallthrough
	case "snappy":
		fallthrough
	case "lz4":
		*e = OutputCompressionConfluentCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputCompressionConfluentCloud: %v", v)
	}
}

// OutputAuthConfluentCloud - Credentials to use when authenticating with the schema registry using basic HTTP authentication
type OutputAuthConfluentCloud struct {
	// Enable authentication
	Disabled *bool `default:"true" json:"disabled"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
}

func (o OutputAuthConfluentCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputAuthConfluentCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *OutputAuthConfluentCloud) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *OutputAuthConfluentCloud) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

// OutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud - Minimum TLS version to use when connecting
type OutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud string

const (
	OutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloudTlSv1  OutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud = "TLSv1"
	OutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloudTlSv11 OutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud = "TLSv1.1"
	OutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloudTlSv12 OutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud = "TLSv1.2"
	OutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloudTlSv13 OutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud = "TLSv1.3"
)

func (e OutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud) ToPointer() *OutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud {
	return &e
}
func (e *OutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = OutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud: %v", v)
	}
}

// OutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud - Maximum TLS version to use when connecting
type OutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud string

const (
	OutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloudTlSv1  OutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud = "TLSv1"
	OutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloudTlSv11 OutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud = "TLSv1.1"
	OutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloudTlSv12 OutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud = "TLSv1.2"
	OutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloudTlSv13 OutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud = "TLSv1.3"
)

func (e OutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud) ToPointer() *OutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud {
	return &e
}
func (e *OutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = OutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud: %v", v)
	}
}

type OutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certs that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (e.g., the system's CA). Defaults to Yes. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate.
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key.
	Passphrase *string `json:"passphrase,omitempty"`
	// Minimum TLS version to use when connecting
	MinVersion *OutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud `json:"minVersion,omitempty"`
	// Maximum TLS version to use when connecting
	MaxVersion *OutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud `json:"maxVersion,omitempty"`
}

func (o OutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetServername() *string {
	if o == nil {
		return nil
	}
	return o.Servername
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetCaPath() *string {
	if o == nil {
		return nil
	}
	return o.CaPath
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetPrivKeyPath() *string {
	if o == nil {
		return nil
	}
	return o.PrivKeyPath
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetCertPath() *string {
	if o == nil {
		return nil
	}
	return o.CertPath
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetPassphrase() *string {
	if o == nil {
		return nil
	}
	return o.Passphrase
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetMinVersion() *OutputKafkaSchemaRegistryMinimumTLSVersionConfluentCloud {
	if o == nil {
		return nil
	}
	return o.MinVersion
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud) GetMaxVersion() *OutputKafkaSchemaRegistryMaximumTLSVersionConfluentCloud {
	if o == nil {
		return nil
	}
	return o.MaxVersion
}

type OutputKafkaSchemaRegistryAuthenticationConfluentCloud struct {
	// Enable Schema Registry
	Disabled *bool `default:"true" json:"disabled"`
	// URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.
	SchemaRegistryURL *string `default:"http://localhost:8081" json:"schemaRegistryURL"`
	// Maximum time to wait for a Schema Registry connection to complete successfully
	ConnectionTimeout *float64 `default:"30000" json:"connectionTimeout"`
	// Maximum time to wait for the Schema Registry to respond to a request
	RequestTimeout *float64 `default:"30000" json:"requestTimeout"`
	// Maximum number of times to try fetching schemas from the Schema Registry
	MaxRetries *float64 `default:"1" json:"maxRetries"`
	// Credentials to use when authenticating with the schema registry using basic HTTP authentication
	Auth *OutputAuthConfluentCloud                                     `json:"auth,omitempty"`
	TLS  *OutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud `json:"tls,omitempty"`
	// Used when __keySchemaIdOut is not present, to transform key values, leave blank if key transformation is not required by default.
	DefaultKeySchemaID *float64 `json:"defaultKeySchemaId,omitempty"`
	// Used when __valueSchemaIdOut is not present, to transform _raw, leave blank if value transformation is not required by default.
	DefaultValueSchemaID *float64 `json:"defaultValueSchemaId,omitempty"`
}

func (o OutputKafkaSchemaRegistryAuthenticationConfluentCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputKafkaSchemaRegistryAuthenticationConfluentCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *OutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *OutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetSchemaRegistryURL() *string {
	if o == nil {
		return nil
	}
	return o.SchemaRegistryURL
}

func (o *OutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetRequestTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.RequestTimeout
}

func (o *OutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetMaxRetries() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetries
}

func (o *OutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetAuth() *OutputAuthConfluentCloud {
	if o == nil {
		return nil
	}
	return o.Auth
}

func (o *OutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetTLS() *OutputKafkaSchemaRegistryTLSSettingsClientSideConfluentCloud {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetDefaultKeySchemaID() *float64 {
	if o == nil {
		return nil
	}
	return o.DefaultKeySchemaID
}

func (o *OutputKafkaSchemaRegistryAuthenticationConfluentCloud) GetDefaultValueSchemaID() *float64 {
	if o == nil {
		return nil
	}
	return o.DefaultValueSchemaID
}

// OutputSASLMechanismConfluentCloud - SASL authentication mechanism to use.
type OutputSASLMechanismConfluentCloud string

const (
	OutputSASLMechanismConfluentCloudPlain       OutputSASLMechanismConfluentCloud = "plain"
	OutputSASLMechanismConfluentCloudScramSha256 OutputSASLMechanismConfluentCloud = "scram-sha-256"
	OutputSASLMechanismConfluentCloudScramSha512 OutputSASLMechanismConfluentCloud = "scram-sha-512"
	OutputSASLMechanismConfluentCloudKerberos    OutputSASLMechanismConfluentCloud = "kerberos"
)

func (e OutputSASLMechanismConfluentCloud) ToPointer() *OutputSASLMechanismConfluentCloud {
	return &e
}
func (e *OutputSASLMechanismConfluentCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "plain":
		fallthrough
	case "scram-sha-256":
		fallthrough
	case "scram-sha-512":
		fallthrough
	case "kerberos":
		*e = OutputSASLMechanismConfluentCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputSASLMechanismConfluentCloud: %v", v)
	}
}

// OutputAuthenticationConfluentCloud - Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
type OutputAuthenticationConfluentCloud struct {
	// Enable Authentication
	Disabled *bool `default:"true" json:"disabled"`
	// SASL authentication mechanism to use.
	Mechanism *OutputSASLMechanismConfluentCloud `default:"plain" json:"mechanism"`
}

func (o OutputAuthenticationConfluentCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputAuthenticationConfluentCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *OutputAuthenticationConfluentCloud) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *OutputAuthenticationConfluentCloud) GetMechanism() *OutputSASLMechanismConfluentCloud {
	if o == nil {
		return nil
	}
	return o.Mechanism
}

// BackpressureBehaviorConfluentCloud - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorConfluentCloud string

const (
	BackpressureBehaviorConfluentCloudBlock BackpressureBehaviorConfluentCloud = "block"
	BackpressureBehaviorConfluentCloudDrop  BackpressureBehaviorConfluentCloud = "drop"
	BackpressureBehaviorConfluentCloudQueue BackpressureBehaviorConfluentCloud = "queue"
)

func (e BackpressureBehaviorConfluentCloud) ToPointer() *BackpressureBehaviorConfluentCloud {
	return &e
}
func (e *BackpressureBehaviorConfluentCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorConfluentCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorConfluentCloud: %v", v)
	}
}

// PqCompressCompressionConfluentCloud - Codec to use to compress the persisted data.
type PqCompressCompressionConfluentCloud string

const (
	PqCompressCompressionConfluentCloudNone PqCompressCompressionConfluentCloud = "none"
	PqCompressCompressionConfluentCloudGzip PqCompressCompressionConfluentCloud = "gzip"
)

func (e PqCompressCompressionConfluentCloud) ToPointer() *PqCompressCompressionConfluentCloud {
	return &e
}
func (e *PqCompressCompressionConfluentCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = PqCompressCompressionConfluentCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PqCompressCompressionConfluentCloud: %v", v)
	}
}

// QueueFullBehaviorConfluentCloud - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorConfluentCloud string

const (
	QueueFullBehaviorConfluentCloudBlock QueueFullBehaviorConfluentCloud = "block"
	QueueFullBehaviorConfluentCloudDrop  QueueFullBehaviorConfluentCloud = "drop"
)

func (e QueueFullBehaviorConfluentCloud) ToPointer() *QueueFullBehaviorConfluentCloud {
	return &e
}
func (e *QueueFullBehaviorConfluentCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorConfluentCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorConfluentCloud: %v", v)
	}
}

// OutputModeConfluentCloud - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type OutputModeConfluentCloud string

const (
	OutputModeConfluentCloudError        OutputModeConfluentCloud = "error"
	OutputModeConfluentCloudBackpressure OutputModeConfluentCloud = "backpressure"
	OutputModeConfluentCloudAlways       OutputModeConfluentCloud = "always"
)

func (e OutputModeConfluentCloud) ToPointer() *OutputModeConfluentCloud {
	return &e
}
func (e *OutputModeConfluentCloud) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = OutputModeConfluentCloud(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputModeConfluentCloud: %v", v)
	}
}

type PqControlsConfluentCloud struct {
}

type OutputConfluentCloud struct {
	// Unique ID for this output
	ID   *string                   `json:"id,omitempty"`
	Type *OutputTypeConfluentCloud `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// List of Confluent Cloud bootstrap servers to use, such as yourAccount.confluent.cloud:9092.
	Brokers []string                                   `json:"brokers"`
	TLS     *OutputTLSSettingsClientSideConfluentCloud `json:"tls,omitempty"`
	// The topic to publish events to. Can be overridden using the __topicOut field.
	Topic string `json:"topic"`
	// Control the number of required acknowledgments.
	Ack *AcknowledgmentsConfluentCloud `default:"1" json:"ack"`
	// Format to use to serialize events before writing to Kafka.
	Format *RecordDataFormatConfluentCloud `default:"json" json:"format"`
	// Codec to use to compress the data before sending to Kafka
	Compression *OutputCompressionConfluentCloud `default:"gzip" json:"compression"`
	// Maximum size of each record batch before compression. The value must not exceed the Kafka brokers' message.max.bytes setting.
	MaxRecordSizeKB *float64 `default:"768" json:"maxRecordSizeKB"`
	// The maximum number of events you want the Destination to allow in a batch before forcing a flush
	FlushEventCount *float64 `default:"1000" json:"flushEventCount"`
	// The maximum amount of time you want the Destination to wait before forcing a flush. Shorter intervals tend to result in smaller batches being sent.
	FlushPeriodSec      *float64                                               `default:"1" json:"flushPeriodSec"`
	KafkaSchemaRegistry *OutputKafkaSchemaRegistryAuthenticationConfluentCloud `json:"kafkaSchemaRegistry,omitempty"`
	// Maximum time to wait for a connection to complete successfully
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Maximum time to wait for Kafka to respond to a request
	RequestTimeout *float64 `default:"60000" json:"requestTimeout"`
	// If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
	MaxRetries *float64 `default:"5" json:"maxRetries"`
	// The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackOff *float64 `default:"30000" json:"maxBackOff"`
	// Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"300" json:"initialBackoff"`
	// Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// Maximum time to wait for Kafka to respond to an authentication request
	AuthenticationTimeout *float64 `default:"10000" json:"authenticationTimeout"`
	// Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backwards from the moment when credentials are set to expire.
	ReauthenticationThreshold *float64 `default:"10000" json:"reauthenticationThreshold"`
	// Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
	Sasl *OutputAuthenticationConfluentCloud `json:"sasl,omitempty"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorConfluentCloud `default:"block" json:"onBackpressure"`
	Description    *string                             `json:"description,omitempty"`
	// Select a set of Protobuf definitions for the events you want to send
	ProtobufLibraryID *string `json:"protobufLibraryId,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *PqCompressCompressionConfluentCloud `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorConfluentCloud `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *OutputModeConfluentCloud `default:"error" json:"pqMode"`
	PqControls *PqControlsConfluentCloud `json:"pqControls,omitempty"`
	Status     *TFStatus                 `json:"status,omitempty"`
}

func (o OutputConfluentCloud) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputConfluentCloud) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputConfluentCloud) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputConfluentCloud) GetType() *OutputTypeConfluentCloud {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputConfluentCloud) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputConfluentCloud) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputConfluentCloud) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputConfluentCloud) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputConfluentCloud) GetBrokers() []string {
	if o == nil {
		return []string{}
	}
	return o.Brokers
}

func (o *OutputConfluentCloud) GetTLS() *OutputTLSSettingsClientSideConfluentCloud {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputConfluentCloud) GetTopic() string {
	if o == nil {
		return ""
	}
	return o.Topic
}

func (o *OutputConfluentCloud) GetAck() *AcknowledgmentsConfluentCloud {
	if o == nil {
		return nil
	}
	return o.Ack
}

func (o *OutputConfluentCloud) GetFormat() *RecordDataFormatConfluentCloud {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputConfluentCloud) GetCompression() *OutputCompressionConfluentCloud {
	if o == nil {
		return nil
	}
	return o.Compression
}

func (o *OutputConfluentCloud) GetMaxRecordSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSizeKB
}

func (o *OutputConfluentCloud) GetFlushEventCount() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushEventCount
}

func (o *OutputConfluentCloud) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputConfluentCloud) GetKafkaSchemaRegistry() *OutputKafkaSchemaRegistryAuthenticationConfluentCloud {
	if o == nil {
		return nil
	}
	return o.KafkaSchemaRegistry
}

func (o *OutputConfluentCloud) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputConfluentCloud) GetRequestTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.RequestTimeout
}

func (o *OutputConfluentCloud) GetMaxRetries() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetries
}

func (o *OutputConfluentCloud) GetMaxBackOff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackOff
}

func (o *OutputConfluentCloud) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *OutputConfluentCloud) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *OutputConfluentCloud) GetAuthenticationTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.AuthenticationTimeout
}

func (o *OutputConfluentCloud) GetReauthenticationThreshold() *float64 {
	if o == nil {
		return nil
	}
	return o.ReauthenticationThreshold
}

func (o *OutputConfluentCloud) GetSasl() *OutputAuthenticationConfluentCloud {
	if o == nil {
		return nil
	}
	return o.Sasl
}

func (o *OutputConfluentCloud) GetOnBackpressure() *BackpressureBehaviorConfluentCloud {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputConfluentCloud) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputConfluentCloud) GetProtobufLibraryID() *string {
	if o == nil {
		return nil
	}
	return o.ProtobufLibraryID
}

func (o *OutputConfluentCloud) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputConfluentCloud) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputConfluentCloud) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputConfluentCloud) GetPqCompress() *PqCompressCompressionConfluentCloud {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputConfluentCloud) GetPqOnBackpressure() *QueueFullBehaviorConfluentCloud {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputConfluentCloud) GetPqMode() *OutputModeConfluentCloud {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputConfluentCloud) GetPqControls() *PqControlsConfluentCloud {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputConfluentCloud) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type OutputTypeKafka string

const (
	OutputTypeKafkaKafka OutputTypeKafka = "kafka"
)

func (e OutputTypeKafka) ToPointer() *OutputTypeKafka {
	return &e
}
func (e *OutputTypeKafka) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "kafka":
		*e = OutputTypeKafka(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeKafka: %v", v)
	}
}

// AcknowledgmentsKafka - Control the number of required acknowledgments.
type AcknowledgmentsKafka int64

const (
	AcknowledgmentsKafkaOne    AcknowledgmentsKafka = 1
	AcknowledgmentsKafkaZero   AcknowledgmentsKafka = 0
	AcknowledgmentsKafkaMinus1 AcknowledgmentsKafka = -1
)

func (e AcknowledgmentsKafka) ToPointer() *AcknowledgmentsKafka {
	return &e
}
func (e *AcknowledgmentsKafka) UnmarshalJSON(data []byte) error {
	var v int64
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case 1:
		fallthrough
	case 0:
		fallthrough
	case -1:
		*e = AcknowledgmentsKafka(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AcknowledgmentsKafka: %v", v)
	}
}

// RecordDataFormatKafka - Format to use to serialize events before writing to Kafka.
type RecordDataFormatKafka string

const (
	RecordDataFormatKafkaJSON     RecordDataFormatKafka = "json"
	RecordDataFormatKafkaRaw      RecordDataFormatKafka = "raw"
	RecordDataFormatKafkaProtobuf RecordDataFormatKafka = "protobuf"
)

func (e RecordDataFormatKafka) ToPointer() *RecordDataFormatKafka {
	return &e
}
func (e *RecordDataFormatKafka) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "json":
		fallthrough
	case "raw":
		fallthrough
	case "protobuf":
		*e = RecordDataFormatKafka(v)
		return nil
	default:
		return fmt.Errorf("invalid value for RecordDataFormatKafka: %v", v)
	}
}

// OutputCompressionKafka - Codec to use to compress the data before sending to Kafka
type OutputCompressionKafka string

const (
	OutputCompressionKafkaNone   OutputCompressionKafka = "none"
	OutputCompressionKafkaGzip   OutputCompressionKafka = "gzip"
	OutputCompressionKafkaSnappy OutputCompressionKafka = "snappy"
	OutputCompressionKafkaLz4    OutputCompressionKafka = "lz4"
)

func (e OutputCompressionKafka) ToPointer() *OutputCompressionKafka {
	return &e
}
func (e *OutputCompressionKafka) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		fallthrough
	case "snappy":
		fallthrough
	case "lz4":
		*e = OutputCompressionKafka(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputCompressionKafka: %v", v)
	}
}

// OutputAuthKafka - Credentials to use when authenticating with the schema registry using basic HTTP authentication
type OutputAuthKafka struct {
	// Enable authentication
	Disabled *bool `default:"true" json:"disabled"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
}

func (o OutputAuthKafka) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputAuthKafka) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *OutputAuthKafka) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *OutputAuthKafka) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

// OutputKafkaSchemaRegistryMinimumTLSVersionKafka - Minimum TLS version to use when connecting
type OutputKafkaSchemaRegistryMinimumTLSVersionKafka string

const (
	OutputKafkaSchemaRegistryMinimumTLSVersionKafkaTlSv1  OutputKafkaSchemaRegistryMinimumTLSVersionKafka = "TLSv1"
	OutputKafkaSchemaRegistryMinimumTLSVersionKafkaTlSv11 OutputKafkaSchemaRegistryMinimumTLSVersionKafka = "TLSv1.1"
	OutputKafkaSchemaRegistryMinimumTLSVersionKafkaTlSv12 OutputKafkaSchemaRegistryMinimumTLSVersionKafka = "TLSv1.2"
	OutputKafkaSchemaRegistryMinimumTLSVersionKafkaTlSv13 OutputKafkaSchemaRegistryMinimumTLSVersionKafka = "TLSv1.3"
)

func (e OutputKafkaSchemaRegistryMinimumTLSVersionKafka) ToPointer() *OutputKafkaSchemaRegistryMinimumTLSVersionKafka {
	return &e
}
func (e *OutputKafkaSchemaRegistryMinimumTLSVersionKafka) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = OutputKafkaSchemaRegistryMinimumTLSVersionKafka(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputKafkaSchemaRegistryMinimumTLSVersionKafka: %v", v)
	}
}

// OutputKafkaSchemaRegistryMaximumTLSVersionKafka - Maximum TLS version to use when connecting
type OutputKafkaSchemaRegistryMaximumTLSVersionKafka string

const (
	OutputKafkaSchemaRegistryMaximumTLSVersionKafkaTlSv1  OutputKafkaSchemaRegistryMaximumTLSVersionKafka = "TLSv1"
	OutputKafkaSchemaRegistryMaximumTLSVersionKafkaTlSv11 OutputKafkaSchemaRegistryMaximumTLSVersionKafka = "TLSv1.1"
	OutputKafkaSchemaRegistryMaximumTLSVersionKafkaTlSv12 OutputKafkaSchemaRegistryMaximumTLSVersionKafka = "TLSv1.2"
	OutputKafkaSchemaRegistryMaximumTLSVersionKafkaTlSv13 OutputKafkaSchemaRegistryMaximumTLSVersionKafka = "TLSv1.3"
)

func (e OutputKafkaSchemaRegistryMaximumTLSVersionKafka) ToPointer() *OutputKafkaSchemaRegistryMaximumTLSVersionKafka {
	return &e
}
func (e *OutputKafkaSchemaRegistryMaximumTLSVersionKafka) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = OutputKafkaSchemaRegistryMaximumTLSVersionKafka(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputKafkaSchemaRegistryMaximumTLSVersionKafka: %v", v)
	}
}

type OutputKafkaSchemaRegistryTLSSettingsClientSideKafka struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certs that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (e.g., the system's CA). Defaults to Yes. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate.
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key.
	Passphrase *string `json:"passphrase,omitempty"`
	// Minimum TLS version to use when connecting
	MinVersion *OutputKafkaSchemaRegistryMinimumTLSVersionKafka `json:"minVersion,omitempty"`
	// Maximum TLS version to use when connecting
	MaxVersion *OutputKafkaSchemaRegistryMaximumTLSVersionKafka `json:"maxVersion,omitempty"`
}

func (o OutputKafkaSchemaRegistryTLSSettingsClientSideKafka) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideKafka) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetServername() *string {
	if o == nil {
		return nil
	}
	return o.Servername
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetCaPath() *string {
	if o == nil {
		return nil
	}
	return o.CaPath
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetPrivKeyPath() *string {
	if o == nil {
		return nil
	}
	return o.PrivKeyPath
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetCertPath() *string {
	if o == nil {
		return nil
	}
	return o.CertPath
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetPassphrase() *string {
	if o == nil {
		return nil
	}
	return o.Passphrase
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetMinVersion() *OutputKafkaSchemaRegistryMinimumTLSVersionKafka {
	if o == nil {
		return nil
	}
	return o.MinVersion
}

func (o *OutputKafkaSchemaRegistryTLSSettingsClientSideKafka) GetMaxVersion() *OutputKafkaSchemaRegistryMaximumTLSVersionKafka {
	if o == nil {
		return nil
	}
	return o.MaxVersion
}

type OutputKafkaSchemaRegistryAuthenticationKafka struct {
	// Enable Schema Registry
	Disabled *bool `default:"true" json:"disabled"`
	// URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.
	SchemaRegistryURL *string `default:"http://localhost:8081" json:"schemaRegistryURL"`
	// Maximum time to wait for a Schema Registry connection to complete successfully
	ConnectionTimeout *float64 `default:"30000" json:"connectionTimeout"`
	// Maximum time to wait for the Schema Registry to respond to a request
	RequestTimeout *float64 `default:"30000" json:"requestTimeout"`
	// Maximum number of times to try fetching schemas from the Schema Registry
	MaxRetries *float64 `default:"1" json:"maxRetries"`
	// Credentials to use when authenticating with the schema registry using basic HTTP authentication
	Auth *OutputAuthKafka                                     `json:"auth,omitempty"`
	TLS  *OutputKafkaSchemaRegistryTLSSettingsClientSideKafka `json:"tls,omitempty"`
	// Used when __keySchemaIdOut is not present, to transform key values, leave blank if key transformation is not required by default.
	DefaultKeySchemaID *float64 `json:"defaultKeySchemaId,omitempty"`
	// Used when __valueSchemaIdOut is not present, to transform _raw, leave blank if value transformation is not required by default.
	DefaultValueSchemaID *float64 `json:"defaultValueSchemaId,omitempty"`
}

func (o OutputKafkaSchemaRegistryAuthenticationKafka) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputKafkaSchemaRegistryAuthenticationKafka) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *OutputKafkaSchemaRegistryAuthenticationKafka) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *OutputKafkaSchemaRegistryAuthenticationKafka) GetSchemaRegistryURL() *string {
	if o == nil {
		return nil
	}
	return o.SchemaRegistryURL
}

func (o *OutputKafkaSchemaRegistryAuthenticationKafka) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputKafkaSchemaRegistryAuthenticationKafka) GetRequestTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.RequestTimeout
}

func (o *OutputKafkaSchemaRegistryAuthenticationKafka) GetMaxRetries() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetries
}

func (o *OutputKafkaSchemaRegistryAuthenticationKafka) GetAuth() *OutputAuthKafka {
	if o == nil {
		return nil
	}
	return o.Auth
}

func (o *OutputKafkaSchemaRegistryAuthenticationKafka) GetTLS() *OutputKafkaSchemaRegistryTLSSettingsClientSideKafka {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputKafkaSchemaRegistryAuthenticationKafka) GetDefaultKeySchemaID() *float64 {
	if o == nil {
		return nil
	}
	return o.DefaultKeySchemaID
}

func (o *OutputKafkaSchemaRegistryAuthenticationKafka) GetDefaultValueSchemaID() *float64 {
	if o == nil {
		return nil
	}
	return o.DefaultValueSchemaID
}

// OutputSASLMechanismKafka - SASL authentication mechanism to use.
type OutputSASLMechanismKafka string

const (
	OutputSASLMechanismKafkaPlain       OutputSASLMechanismKafka = "plain"
	OutputSASLMechanismKafkaScramSha256 OutputSASLMechanismKafka = "scram-sha-256"
	OutputSASLMechanismKafkaScramSha512 OutputSASLMechanismKafka = "scram-sha-512"
	OutputSASLMechanismKafkaKerberos    OutputSASLMechanismKafka = "kerberos"
)

func (e OutputSASLMechanismKafka) ToPointer() *OutputSASLMechanismKafka {
	return &e
}
func (e *OutputSASLMechanismKafka) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "plain":
		fallthrough
	case "scram-sha-256":
		fallthrough
	case "scram-sha-512":
		fallthrough
	case "kerberos":
		*e = OutputSASLMechanismKafka(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputSASLMechanismKafka: %v", v)
	}
}

// OutputAuthenticationKafka - Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
type OutputAuthenticationKafka struct {
	// Enable Authentication
	Disabled *bool `default:"true" json:"disabled"`
	// SASL authentication mechanism to use.
	Mechanism *OutputSASLMechanismKafka `default:"plain" json:"mechanism"`
}

func (o OutputAuthenticationKafka) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputAuthenticationKafka) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *OutputAuthenticationKafka) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *OutputAuthenticationKafka) GetMechanism() *OutputSASLMechanismKafka {
	if o == nil {
		return nil
	}
	return o.Mechanism
}

// OutputMinimumTLSVersionKafka - Minimum TLS version to use when connecting
type OutputMinimumTLSVersionKafka string

const (
	OutputMinimumTLSVersionKafkaTlSv1  OutputMinimumTLSVersionKafka = "TLSv1"
	OutputMinimumTLSVersionKafkaTlSv11 OutputMinimumTLSVersionKafka = "TLSv1.1"
	OutputMinimumTLSVersionKafkaTlSv12 OutputMinimumTLSVersionKafka = "TLSv1.2"
	OutputMinimumTLSVersionKafkaTlSv13 OutputMinimumTLSVersionKafka = "TLSv1.3"
)

func (e OutputMinimumTLSVersionKafka) ToPointer() *OutputMinimumTLSVersionKafka {
	return &e
}
func (e *OutputMinimumTLSVersionKafka) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = OutputMinimumTLSVersionKafka(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMinimumTLSVersionKafka: %v", v)
	}
}

// OutputMaximumTLSVersionKafka - Maximum TLS version to use when connecting
type OutputMaximumTLSVersionKafka string

const (
	OutputMaximumTLSVersionKafkaTlSv1  OutputMaximumTLSVersionKafka = "TLSv1"
	OutputMaximumTLSVersionKafkaTlSv11 OutputMaximumTLSVersionKafka = "TLSv1.1"
	OutputMaximumTLSVersionKafkaTlSv12 OutputMaximumTLSVersionKafka = "TLSv1.2"
	OutputMaximumTLSVersionKafkaTlSv13 OutputMaximumTLSVersionKafka = "TLSv1.3"
)

func (e OutputMaximumTLSVersionKafka) ToPointer() *OutputMaximumTLSVersionKafka {
	return &e
}
func (e *OutputMaximumTLSVersionKafka) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = OutputMaximumTLSVersionKafka(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMaximumTLSVersionKafka: %v", v)
	}
}

type OutputTLSSettingsClientSideKafka struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certs that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (e.g., the system's CA). Defaults to Yes. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate.
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key.
	Passphrase *string `json:"passphrase,omitempty"`
	// Minimum TLS version to use when connecting
	MinVersion *OutputMinimumTLSVersionKafka `json:"minVersion,omitempty"`
	// Maximum TLS version to use when connecting
	MaxVersion *OutputMaximumTLSVersionKafka `json:"maxVersion,omitempty"`
}

func (o OutputTLSSettingsClientSideKafka) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputTLSSettingsClientSideKafka) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *OutputTLSSettingsClientSideKafka) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *OutputTLSSettingsClientSideKafka) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputTLSSettingsClientSideKafka) GetServername() *string {
	if o == nil {
		return nil
	}
	return o.Servername
}

func (o *OutputTLSSettingsClientSideKafka) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

func (o *OutputTLSSettingsClientSideKafka) GetCaPath() *string {
	if o == nil {
		return nil
	}
	return o.CaPath
}

func (o *OutputTLSSettingsClientSideKafka) GetPrivKeyPath() *string {
	if o == nil {
		return nil
	}
	return o.PrivKeyPath
}

func (o *OutputTLSSettingsClientSideKafka) GetCertPath() *string {
	if o == nil {
		return nil
	}
	return o.CertPath
}

func (o *OutputTLSSettingsClientSideKafka) GetPassphrase() *string {
	if o == nil {
		return nil
	}
	return o.Passphrase
}

func (o *OutputTLSSettingsClientSideKafka) GetMinVersion() *OutputMinimumTLSVersionKafka {
	if o == nil {
		return nil
	}
	return o.MinVersion
}

func (o *OutputTLSSettingsClientSideKafka) GetMaxVersion() *OutputMaximumTLSVersionKafka {
	if o == nil {
		return nil
	}
	return o.MaxVersion
}

// BackpressureBehaviorKafka - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorKafka string

const (
	BackpressureBehaviorKafkaBlock BackpressureBehaviorKafka = "block"
	BackpressureBehaviorKafkaDrop  BackpressureBehaviorKafka = "drop"
	BackpressureBehaviorKafkaQueue BackpressureBehaviorKafka = "queue"
)

func (e BackpressureBehaviorKafka) ToPointer() *BackpressureBehaviorKafka {
	return &e
}
func (e *BackpressureBehaviorKafka) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorKafka(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorKafka: %v", v)
	}
}

// PqCompressCompressionKafka - Codec to use to compress the persisted data.
type PqCompressCompressionKafka string

const (
	PqCompressCompressionKafkaNone PqCompressCompressionKafka = "none"
	PqCompressCompressionKafkaGzip PqCompressCompressionKafka = "gzip"
)

func (e PqCompressCompressionKafka) ToPointer() *PqCompressCompressionKafka {
	return &e
}
func (e *PqCompressCompressionKafka) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = PqCompressCompressionKafka(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PqCompressCompressionKafka: %v", v)
	}
}

// QueueFullBehaviorKafka - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorKafka string

const (
	QueueFullBehaviorKafkaBlock QueueFullBehaviorKafka = "block"
	QueueFullBehaviorKafkaDrop  QueueFullBehaviorKafka = "drop"
)

func (e QueueFullBehaviorKafka) ToPointer() *QueueFullBehaviorKafka {
	return &e
}
func (e *QueueFullBehaviorKafka) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorKafka(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorKafka: %v", v)
	}
}

// OutputModeKafka - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type OutputModeKafka string

const (
	OutputModeKafkaError        OutputModeKafka = "error"
	OutputModeKafkaBackpressure OutputModeKafka = "backpressure"
	OutputModeKafkaAlways       OutputModeKafka = "always"
)

func (e OutputModeKafka) ToPointer() *OutputModeKafka {
	return &e
}
func (e *OutputModeKafka) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = OutputModeKafka(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputModeKafka: %v", v)
	}
}

type PqControlsKafka struct {
}

type OutputKafka struct {
	// Unique ID for this output
	ID   *string          `json:"id,omitempty"`
	Type *OutputTypeKafka `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Enter each Kafka bootstrap server you want to use. Specify hostname and port, e.g., mykafkabroker:9092, or just hostname, in which case @{product} will assign port 9092.
	Brokers []string `json:"brokers"`
	// The topic to publish events to. Can be overridden using the __topicOut field.
	Topic string `json:"topic"`
	// Control the number of required acknowledgments.
	Ack *AcknowledgmentsKafka `default:"1" json:"ack"`
	// Format to use to serialize events before writing to Kafka.
	Format *RecordDataFormatKafka `default:"json" json:"format"`
	// Codec to use to compress the data before sending to Kafka
	Compression *OutputCompressionKafka `default:"gzip" json:"compression"`
	// Maximum size of each record batch before compression. The value must not exceed the Kafka brokers' message.max.bytes setting.
	MaxRecordSizeKB *float64 `default:"768" json:"maxRecordSizeKB"`
	// The maximum number of events you want the Destination to allow in a batch before forcing a flush
	FlushEventCount *float64 `default:"1000" json:"flushEventCount"`
	// The maximum amount of time you want the Destination to wait before forcing a flush. Shorter intervals tend to result in smaller batches being sent.
	FlushPeriodSec      *float64                                      `default:"1" json:"flushPeriodSec"`
	KafkaSchemaRegistry *OutputKafkaSchemaRegistryAuthenticationKafka `json:"kafkaSchemaRegistry,omitempty"`
	// Maximum time to wait for a connection to complete successfully
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Maximum time to wait for Kafka to respond to a request
	RequestTimeout *float64 `default:"60000" json:"requestTimeout"`
	// If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
	MaxRetries *float64 `default:"5" json:"maxRetries"`
	// The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackOff *float64 `default:"30000" json:"maxBackOff"`
	// Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"300" json:"initialBackoff"`
	// Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// Maximum time to wait for Kafka to respond to an authentication request
	AuthenticationTimeout *float64 `default:"10000" json:"authenticationTimeout"`
	// Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backwards from the moment when credentials are set to expire.
	ReauthenticationThreshold *float64 `default:"10000" json:"reauthenticationThreshold"`
	// Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
	Sasl *OutputAuthenticationKafka        `json:"sasl,omitempty"`
	TLS  *OutputTLSSettingsClientSideKafka `json:"tls,omitempty"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorKafka `default:"block" json:"onBackpressure"`
	Description    *string                    `json:"description,omitempty"`
	// Select a set of Protobuf definitions for the events you want to send
	ProtobufLibraryID *string `json:"protobufLibraryId,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *PqCompressCompressionKafka `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorKafka `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *OutputModeKafka `default:"error" json:"pqMode"`
	PqControls *PqControlsKafka `json:"pqControls,omitempty"`
	Status     *TFStatus        `json:"status,omitempty"`
}

func (o OutputKafka) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputKafka) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputKafka) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputKafka) GetType() *OutputTypeKafka {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputKafka) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputKafka) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputKafka) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputKafka) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputKafka) GetBrokers() []string {
	if o == nil {
		return []string{}
	}
	return o.Brokers
}

func (o *OutputKafka) GetTopic() string {
	if o == nil {
		return ""
	}
	return o.Topic
}

func (o *OutputKafka) GetAck() *AcknowledgmentsKafka {
	if o == nil {
		return nil
	}
	return o.Ack
}

func (o *OutputKafka) GetFormat() *RecordDataFormatKafka {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputKafka) GetCompression() *OutputCompressionKafka {
	if o == nil {
		return nil
	}
	return o.Compression
}

func (o *OutputKafka) GetMaxRecordSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSizeKB
}

func (o *OutputKafka) GetFlushEventCount() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushEventCount
}

func (o *OutputKafka) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputKafka) GetKafkaSchemaRegistry() *OutputKafkaSchemaRegistryAuthenticationKafka {
	if o == nil {
		return nil
	}
	return o.KafkaSchemaRegistry
}

func (o *OutputKafka) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputKafka) GetRequestTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.RequestTimeout
}

func (o *OutputKafka) GetMaxRetries() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetries
}

func (o *OutputKafka) GetMaxBackOff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackOff
}

func (o *OutputKafka) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *OutputKafka) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *OutputKafka) GetAuthenticationTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.AuthenticationTimeout
}

func (o *OutputKafka) GetReauthenticationThreshold() *float64 {
	if o == nil {
		return nil
	}
	return o.ReauthenticationThreshold
}

func (o *OutputKafka) GetSasl() *OutputAuthenticationKafka {
	if o == nil {
		return nil
	}
	return o.Sasl
}

func (o *OutputKafka) GetTLS() *OutputTLSSettingsClientSideKafka {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputKafka) GetOnBackpressure() *BackpressureBehaviorKafka {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputKafka) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputKafka) GetProtobufLibraryID() *string {
	if o == nil {
		return nil
	}
	return o.ProtobufLibraryID
}

func (o *OutputKafka) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputKafka) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputKafka) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputKafka) GetPqCompress() *PqCompressCompressionKafka {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputKafka) GetPqOnBackpressure() *QueueFullBehaviorKafka {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputKafka) GetPqMode() *OutputModeKafka {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputKafka) GetPqControls() *PqControlsKafka {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputKafka) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeExabeam string

const (
	TypeExabeamExabeam TypeExabeam = "exabeam"
)

func (e TypeExabeam) ToPointer() *TypeExabeam {
	return &e
}
func (e *TypeExabeam) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "exabeam":
		*e = TypeExabeam(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeExabeam: %v", v)
	}
}

// SignatureVersionExabeam - Signature version to use for signing Google Cloud Storage requests.
type SignatureVersionExabeam string

const (
	SignatureVersionExabeamV2 SignatureVersionExabeam = "v2"
	SignatureVersionExabeamV4 SignatureVersionExabeam = "v4"
)

func (e SignatureVersionExabeam) ToPointer() *SignatureVersionExabeam {
	return &e
}
func (e *SignatureVersionExabeam) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "v2":
		fallthrough
	case "v4":
		*e = SignatureVersionExabeam(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SignatureVersionExabeam: %v", v)
	}
}

// ObjectACLExabeam - Object ACL to assign to uploaded objects.
type ObjectACLExabeam string

const (
	ObjectACLExabeamPrivate                ObjectACLExabeam = "private"
	ObjectACLExabeamBucketOwnerRead        ObjectACLExabeam = "bucket-owner-read"
	ObjectACLExabeamBucketOwnerFullControl ObjectACLExabeam = "bucket-owner-full-control"
	ObjectACLExabeamProjectPrivate         ObjectACLExabeam = "project-private"
	ObjectACLExabeamAuthenticatedRead      ObjectACLExabeam = "authenticated-read"
	ObjectACLExabeamPublicRead             ObjectACLExabeam = "public-read"
)

func (e ObjectACLExabeam) ToPointer() *ObjectACLExabeam {
	return &e
}
func (e *ObjectACLExabeam) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "private":
		fallthrough
	case "bucket-owner-read":
		fallthrough
	case "bucket-owner-full-control":
		fallthrough
	case "project-private":
		fallthrough
	case "authenticated-read":
		fallthrough
	case "public-read":
		*e = ObjectACLExabeam(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ObjectACLExabeam: %v", v)
	}
}

// StorageClassExabeam - Storage class to select for uploaded objects.
type StorageClassExabeam string

const (
	StorageClassExabeamStandard StorageClassExabeam = "STANDARD"
	StorageClassExabeamNearline StorageClassExabeam = "NEARLINE"
	StorageClassExabeamColdline StorageClassExabeam = "COLDLINE"
	StorageClassExabeamArchive  StorageClassExabeam = "ARCHIVE"
)

func (e StorageClassExabeam) ToPointer() *StorageClassExabeam {
	return &e
}
func (e *StorageClassExabeam) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "STANDARD":
		fallthrough
	case "NEARLINE":
		fallthrough
	case "COLDLINE":
		fallthrough
	case "ARCHIVE":
		*e = StorageClassExabeam(v)
		return nil
	default:
		return fmt.Errorf("invalid value for StorageClassExabeam: %v", v)
	}
}

// BackpressureBehaviorExabeam - Whether to block or drop events when all receivers are exerting backpressure
type BackpressureBehaviorExabeam string

const (
	BackpressureBehaviorExabeamBlock BackpressureBehaviorExabeam = "block"
	BackpressureBehaviorExabeamDrop  BackpressureBehaviorExabeam = "drop"
)

func (e BackpressureBehaviorExabeam) ToPointer() *BackpressureBehaviorExabeam {
	return &e
}
func (e *BackpressureBehaviorExabeam) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = BackpressureBehaviorExabeam(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorExabeam: %v", v)
	}
}

// DiskSpaceProtectionExabeam - Whether to block or drop events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionExabeam string

const (
	DiskSpaceProtectionExabeamBlock DiskSpaceProtectionExabeam = "block"
	DiskSpaceProtectionExabeamDrop  DiskSpaceProtectionExabeam = "drop"
)

func (e DiskSpaceProtectionExabeam) ToPointer() *DiskSpaceProtectionExabeam {
	return &e
}
func (e *DiskSpaceProtectionExabeam) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = DiskSpaceProtectionExabeam(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DiskSpaceProtectionExabeam: %v", v)
	}
}

type OutputExabeam struct {
	// Unique ID for this output
	ID   *string      `json:"id,omitempty"`
	Type *TypeExabeam `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Name of the destination bucket. A constant or a JavaScript expression that can only be evaluated at init time. Example of referencing a JavaScript Global Variable: `myBucket-${C.vars.myVar}`.
	Bucket string `json:"bucket"`
	// Region where the bucket is located
	Region string `json:"region"`
	// Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant stable storage.
	StagePath *string `default:"\\$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Google Cloud Storage service endpoint.
	Endpoint *string `default:"https://storage.googleapis.com" json:"endpoint"`
	// Signature version to use for signing Google Cloud Storage requests.
	SignatureVersion *SignatureVersionExabeam `default:"v4" json:"signatureVersion"`
	// Object ACL to assign to uploaded objects.
	ObjectACL *ObjectACLExabeam `default:"private" json:"objectACL"`
	// Storage class to select for uploaded objects.
	StorageClass *StorageClassExabeam `json:"storageClass,omitempty"`
	// Whether to reuse connections between requests, which can improve performance.
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Whether to reject certificates that cannot be verified against a valid CA (e.g., self-signed certificates).
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Append output's ID to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// Whether to block or drop events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorExabeam `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, dead-letter it to prevent further errors.
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// Whether to block or drop events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionExabeam `default:"block" json:"onDiskFullBackpressure"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"10" json:"maxFileSizeMB"`
	// Enter an encoded string containing Exabeam configurations.
	EncodedConfiguration *string `json:"encodedConfiguration,omitempty"`
	// ID of the Exabeam Collector where data should be sent. Example: 11112222-3333-4444-5555-666677778888
	//
	CollectorInstanceID string `json:"collectorInstanceId"`
	// Constant or JavaScript expression to create an Exabeam site name. Values that aren't successfully evaluated will be treated as string constants.
	SiteName *string `json:"siteName,omitempty"`
	// Exabeam site ID. If left blank, @{product} will use the value of the Exabeam site name.
	SiteID *string `json:"siteId,omitempty"`
	// Exabeam timezone offset.
	TimezoneOffset *string `json:"timezoneOffset,omitempty"`
	// HMAC access key. Can be a constant or a JavaScript expression, such as `${C.env.GCS_ACCESS_KEY}`.
	AwsAPIKey *string `json:"awsApiKey,omitempty"`
	// HMAC secret. Can be a constant or a JavaScript expression, such as `${C.env.GCS_SECRET}`.
	AwsSecretKey *string `json:"awsSecretKey,omitempty"`
	Description  *string `json:"description,omitempty"`
	// How frequently, in seconds, to clean up empty directories when 'Remove empty staging dirs' is enabled
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"\\$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64  `default:"20" json:"maxRetryNum"`
	Status      *TFStatus `json:"status,omitempty"`
}

func (o OutputExabeam) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputExabeam) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputExabeam) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputExabeam) GetType() *TypeExabeam {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputExabeam) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputExabeam) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputExabeam) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputExabeam) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputExabeam) GetBucket() string {
	if o == nil {
		return ""
	}
	return o.Bucket
}

func (o *OutputExabeam) GetRegion() string {
	if o == nil {
		return ""
	}
	return o.Region
}

func (o *OutputExabeam) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputExabeam) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputExabeam) GetSignatureVersion() *SignatureVersionExabeam {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputExabeam) GetObjectACL() *ObjectACLExabeam {
	if o == nil {
		return nil
	}
	return o.ObjectACL
}

func (o *OutputExabeam) GetStorageClass() *StorageClassExabeam {
	if o == nil {
		return nil
	}
	return o.StorageClass
}

func (o *OutputExabeam) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputExabeam) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputExabeam) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputExabeam) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputExabeam) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputExabeam) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputExabeam) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputExabeam) GetOnBackpressure() *BackpressureBehaviorExabeam {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputExabeam) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputExabeam) GetOnDiskFullBackpressure() *DiskSpaceProtectionExabeam {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputExabeam) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputExabeam) GetEncodedConfiguration() *string {
	if o == nil {
		return nil
	}
	return o.EncodedConfiguration
}

func (o *OutputExabeam) GetCollectorInstanceID() string {
	if o == nil {
		return ""
	}
	return o.CollectorInstanceID
}

func (o *OutputExabeam) GetSiteName() *string {
	if o == nil {
		return nil
	}
	return o.SiteName
}

func (o *OutputExabeam) GetSiteID() *string {
	if o == nil {
		return nil
	}
	return o.SiteID
}

func (o *OutputExabeam) GetTimezoneOffset() *string {
	if o == nil {
		return nil
	}
	return o.TimezoneOffset
}

func (o *OutputExabeam) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputExabeam) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputExabeam) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputExabeam) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputExabeam) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputExabeam) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

func (o *OutputExabeam) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type OutputTypeGooglePubsub string

const (
	OutputTypeGooglePubsubGooglePubsub OutputTypeGooglePubsub = "google_pubsub"
)

func (e OutputTypeGooglePubsub) ToPointer() *OutputTypeGooglePubsub {
	return &e
}
func (e *OutputTypeGooglePubsub) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "google_pubsub":
		*e = OutputTypeGooglePubsub(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeGooglePubsub: %v", v)
	}
}

// OutputAuthenticationMethodGooglePubsub - Google authentication method. Choose Auto to use Google Application Default Credentials.
type OutputAuthenticationMethodGooglePubsub string

const (
	OutputAuthenticationMethodGooglePubsubAuto   OutputAuthenticationMethodGooglePubsub = "auto"
	OutputAuthenticationMethodGooglePubsubManual OutputAuthenticationMethodGooglePubsub = "manual"
	OutputAuthenticationMethodGooglePubsubSecret OutputAuthenticationMethodGooglePubsub = "secret"
)

func (e OutputAuthenticationMethodGooglePubsub) ToPointer() *OutputAuthenticationMethodGooglePubsub {
	return &e
}
func (e *OutputAuthenticationMethodGooglePubsub) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "auto":
		fallthrough
	case "manual":
		fallthrough
	case "secret":
		*e = OutputAuthenticationMethodGooglePubsub(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputAuthenticationMethodGooglePubsub: %v", v)
	}
}

// BackpressureBehaviorGooglePubsub - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorGooglePubsub string

const (
	BackpressureBehaviorGooglePubsubBlock BackpressureBehaviorGooglePubsub = "block"
	BackpressureBehaviorGooglePubsubDrop  BackpressureBehaviorGooglePubsub = "drop"
	BackpressureBehaviorGooglePubsubQueue BackpressureBehaviorGooglePubsub = "queue"
)

func (e BackpressureBehaviorGooglePubsub) ToPointer() *BackpressureBehaviorGooglePubsub {
	return &e
}
func (e *BackpressureBehaviorGooglePubsub) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorGooglePubsub(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorGooglePubsub: %v", v)
	}
}

// PqCompressCompressionGooglePubsub - Codec to use to compress the persisted data.
type PqCompressCompressionGooglePubsub string

const (
	PqCompressCompressionGooglePubsubNone PqCompressCompressionGooglePubsub = "none"
	PqCompressCompressionGooglePubsubGzip PqCompressCompressionGooglePubsub = "gzip"
)

func (e PqCompressCompressionGooglePubsub) ToPointer() *PqCompressCompressionGooglePubsub {
	return &e
}
func (e *PqCompressCompressionGooglePubsub) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = PqCompressCompressionGooglePubsub(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PqCompressCompressionGooglePubsub: %v", v)
	}
}

// QueueFullBehaviorGooglePubsub - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorGooglePubsub string

const (
	QueueFullBehaviorGooglePubsubBlock QueueFullBehaviorGooglePubsub = "block"
	QueueFullBehaviorGooglePubsubDrop  QueueFullBehaviorGooglePubsub = "drop"
)

func (e QueueFullBehaviorGooglePubsub) ToPointer() *QueueFullBehaviorGooglePubsub {
	return &e
}
func (e *QueueFullBehaviorGooglePubsub) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorGooglePubsub(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorGooglePubsub: %v", v)
	}
}

// OutputModeGooglePubsub - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type OutputModeGooglePubsub string

const (
	OutputModeGooglePubsubError        OutputModeGooglePubsub = "error"
	OutputModeGooglePubsubBackpressure OutputModeGooglePubsub = "backpressure"
	OutputModeGooglePubsubAlways       OutputModeGooglePubsub = "always"
)

func (e OutputModeGooglePubsub) ToPointer() *OutputModeGooglePubsub {
	return &e
}
func (e *OutputModeGooglePubsub) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = OutputModeGooglePubsub(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputModeGooglePubsub: %v", v)
	}
}

type PqControlsGooglePubsub struct {
}

type OutputGooglePubsub struct {
	// Unique ID for this output
	ID   *string                `json:"id,omitempty"`
	Type OutputTypeGooglePubsub `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// ID of the topic to send events to.
	TopicName string `json:"topicName"`
	// If enabled, create topic if it does not exist.
	CreateTopic *bool `default:"false" json:"createTopic"`
	// If enabled, send events in the order they were added to the queue. For this to work correctly, the process receiving events must have ordering enabled.
	OrderedDelivery *bool `default:"false" json:"orderedDelivery"`
	// Region to publish messages to. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy.
	Region *string `json:"region,omitempty"`
	// Google authentication method. Choose Auto to use Google Application Default Credentials.
	GoogleAuthMethod *OutputAuthenticationMethodGooglePubsub `default:"manual" json:"googleAuthMethod"`
	// Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.
	ServiceAccountCredentials *string `json:"serviceAccountCredentials,omitempty"`
	// Select or create a stored text secret
	Secret *string `json:"secret,omitempty"`
	// The maximum number of items the Google API should batch before it sends them to the topic.
	BatchSize *float64 `default:"1000" json:"batchSize"`
	// The maximum amount of time, in milliseconds, that the Google API should wait to send a batch (if the Batch size is not reached).
	BatchTimeout *float64 `default:"100" json:"batchTimeout"`
	// Maximum number of queued batches before blocking.
	MaxQueueSize *float64 `default:"100" json:"maxQueueSize"`
	// Maximum size (KB) of batches to send.
	MaxRecordSizeKB *float64 `default:"256" json:"maxRecordSizeKB"`
	// Maximum time to wait before sending a batch (when batch size limit is not reached).
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// The maximum number of in-progress API requests before backpressure is applied.
	MaxInProgress *float64 `default:"10" json:"maxInProgress"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorGooglePubsub `default:"block" json:"onBackpressure"`
	Description    *string                           `json:"description,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *PqCompressCompressionGooglePubsub `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorGooglePubsub `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *OutputModeGooglePubsub `default:"error" json:"pqMode"`
	PqControls *PqControlsGooglePubsub `json:"pqControls,omitempty"`
	Status     *TFStatus               `json:"status,omitempty"`
}

func (o OutputGooglePubsub) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGooglePubsub) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputGooglePubsub) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputGooglePubsub) GetType() OutputTypeGooglePubsub {
	if o == nil {
		return OutputTypeGooglePubsub("")
	}
	return o.Type
}

func (o *OutputGooglePubsub) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputGooglePubsub) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputGooglePubsub) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputGooglePubsub) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputGooglePubsub) GetTopicName() string {
	if o == nil {
		return ""
	}
	return o.TopicName
}

func (o *OutputGooglePubsub) GetCreateTopic() *bool {
	if o == nil {
		return nil
	}
	return o.CreateTopic
}

func (o *OutputGooglePubsub) GetOrderedDelivery() *bool {
	if o == nil {
		return nil
	}
	return o.OrderedDelivery
}

func (o *OutputGooglePubsub) GetRegion() *string {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputGooglePubsub) GetGoogleAuthMethod() *OutputAuthenticationMethodGooglePubsub {
	if o == nil {
		return nil
	}
	return o.GoogleAuthMethod
}

func (o *OutputGooglePubsub) GetServiceAccountCredentials() *string {
	if o == nil {
		return nil
	}
	return o.ServiceAccountCredentials
}

func (o *OutputGooglePubsub) GetSecret() *string {
	if o == nil {
		return nil
	}
	return o.Secret
}

func (o *OutputGooglePubsub) GetBatchSize() *float64 {
	if o == nil {
		return nil
	}
	return o.BatchSize
}

func (o *OutputGooglePubsub) GetBatchTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.BatchTimeout
}

func (o *OutputGooglePubsub) GetMaxQueueSize() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxQueueSize
}

func (o *OutputGooglePubsub) GetMaxRecordSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSizeKB
}

func (o *OutputGooglePubsub) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputGooglePubsub) GetMaxInProgress() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxInProgress
}

func (o *OutputGooglePubsub) GetOnBackpressure() *BackpressureBehaviorGooglePubsub {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputGooglePubsub) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputGooglePubsub) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputGooglePubsub) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputGooglePubsub) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputGooglePubsub) GetPqCompress() *PqCompressCompressionGooglePubsub {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputGooglePubsub) GetPqOnBackpressure() *QueueFullBehaviorGooglePubsub {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputGooglePubsub) GetPqMode() *OutputModeGooglePubsub {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputGooglePubsub) GetPqControls() *PqControlsGooglePubsub {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputGooglePubsub) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeGoogleCloudLogging string

const (
	TypeGoogleCloudLoggingGoogleCloudLogging TypeGoogleCloudLogging = "google_cloud_logging"
)

func (e TypeGoogleCloudLogging) ToPointer() *TypeGoogleCloudLogging {
	return &e
}
func (e *TypeGoogleCloudLogging) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "google_cloud_logging":
		*e = TypeGoogleCloudLogging(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeGoogleCloudLogging: %v", v)
	}
}

type LogLocationType string

const (
	LogLocationTypeProject        LogLocationType = "project"
	LogLocationTypeOrganization   LogLocationType = "organization"
	LogLocationTypeBillingAccount LogLocationType = "billingAccount"
	LogLocationTypeFolder         LogLocationType = "folder"
)

func (e LogLocationType) ToPointer() *LogLocationType {
	return &e
}
func (e *LogLocationType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "project":
		fallthrough
	case "organization":
		fallthrough
	case "billingAccount":
		fallthrough
	case "folder":
		*e = LogLocationType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for LogLocationType: %v", v)
	}
}

// PayloadFormat - Format to use when sending payload. Defaults to Text.
type PayloadFormat string

const (
	PayloadFormatText PayloadFormat = "text"
	PayloadFormatJSON PayloadFormat = "json"
)

func (e PayloadFormat) ToPointer() *PayloadFormat {
	return &e
}
func (e *PayloadFormat) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "text":
		fallthrough
	case "json":
		*e = PayloadFormat(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PayloadFormat: %v", v)
	}
}

type LogLabel struct {
	// Label name
	Label string `json:"label"`
	// JavaScript expression to compute the label's value.
	ValueExpression string `json:"valueExpression"`
}

func (o *LogLabel) GetLabel() string {
	if o == nil {
		return ""
	}
	return o.Label
}

func (o *LogLabel) GetValueExpression() string {
	if o == nil {
		return ""
	}
	return o.ValueExpression
}

type ResourceTypeLabel struct {
	// Label name
	Label string `json:"label"`
	// JavaScript expression to compute the label's value.
	ValueExpression string `json:"valueExpression"`
}

func (o *ResourceTypeLabel) GetLabel() string {
	if o == nil {
		return ""
	}
	return o.Label
}

func (o *ResourceTypeLabel) GetValueExpression() string {
	if o == nil {
		return ""
	}
	return o.ValueExpression
}

// AuthenticationMethodGoogleCloudLogging - Google authentication method. Choose Auto to use Google Application Default Credentials.
type AuthenticationMethodGoogleCloudLogging string

const (
	AuthenticationMethodGoogleCloudLoggingAuto   AuthenticationMethodGoogleCloudLogging = "auto"
	AuthenticationMethodGoogleCloudLoggingManual AuthenticationMethodGoogleCloudLogging = "manual"
	AuthenticationMethodGoogleCloudLoggingSecret AuthenticationMethodGoogleCloudLogging = "secret"
)

func (e AuthenticationMethodGoogleCloudLogging) ToPointer() *AuthenticationMethodGoogleCloudLogging {
	return &e
}
func (e *AuthenticationMethodGoogleCloudLogging) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "auto":
		fallthrough
	case "manual":
		fallthrough
	case "secret":
		*e = AuthenticationMethodGoogleCloudLogging(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodGoogleCloudLogging: %v", v)
	}
}

// BackpressureBehaviorGoogleCloudLogging - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorGoogleCloudLogging string

const (
	BackpressureBehaviorGoogleCloudLoggingBlock BackpressureBehaviorGoogleCloudLogging = "block"
	BackpressureBehaviorGoogleCloudLoggingDrop  BackpressureBehaviorGoogleCloudLogging = "drop"
	BackpressureBehaviorGoogleCloudLoggingQueue BackpressureBehaviorGoogleCloudLogging = "queue"
)

func (e BackpressureBehaviorGoogleCloudLogging) ToPointer() *BackpressureBehaviorGoogleCloudLogging {
	return &e
}
func (e *BackpressureBehaviorGoogleCloudLogging) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorGoogleCloudLogging(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorGoogleCloudLogging: %v", v)
	}
}

// CompressionGoogleCloudLogging - Codec to use to compress the persisted data.
type CompressionGoogleCloudLogging string

const (
	CompressionGoogleCloudLoggingNone CompressionGoogleCloudLogging = "none"
	CompressionGoogleCloudLoggingGzip CompressionGoogleCloudLogging = "gzip"
)

func (e CompressionGoogleCloudLogging) ToPointer() *CompressionGoogleCloudLogging {
	return &e
}
func (e *CompressionGoogleCloudLogging) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionGoogleCloudLogging(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionGoogleCloudLogging: %v", v)
	}
}

// QueueFullBehaviorGoogleCloudLogging - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorGoogleCloudLogging string

const (
	QueueFullBehaviorGoogleCloudLoggingBlock QueueFullBehaviorGoogleCloudLogging = "block"
	QueueFullBehaviorGoogleCloudLoggingDrop  QueueFullBehaviorGoogleCloudLogging = "drop"
)

func (e QueueFullBehaviorGoogleCloudLogging) ToPointer() *QueueFullBehaviorGoogleCloudLogging {
	return &e
}
func (e *QueueFullBehaviorGoogleCloudLogging) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorGoogleCloudLogging(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorGoogleCloudLogging: %v", v)
	}
}

// ModeGoogleCloudLogging - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type ModeGoogleCloudLogging string

const (
	ModeGoogleCloudLoggingError        ModeGoogleCloudLogging = "error"
	ModeGoogleCloudLoggingBackpressure ModeGoogleCloudLogging = "backpressure"
	ModeGoogleCloudLoggingAlways       ModeGoogleCloudLogging = "always"
)

func (e ModeGoogleCloudLogging) ToPointer() *ModeGoogleCloudLogging {
	return &e
}
func (e *ModeGoogleCloudLogging) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeGoogleCloudLogging(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeGoogleCloudLogging: %v", v)
	}
}

type PqControlsGoogleCloudLogging struct {
}

type OutputGoogleCloudLogging struct {
	// Unique ID for this output
	ID   *string                 `json:"id,omitempty"`
	Type *TypeGoogleCloudLogging `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags      []string        `json:"streamtags,omitempty"`
	LogLocationType LogLocationType `json:"logLocationType"`
	// JavaScript expression to compute the value of the log name.
	LogNameExpression string `json:"logNameExpression"`
	// Format to use when sending payload. Defaults to Text.
	PayloadFormat *PayloadFormat `default:"text" json:"payloadFormat"`
	// Labels to apply to the log entry
	LogLabels []LogLabel `json:"logLabels,omitempty"`
	// JavaScript expression to compute the value of the managed resource type field. Must evaluate to one of the valid values [here](https://cloud.google.com/logging/docs/api/v2/resource-list#resource-types). Defaults to "global".
	ResourceTypeExpression *string `json:"resourceTypeExpression,omitempty"`
	// Labels to apply to the managed resource. These must correspond to the valid labels for the specified resource type (see [here](https://cloud.google.com/logging/docs/api/v2/resource-list#resource-types)). Otherwise, they will be dropped by Google Cloud Logging.
	ResourceTypeLabels []ResourceTypeLabel `json:"resourceTypeLabels,omitempty"`
	// JavaScript expression to compute the value of the severity field. Must evaluate to one of the severity values supported by Google Cloud Logging [here](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logseverity) (case insensitive). Defaults to "DEFAULT".
	SeverityExpression *string `json:"severityExpression,omitempty"`
	// JavaScript expression to compute the value of the insert ID field.
	InsertIDExpression *string `json:"insertIdExpression,omitempty"`
	// Google authentication method. Choose Auto to use Google Application Default Credentials.
	GoogleAuthMethod *AuthenticationMethodGoogleCloudLogging `default:"manual" json:"googleAuthMethod"`
	// Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.
	ServiceAccountCredentials *string `json:"serviceAccountCredentials,omitempty"`
	// Select or create a stored text secret
	Secret *string `json:"secret,omitempty"`
	// Maximum size, in KB, of the request body.
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Max number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Maximum number of ongoing requests before blocking.
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it.
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum number of requests to limit to per second.
	ThrottleRateReqPerSec *int64 `json:"throttleRateReqPerSec,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request method as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	RequestMethodExpression *string `json:"requestMethodExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request URL as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	RequestURLExpression *string `json:"requestUrlExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request size as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	RequestSizeExpression *string `json:"requestSizeExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request method as a number. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	StatusExpression *string `json:"statusExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP response size as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	ResponseSizeExpression *string `json:"responseSizeExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request user agent as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	UserAgentExpression *string `json:"userAgentExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request remote IP as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	RemoteIPExpression *string `json:"remoteIpExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request server IP as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	ServerIPExpression *string `json:"serverIpExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request referer as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	RefererExpression *string `json:"refererExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request latency, formatted as <seconds>.<nanoseconds>s (for example, 1.23s). See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	LatencyExpression *string `json:"latencyExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request cache lookup as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	CacheLookupExpression *string `json:"cacheLookupExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request cache hit as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	CacheHitExpression *string `json:"cacheHitExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request cache validated with origin server as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	CacheValidatedExpression *string `json:"cacheValidatedExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request cache fill bytes as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	CacheFillBytesExpression *string `json:"cacheFillBytesExpression,omitempty"`
	// A JavaScript expression that evaluates to the HTTP request protocol as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.
	ProtocolExpression *string `json:"protocolExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry operation ID as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.
	IDExpression *string `json:"idExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry operation producer as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.
	ProducerExpression *string `json:"producerExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry operation first flag as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.
	FirstExpression *string `json:"firstExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry operation last flag as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.
	LastExpression *string `json:"lastExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry source location file as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentrysourcelocation) for details.
	FileExpression *string `json:"fileExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry source location line as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentrysourcelocation) for details.
	LineExpression *string `json:"lineExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry source location function as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentrysourcelocation) for details.
	FunctionExpression *string `json:"functionExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry log split UID as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logsplit) for details.
	UIDExpression *string `json:"uidExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry log split index as a number. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logsplit) for details.
	IndexExpression *string `json:"indexExpression,omitempty"`
	// A JavaScript expression that evaluates to the log entry log split total splits as a number. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logsplit) for details.
	TotalSplitsExpression *string `json:"totalSplitsExpression,omitempty"`
	// A JavaScript expression that evaluates to the REST resource name of the trace being written as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry) for details.
	TraceExpression *string `json:"traceExpression,omitempty"`
	// A JavaScript expression that evaluates to the ID of the cloud trace span associated with the current operation in which the log is being written as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry) for details.
	SpanIDExpression *string `json:"spanIdExpression,omitempty"`
	// A JavaScript expression that evaluates to the the sampling decision of the span associated with the log entry. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry) for details.
	TraceSampledExpression *string `json:"traceSampledExpression,omitempty"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorGoogleCloudLogging `default:"block" json:"onBackpressure"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	Description        *string  `json:"description,omitempty"`
	// JavaScript expression to compute the value of the folder ID with which log entries should be associated.
	LogLocationExpression string `json:"logLocationExpression"`
	// JavaScript expression to compute the value of the payload. Must evaluate to a JavaScript object value. If an invalid value is encountered it will result in the default value instead. Defaults to the entire event.
	PayloadExpression *string `json:"payloadExpression,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *CompressionGoogleCloudLogging `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorGoogleCloudLogging `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeGoogleCloudLogging       `default:"error" json:"pqMode"`
	PqControls *PqControlsGoogleCloudLogging `json:"pqControls,omitempty"`
	Status     *TFStatus                     `json:"status,omitempty"`
}

func (o OutputGoogleCloudLogging) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGoogleCloudLogging) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputGoogleCloudLogging) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputGoogleCloudLogging) GetType() *TypeGoogleCloudLogging {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputGoogleCloudLogging) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputGoogleCloudLogging) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputGoogleCloudLogging) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputGoogleCloudLogging) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputGoogleCloudLogging) GetLogLocationType() LogLocationType {
	if o == nil {
		return LogLocationType("")
	}
	return o.LogLocationType
}

func (o *OutputGoogleCloudLogging) GetLogNameExpression() string {
	if o == nil {
		return ""
	}
	return o.LogNameExpression
}

func (o *OutputGoogleCloudLogging) GetPayloadFormat() *PayloadFormat {
	if o == nil {
		return nil
	}
	return o.PayloadFormat
}

func (o *OutputGoogleCloudLogging) GetLogLabels() []LogLabel {
	if o == nil {
		return nil
	}
	return o.LogLabels
}

func (o *OutputGoogleCloudLogging) GetResourceTypeExpression() *string {
	if o == nil {
		return nil
	}
	return o.ResourceTypeExpression
}

func (o *OutputGoogleCloudLogging) GetResourceTypeLabels() []ResourceTypeLabel {
	if o == nil {
		return nil
	}
	return o.ResourceTypeLabels
}

func (o *OutputGoogleCloudLogging) GetSeverityExpression() *string {
	if o == nil {
		return nil
	}
	return o.SeverityExpression
}

func (o *OutputGoogleCloudLogging) GetInsertIDExpression() *string {
	if o == nil {
		return nil
	}
	return o.InsertIDExpression
}

func (o *OutputGoogleCloudLogging) GetGoogleAuthMethod() *AuthenticationMethodGoogleCloudLogging {
	if o == nil {
		return nil
	}
	return o.GoogleAuthMethod
}

func (o *OutputGoogleCloudLogging) GetServiceAccountCredentials() *string {
	if o == nil {
		return nil
	}
	return o.ServiceAccountCredentials
}

func (o *OutputGoogleCloudLogging) GetSecret() *string {
	if o == nil {
		return nil
	}
	return o.Secret
}

func (o *OutputGoogleCloudLogging) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputGoogleCloudLogging) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputGoogleCloudLogging) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputGoogleCloudLogging) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputGoogleCloudLogging) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputGoogleCloudLogging) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputGoogleCloudLogging) GetThrottleRateReqPerSec() *int64 {
	if o == nil {
		return nil
	}
	return o.ThrottleRateReqPerSec
}

func (o *OutputGoogleCloudLogging) GetRequestMethodExpression() *string {
	if o == nil {
		return nil
	}
	return o.RequestMethodExpression
}

func (o *OutputGoogleCloudLogging) GetRequestURLExpression() *string {
	if o == nil {
		return nil
	}
	return o.RequestURLExpression
}

func (o *OutputGoogleCloudLogging) GetRequestSizeExpression() *string {
	if o == nil {
		return nil
	}
	return o.RequestSizeExpression
}

func (o *OutputGoogleCloudLogging) GetStatusExpression() *string {
	if o == nil {
		return nil
	}
	return o.StatusExpression
}

func (o *OutputGoogleCloudLogging) GetResponseSizeExpression() *string {
	if o == nil {
		return nil
	}
	return o.ResponseSizeExpression
}

func (o *OutputGoogleCloudLogging) GetUserAgentExpression() *string {
	if o == nil {
		return nil
	}
	return o.UserAgentExpression
}

func (o *OutputGoogleCloudLogging) GetRemoteIPExpression() *string {
	if o == nil {
		return nil
	}
	return o.RemoteIPExpression
}

func (o *OutputGoogleCloudLogging) GetServerIPExpression() *string {
	if o == nil {
		return nil
	}
	return o.ServerIPExpression
}

func (o *OutputGoogleCloudLogging) GetRefererExpression() *string {
	if o == nil {
		return nil
	}
	return o.RefererExpression
}

func (o *OutputGoogleCloudLogging) GetLatencyExpression() *string {
	if o == nil {
		return nil
	}
	return o.LatencyExpression
}

func (o *OutputGoogleCloudLogging) GetCacheLookupExpression() *string {
	if o == nil {
		return nil
	}
	return o.CacheLookupExpression
}

func (o *OutputGoogleCloudLogging) GetCacheHitExpression() *string {
	if o == nil {
		return nil
	}
	return o.CacheHitExpression
}

func (o *OutputGoogleCloudLogging) GetCacheValidatedExpression() *string {
	if o == nil {
		return nil
	}
	return o.CacheValidatedExpression
}

func (o *OutputGoogleCloudLogging) GetCacheFillBytesExpression() *string {
	if o == nil {
		return nil
	}
	return o.CacheFillBytesExpression
}

func (o *OutputGoogleCloudLogging) GetProtocolExpression() *string {
	if o == nil {
		return nil
	}
	return o.ProtocolExpression
}

func (o *OutputGoogleCloudLogging) GetIDExpression() *string {
	if o == nil {
		return nil
	}
	return o.IDExpression
}

func (o *OutputGoogleCloudLogging) GetProducerExpression() *string {
	if o == nil {
		return nil
	}
	return o.ProducerExpression
}

func (o *OutputGoogleCloudLogging) GetFirstExpression() *string {
	if o == nil {
		return nil
	}
	return o.FirstExpression
}

func (o *OutputGoogleCloudLogging) GetLastExpression() *string {
	if o == nil {
		return nil
	}
	return o.LastExpression
}

func (o *OutputGoogleCloudLogging) GetFileExpression() *string {
	if o == nil {
		return nil
	}
	return o.FileExpression
}

func (o *OutputGoogleCloudLogging) GetLineExpression() *string {
	if o == nil {
		return nil
	}
	return o.LineExpression
}

func (o *OutputGoogleCloudLogging) GetFunctionExpression() *string {
	if o == nil {
		return nil
	}
	return o.FunctionExpression
}

func (o *OutputGoogleCloudLogging) GetUIDExpression() *string {
	if o == nil {
		return nil
	}
	return o.UIDExpression
}

func (o *OutputGoogleCloudLogging) GetIndexExpression() *string {
	if o == nil {
		return nil
	}
	return o.IndexExpression
}

func (o *OutputGoogleCloudLogging) GetTotalSplitsExpression() *string {
	if o == nil {
		return nil
	}
	return o.TotalSplitsExpression
}

func (o *OutputGoogleCloudLogging) GetTraceExpression() *string {
	if o == nil {
		return nil
	}
	return o.TraceExpression
}

func (o *OutputGoogleCloudLogging) GetSpanIDExpression() *string {
	if o == nil {
		return nil
	}
	return o.SpanIDExpression
}

func (o *OutputGoogleCloudLogging) GetTraceSampledExpression() *string {
	if o == nil {
		return nil
	}
	return o.TraceSampledExpression
}

func (o *OutputGoogleCloudLogging) GetOnBackpressure() *BackpressureBehaviorGoogleCloudLogging {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputGoogleCloudLogging) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputGoogleCloudLogging) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputGoogleCloudLogging) GetLogLocationExpression() string {
	if o == nil {
		return ""
	}
	return o.LogLocationExpression
}

func (o *OutputGoogleCloudLogging) GetPayloadExpression() *string {
	if o == nil {
		return nil
	}
	return o.PayloadExpression
}

func (o *OutputGoogleCloudLogging) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputGoogleCloudLogging) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputGoogleCloudLogging) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputGoogleCloudLogging) GetPqCompress() *CompressionGoogleCloudLogging {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputGoogleCloudLogging) GetPqOnBackpressure() *QueueFullBehaviorGoogleCloudLogging {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputGoogleCloudLogging) GetPqMode() *ModeGoogleCloudLogging {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputGoogleCloudLogging) GetPqControls() *PqControlsGoogleCloudLogging {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputGoogleCloudLogging) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeGoogleCloudStorage string

const (
	TypeGoogleCloudStorageGoogleCloudStorage TypeGoogleCloudStorage = "google_cloud_storage"
)

func (e TypeGoogleCloudStorage) ToPointer() *TypeGoogleCloudStorage {
	return &e
}
func (e *TypeGoogleCloudStorage) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "google_cloud_storage":
		*e = TypeGoogleCloudStorage(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeGoogleCloudStorage: %v", v)
	}
}

// SignatureVersionGoogleCloudStorage - Signature version to use for signing Google Cloud Storage requests.
type SignatureVersionGoogleCloudStorage string

const (
	SignatureVersionGoogleCloudStorageV2 SignatureVersionGoogleCloudStorage = "v2"
	SignatureVersionGoogleCloudStorageV4 SignatureVersionGoogleCloudStorage = "v4"
)

func (e SignatureVersionGoogleCloudStorage) ToPointer() *SignatureVersionGoogleCloudStorage {
	return &e
}
func (e *SignatureVersionGoogleCloudStorage) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "v2":
		fallthrough
	case "v4":
		*e = SignatureVersionGoogleCloudStorage(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SignatureVersionGoogleCloudStorage: %v", v)
	}
}

type AuthenticationMethodGoogleCloudStorage string

const (
	AuthenticationMethodGoogleCloudStorageAuto   AuthenticationMethodGoogleCloudStorage = "auto"
	AuthenticationMethodGoogleCloudStorageManual AuthenticationMethodGoogleCloudStorage = "manual"
	AuthenticationMethodGoogleCloudStorageSecret AuthenticationMethodGoogleCloudStorage = "secret"
)

func (e AuthenticationMethodGoogleCloudStorage) ToPointer() *AuthenticationMethodGoogleCloudStorage {
	return &e
}
func (e *AuthenticationMethodGoogleCloudStorage) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "auto":
		fallthrough
	case "manual":
		fallthrough
	case "secret":
		*e = AuthenticationMethodGoogleCloudStorage(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodGoogleCloudStorage: %v", v)
	}
}

// ObjectACLGoogleCloudStorage - Object ACL to assign to uploaded objects.
type ObjectACLGoogleCloudStorage string

const (
	ObjectACLGoogleCloudStoragePrivate                ObjectACLGoogleCloudStorage = "private"
	ObjectACLGoogleCloudStorageBucketOwnerRead        ObjectACLGoogleCloudStorage = "bucket-owner-read"
	ObjectACLGoogleCloudStorageBucketOwnerFullControl ObjectACLGoogleCloudStorage = "bucket-owner-full-control"
	ObjectACLGoogleCloudStorageProjectPrivate         ObjectACLGoogleCloudStorage = "project-private"
	ObjectACLGoogleCloudStorageAuthenticatedRead      ObjectACLGoogleCloudStorage = "authenticated-read"
	ObjectACLGoogleCloudStoragePublicRead             ObjectACLGoogleCloudStorage = "public-read"
)

func (e ObjectACLGoogleCloudStorage) ToPointer() *ObjectACLGoogleCloudStorage {
	return &e
}
func (e *ObjectACLGoogleCloudStorage) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "private":
		fallthrough
	case "bucket-owner-read":
		fallthrough
	case "bucket-owner-full-control":
		fallthrough
	case "project-private":
		fallthrough
	case "authenticated-read":
		fallthrough
	case "public-read":
		*e = ObjectACLGoogleCloudStorage(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ObjectACLGoogleCloudStorage: %v", v)
	}
}

// StorageClassGoogleCloudStorage - Storage class to select for uploaded objects.
type StorageClassGoogleCloudStorage string

const (
	StorageClassGoogleCloudStorageStandard StorageClassGoogleCloudStorage = "STANDARD"
	StorageClassGoogleCloudStorageNearline StorageClassGoogleCloudStorage = "NEARLINE"
	StorageClassGoogleCloudStorageColdline StorageClassGoogleCloudStorage = "COLDLINE"
	StorageClassGoogleCloudStorageArchive  StorageClassGoogleCloudStorage = "ARCHIVE"
)

func (e StorageClassGoogleCloudStorage) ToPointer() *StorageClassGoogleCloudStorage {
	return &e
}
func (e *StorageClassGoogleCloudStorage) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "STANDARD":
		fallthrough
	case "NEARLINE":
		fallthrough
	case "COLDLINE":
		fallthrough
	case "ARCHIVE":
		*e = StorageClassGoogleCloudStorage(v)
		return nil
	default:
		return fmt.Errorf("invalid value for StorageClassGoogleCloudStorage: %v", v)
	}
}

// DataFormatGoogleCloudStorage - Format of the output data
type DataFormatGoogleCloudStorage string

const (
	DataFormatGoogleCloudStorageJSON    DataFormatGoogleCloudStorage = "json"
	DataFormatGoogleCloudStorageRaw     DataFormatGoogleCloudStorage = "raw"
	DataFormatGoogleCloudStorageParquet DataFormatGoogleCloudStorage = "parquet"
)

func (e DataFormatGoogleCloudStorage) ToPointer() *DataFormatGoogleCloudStorage {
	return &e
}
func (e *DataFormatGoogleCloudStorage) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "json":
		fallthrough
	case "raw":
		fallthrough
	case "parquet":
		*e = DataFormatGoogleCloudStorage(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DataFormatGoogleCloudStorage: %v", v)
	}
}

// BackpressureBehaviorGoogleCloudStorage - Whether to block or drop events when all receivers are exerting backpressure
type BackpressureBehaviorGoogleCloudStorage string

const (
	BackpressureBehaviorGoogleCloudStorageBlock BackpressureBehaviorGoogleCloudStorage = "block"
	BackpressureBehaviorGoogleCloudStorageDrop  BackpressureBehaviorGoogleCloudStorage = "drop"
)

func (e BackpressureBehaviorGoogleCloudStorage) ToPointer() *BackpressureBehaviorGoogleCloudStorage {
	return &e
}
func (e *BackpressureBehaviorGoogleCloudStorage) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = BackpressureBehaviorGoogleCloudStorage(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorGoogleCloudStorage: %v", v)
	}
}

// DiskSpaceProtectionGoogleCloudStorage - Whether to block or drop events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionGoogleCloudStorage string

const (
	DiskSpaceProtectionGoogleCloudStorageBlock DiskSpaceProtectionGoogleCloudStorage = "block"
	DiskSpaceProtectionGoogleCloudStorageDrop  DiskSpaceProtectionGoogleCloudStorage = "drop"
)

func (e DiskSpaceProtectionGoogleCloudStorage) ToPointer() *DiskSpaceProtectionGoogleCloudStorage {
	return &e
}
func (e *DiskSpaceProtectionGoogleCloudStorage) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = DiskSpaceProtectionGoogleCloudStorage(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DiskSpaceProtectionGoogleCloudStorage: %v", v)
	}
}

// CompressGoogleCloudStorage - Choose data compression format to apply before moving files to final destination
type CompressGoogleCloudStorage string

const (
	CompressGoogleCloudStorageNone CompressGoogleCloudStorage = "none"
	CompressGoogleCloudStorageGzip CompressGoogleCloudStorage = "gzip"
)

func (e CompressGoogleCloudStorage) ToPointer() *CompressGoogleCloudStorage {
	return &e
}
func (e *CompressGoogleCloudStorage) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressGoogleCloudStorage(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressGoogleCloudStorage: %v", v)
	}
}

// CompressionLevelGoogleCloudStorage - Compression level to apply before moving files to final destination
type CompressionLevelGoogleCloudStorage string

const (
	CompressionLevelGoogleCloudStorageBestSpeed       CompressionLevelGoogleCloudStorage = "best_speed"
	CompressionLevelGoogleCloudStorageNormal          CompressionLevelGoogleCloudStorage = "normal"
	CompressionLevelGoogleCloudStorageBestCompression CompressionLevelGoogleCloudStorage = "best_compression"
)

func (e CompressionLevelGoogleCloudStorage) ToPointer() *CompressionLevelGoogleCloudStorage {
	return &e
}
func (e *CompressionLevelGoogleCloudStorage) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "best_speed":
		fallthrough
	case "normal":
		fallthrough
	case "best_compression":
		*e = CompressionLevelGoogleCloudStorage(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionLevelGoogleCloudStorage: %v", v)
	}
}

// ParquetVersionGoogleCloudStorage - Determines which data types are supported and how they are represented
type ParquetVersionGoogleCloudStorage string

const (
	ParquetVersionGoogleCloudStorageParquet10 ParquetVersionGoogleCloudStorage = "PARQUET_1_0"
	ParquetVersionGoogleCloudStorageParquet24 ParquetVersionGoogleCloudStorage = "PARQUET_2_4"
	ParquetVersionGoogleCloudStorageParquet26 ParquetVersionGoogleCloudStorage = "PARQUET_2_6"
)

func (e ParquetVersionGoogleCloudStorage) ToPointer() *ParquetVersionGoogleCloudStorage {
	return &e
}
func (e *ParquetVersionGoogleCloudStorage) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "PARQUET_1_0":
		fallthrough
	case "PARQUET_2_4":
		fallthrough
	case "PARQUET_2_6":
		*e = ParquetVersionGoogleCloudStorage(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ParquetVersionGoogleCloudStorage: %v", v)
	}
}

// DataPageVersionGoogleCloudStorage - Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
type DataPageVersionGoogleCloudStorage string

const (
	DataPageVersionGoogleCloudStorageDataPageV1 DataPageVersionGoogleCloudStorage = "DATA_PAGE_V1"
	DataPageVersionGoogleCloudStorageDataPageV2 DataPageVersionGoogleCloudStorage = "DATA_PAGE_V2"
)

func (e DataPageVersionGoogleCloudStorage) ToPointer() *DataPageVersionGoogleCloudStorage {
	return &e
}
func (e *DataPageVersionGoogleCloudStorage) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "DATA_PAGE_V1":
		fallthrough
	case "DATA_PAGE_V2":
		*e = DataPageVersionGoogleCloudStorage(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DataPageVersionGoogleCloudStorage: %v", v)
	}
}

type KeyValueMetadatumGoogleCloudStorage struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (k KeyValueMetadatumGoogleCloudStorage) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(k, "", false)
}

func (k *KeyValueMetadatumGoogleCloudStorage) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &k, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *KeyValueMetadatumGoogleCloudStorage) GetKey() *string {
	if o == nil {
		return nil
	}
	return o.Key
}

func (o *KeyValueMetadatumGoogleCloudStorage) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OutputGoogleCloudStorage struct {
	// Unique ID for this output
	ID   *string                 `json:"id,omitempty"`
	Type *TypeGoogleCloudStorage `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Name of the destination bucket. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example of referencing a Global Variable: `myBucket-${C.vars.myVar}`.
	Bucket string `json:"bucket"`
	// Region where the bucket is located
	Region string `json:"region"`
	// Google Cloud Storage service endpoint.
	Endpoint *string `default:"https://storage.googleapis.com" json:"endpoint"`
	// Signature version to use for signing Google Cloud Storage requests.
	SignatureVersion        *SignatureVersionGoogleCloudStorage     `default:"v4" json:"signatureVersion"`
	AwsAuthenticationMethod *AuthenticationMethodGoogleCloudStorage `default:"manual" json:"awsAuthenticationMethod"`
	// Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant stable storage.
	StagePath *string `default:"\\$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Prefix to append to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. E.g., referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`.
	DestPath *string `default:"" json:"destPath"`
	// Disable if you can access files within the bucket but not the bucket itself.
	VerifyPermissions *bool `default:"true" json:"verifyPermissions"`
	// Object ACL to assign to uploaded objects.
	ObjectACL *ObjectACLGoogleCloudStorage `default:"private" json:"objectACL"`
	// Storage class to select for uploaded objects.
	StorageClass *StorageClassGoogleCloudStorage `json:"storageClass,omitempty"`
	// Whether to reuse connections between requests, which can improve performance.
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Whether to reject certificates that cannot be verified against a valid CA (e.g., self-signed certificates).
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Append output's ID to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value  if present  otherwise to each location's root directory.
	PartitionExpr *string `default:"C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')" json:"partitionExpr"`
	// Format of the output data
	Format *DataFormatGoogleCloudStorage `default:"json" json:"format"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// Whether to block or drop events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorGoogleCloudStorage `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, dead-letter it to prevent further errors.
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// Whether to block or drop events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionGoogleCloudStorage `default:"block" json:"onDiskFullBackpressure"`
	Description            *string                                `json:"description,omitempty"`
	// Choose data compression format to apply before moving files to final destination
	Compress *CompressGoogleCloudStorage `default:"gzip" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *CompressionLevelGoogleCloudStorage `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionGoogleCloudStorage `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *DataPageVersionGoogleCloudStorage `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging, e.g., "key":"OCSF Event Class", "value":"9001".
	KeyValueMetadata []KeyValueMetadatumGoogleCloudStorage `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// How frequently, in seconds, to clean up empty directories when 'Remove empty staging dirs' is enabled
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"\\$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64 `default:"20" json:"maxRetryNum"`
	// HMAC access key. This value can be a constant or a JavaScript expression (e.g., `${C.env.GCS_ACCESS_KEY}`).
	AwsAPIKey *string `json:"awsApiKey,omitempty"`
	// HMAC secret. This value can be a constant or a JavaScript expression (e.g., `${C.env.GCS_SECRET}`).
	AwsSecretKey *string `json:"awsSecretKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string   `json:"awsSecret,omitempty"`
	Status    *TFStatus `json:"status,omitempty"`
}

func (o OutputGoogleCloudStorage) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGoogleCloudStorage) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputGoogleCloudStorage) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputGoogleCloudStorage) GetType() *TypeGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputGoogleCloudStorage) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputGoogleCloudStorage) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputGoogleCloudStorage) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputGoogleCloudStorage) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputGoogleCloudStorage) GetBucket() string {
	if o == nil {
		return ""
	}
	return o.Bucket
}

func (o *OutputGoogleCloudStorage) GetRegion() string {
	if o == nil {
		return ""
	}
	return o.Region
}

func (o *OutputGoogleCloudStorage) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputGoogleCloudStorage) GetSignatureVersion() *SignatureVersionGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputGoogleCloudStorage) GetAwsAuthenticationMethod() *AuthenticationMethodGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputGoogleCloudStorage) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputGoogleCloudStorage) GetDestPath() *string {
	if o == nil {
		return nil
	}
	return o.DestPath
}

func (o *OutputGoogleCloudStorage) GetVerifyPermissions() *bool {
	if o == nil {
		return nil
	}
	return o.VerifyPermissions
}

func (o *OutputGoogleCloudStorage) GetObjectACL() *ObjectACLGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.ObjectACL
}

func (o *OutputGoogleCloudStorage) GetStorageClass() *StorageClassGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.StorageClass
}

func (o *OutputGoogleCloudStorage) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputGoogleCloudStorage) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputGoogleCloudStorage) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputGoogleCloudStorage) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputGoogleCloudStorage) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputGoogleCloudStorage) GetFormat() *DataFormatGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputGoogleCloudStorage) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputGoogleCloudStorage) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputGoogleCloudStorage) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputGoogleCloudStorage) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputGoogleCloudStorage) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputGoogleCloudStorage) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputGoogleCloudStorage) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputGoogleCloudStorage) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputGoogleCloudStorage) GetOnBackpressure() *BackpressureBehaviorGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputGoogleCloudStorage) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputGoogleCloudStorage) GetOnDiskFullBackpressure() *DiskSpaceProtectionGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputGoogleCloudStorage) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputGoogleCloudStorage) GetCompress() *CompressGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputGoogleCloudStorage) GetCompressionLevel() *CompressionLevelGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputGoogleCloudStorage) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputGoogleCloudStorage) GetParquetVersion() *ParquetVersionGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputGoogleCloudStorage) GetParquetDataPageVersion() *DataPageVersionGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputGoogleCloudStorage) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputGoogleCloudStorage) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputGoogleCloudStorage) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputGoogleCloudStorage) GetKeyValueMetadata() []KeyValueMetadatumGoogleCloudStorage {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputGoogleCloudStorage) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputGoogleCloudStorage) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputGoogleCloudStorage) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputGoogleCloudStorage) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputGoogleCloudStorage) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputGoogleCloudStorage) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

func (o *OutputGoogleCloudStorage) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputGoogleCloudStorage) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputGoogleCloudStorage) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputGoogleCloudStorage) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeGoogleChronicle string

const (
	TypeGoogleChronicleGoogleChronicle TypeGoogleChronicle = "google_chronicle"
)

func (e TypeGoogleChronicle) ToPointer() *TypeGoogleChronicle {
	return &e
}
func (e *TypeGoogleChronicle) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "google_chronicle":
		*e = TypeGoogleChronicle(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeGoogleChronicle: %v", v)
	}
}

type OutputAPIVersion string

const (
	OutputAPIVersionV1 OutputAPIVersion = "v1"
	OutputAPIVersionV2 OutputAPIVersion = "v2"
)

func (e OutputAPIVersion) ToPointer() *OutputAPIVersion {
	return &e
}
func (e *OutputAPIVersion) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "v1":
		fallthrough
	case "v2":
		*e = OutputAPIVersion(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputAPIVersion: %v", v)
	}
}

type AuthenticationMethodGoogleChronicle string

const (
	AuthenticationMethodGoogleChronicleManual               AuthenticationMethodGoogleChronicle = "manual"
	AuthenticationMethodGoogleChronicleSecret               AuthenticationMethodGoogleChronicle = "secret"
	AuthenticationMethodGoogleChronicleServiceAccount       AuthenticationMethodGoogleChronicle = "serviceAccount"
	AuthenticationMethodGoogleChronicleServiceAccountSecret AuthenticationMethodGoogleChronicle = "serviceAccountSecret"
)

func (e AuthenticationMethodGoogleChronicle) ToPointer() *AuthenticationMethodGoogleChronicle {
	return &e
}
func (e *AuthenticationMethodGoogleChronicle) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		fallthrough
	case "serviceAccount":
		fallthrough
	case "serviceAccountSecret":
		*e = AuthenticationMethodGoogleChronicle(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodGoogleChronicle: %v", v)
	}
}

type ResponseRetrySettingGoogleChronicle struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingGoogleChronicle) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingGoogleChronicle) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingGoogleChronicle) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingGoogleChronicle) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingGoogleChronicle) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingGoogleChronicle) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsGoogleChronicle struct {
	// Enable to retry on request timeout
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsGoogleChronicle) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsGoogleChronicle) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsGoogleChronicle) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsGoogleChronicle) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsGoogleChronicle) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsGoogleChronicle) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type SendEventsAs string

const (
	SendEventsAsUnstructured SendEventsAs = "unstructured"
	SendEventsAsUdm          SendEventsAs = "udm"
)

func (e SendEventsAs) ToPointer() *SendEventsAs {
	return &e
}
func (e *SendEventsAs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "unstructured":
		fallthrough
	case "udm":
		*e = SendEventsAs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SendEventsAs: %v", v)
	}
}

type ExtraHTTPHeaderGoogleChronicle struct {
	// Field name
	Name *string `json:"name,omitempty"`
	// Field value
	Value string `json:"value"`
}

func (o *ExtraHTTPHeaderGoogleChronicle) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderGoogleChronicle) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeGoogleChronicle - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeGoogleChronicle string

const (
	FailedRequestLoggingModeGoogleChroniclePayload           FailedRequestLoggingModeGoogleChronicle = "payload"
	FailedRequestLoggingModeGoogleChroniclePayloadAndHeaders FailedRequestLoggingModeGoogleChronicle = "payloadAndHeaders"
	FailedRequestLoggingModeGoogleChronicleNone              FailedRequestLoggingModeGoogleChronicle = "none"
)

func (e FailedRequestLoggingModeGoogleChronicle) ToPointer() *FailedRequestLoggingModeGoogleChronicle {
	return &e
}
func (e *FailedRequestLoggingModeGoogleChronicle) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeGoogleChronicle(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeGoogleChronicle: %v", v)
	}
}

// BackpressureBehaviorGoogleChronicle - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorGoogleChronicle string

const (
	BackpressureBehaviorGoogleChronicleBlock BackpressureBehaviorGoogleChronicle = "block"
	BackpressureBehaviorGoogleChronicleDrop  BackpressureBehaviorGoogleChronicle = "drop"
	BackpressureBehaviorGoogleChronicleQueue BackpressureBehaviorGoogleChronicle = "queue"
)

func (e BackpressureBehaviorGoogleChronicle) ToPointer() *BackpressureBehaviorGoogleChronicle {
	return &e
}
func (e *BackpressureBehaviorGoogleChronicle) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorGoogleChronicle(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorGoogleChronicle: %v", v)
	}
}

type ExtraLogType struct {
	// Log type
	LogType string `json:"logType"`
	// Log type description
	Description *string `json:"description,omitempty"`
}

func (o *ExtraLogType) GetLogType() string {
	if o == nil {
		return ""
	}
	return o.LogType
}

func (o *ExtraLogType) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

type CustomLabel struct {
	// Label key
	Key string `json:"key"`
	// Label value
	Value string `json:"value"`
}

func (o *CustomLabel) GetKey() string {
	if o == nil {
		return ""
	}
	return o.Key
}

func (o *CustomLabel) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// CompressionGoogleChronicle - Codec to use to compress the persisted data.
type CompressionGoogleChronicle string

const (
	CompressionGoogleChronicleNone CompressionGoogleChronicle = "none"
	CompressionGoogleChronicleGzip CompressionGoogleChronicle = "gzip"
)

func (e CompressionGoogleChronicle) ToPointer() *CompressionGoogleChronicle {
	return &e
}
func (e *CompressionGoogleChronicle) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionGoogleChronicle(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionGoogleChronicle: %v", v)
	}
}

// QueueFullBehaviorGoogleChronicle - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorGoogleChronicle string

const (
	QueueFullBehaviorGoogleChronicleBlock QueueFullBehaviorGoogleChronicle = "block"
	QueueFullBehaviorGoogleChronicleDrop  QueueFullBehaviorGoogleChronicle = "drop"
)

func (e QueueFullBehaviorGoogleChronicle) ToPointer() *QueueFullBehaviorGoogleChronicle {
	return &e
}
func (e *QueueFullBehaviorGoogleChronicle) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorGoogleChronicle(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorGoogleChronicle: %v", v)
	}
}

// ModeGoogleChronicle - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type ModeGoogleChronicle string

const (
	ModeGoogleChronicleError        ModeGoogleChronicle = "error"
	ModeGoogleChronicleBackpressure ModeGoogleChronicle = "backpressure"
	ModeGoogleChronicleAlways       ModeGoogleChronicle = "always"
)

func (e ModeGoogleChronicle) ToPointer() *ModeGoogleChronicle {
	return &e
}
func (e *ModeGoogleChronicle) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeGoogleChronicle(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeGoogleChronicle: %v", v)
	}
}

type PqControlsGoogleChronicle struct {
}

type OutputGoogleChronicle struct {
	// Unique ID for this output
	ID   *string             `json:"id,omitempty"`
	Type TypeGoogleChronicle `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags           []string                             `json:"streamtags,omitempty"`
	APIVersion           *OutputAPIVersion                    `default:"v1" json:"apiVersion"`
	AuthenticationMethod *AuthenticationMethodGoogleChronicle `default:"serviceAccount" json:"authenticationMethod"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable).
	ResponseRetrySettings []ResponseRetrySettingGoogleChronicle `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsGoogleChronicle  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool         `default:"false" json:"responseHonorRetryAfterHeader"`
	LogFormatType                 *SendEventsAs `default:"unstructured" json:"logFormatType"`
	// Regional endpoint to send events to
	Region *string `json:"region,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"1024" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Defaults to Yes. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"90" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max body size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events.
	ExtraHTTPHeaders []ExtraHTTPHeaderGoogleChronicle `json:"extraHttpHeaders,omitempty"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeGoogleChronicle `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Enables round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorGoogleChronicle `default:"block" json:"onBackpressure"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	Description        *string  `json:"description,omitempty"`
	// Custom log types. If the value "Custom" is selected in the setting "Default log type" above, the first custom log type in this table will be automatically selected as default log type.
	ExtraLogTypes []ExtraLogType `json:"extraLogTypes,omitempty"`
	// Default log type value to send to SecOps. Can be overwritten by event field __logType.
	LogType *string `json:"logType,omitempty"`
	// Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event.
	LogTextField *string `json:"logTextField,omitempty"`
	// Unique identifier (UUID) corresponding to a particular SecOps instance. Provided by your SecOps representative.
	CustomerID *string `json:"customerId,omitempty"`
	// User-configured environment namespace to identify the data domain the logs originated from. Use namespace as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace.
	Namespace *string `json:"namespace,omitempty"`
	// Custom labels to be added to every batch.
	CustomLabels []CustomLabel `json:"customLabels,omitempty"`
	// Organization's API key in Google SecOps
	APIKey *string `json:"apiKey,omitempty"`
	// Select or create a stored text secret
	APIKeySecret *string `json:"apiKeySecret,omitempty"`
	// Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.
	ServiceAccountCredentials *string `json:"serviceAccountCredentials,omitempty"`
	// Select or create a stored text secret
	ServiceAccountCredentialsSecret *string `json:"serviceAccountCredentialsSecret,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *CompressionGoogleChronicle `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorGoogleChronicle `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeGoogleChronicle       `default:"error" json:"pqMode"`
	PqControls *PqControlsGoogleChronicle `json:"pqControls,omitempty"`
	Status     *TFStatus                  `json:"status,omitempty"`
}

func (o OutputGoogleChronicle) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputGoogleChronicle) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputGoogleChronicle) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputGoogleChronicle) GetType() TypeGoogleChronicle {
	if o == nil {
		return TypeGoogleChronicle("")
	}
	return o.Type
}

func (o *OutputGoogleChronicle) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputGoogleChronicle) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputGoogleChronicle) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputGoogleChronicle) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputGoogleChronicle) GetAPIVersion() *OutputAPIVersion {
	if o == nil {
		return nil
	}
	return o.APIVersion
}

func (o *OutputGoogleChronicle) GetAuthenticationMethod() *AuthenticationMethodGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.AuthenticationMethod
}

func (o *OutputGoogleChronicle) GetResponseRetrySettings() []ResponseRetrySettingGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputGoogleChronicle) GetTimeoutRetrySettings() *TimeoutRetrySettingsGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputGoogleChronicle) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputGoogleChronicle) GetLogFormatType() *SendEventsAs {
	if o == nil {
		return nil
	}
	return o.LogFormatType
}

func (o *OutputGoogleChronicle) GetRegion() *string {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputGoogleChronicle) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputGoogleChronicle) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputGoogleChronicle) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputGoogleChronicle) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputGoogleChronicle) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputGoogleChronicle) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputGoogleChronicle) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputGoogleChronicle) GetExtraHTTPHeaders() []ExtraHTTPHeaderGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputGoogleChronicle) GetFailedRequestLoggingMode() *FailedRequestLoggingModeGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputGoogleChronicle) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputGoogleChronicle) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputGoogleChronicle) GetOnBackpressure() *BackpressureBehaviorGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputGoogleChronicle) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputGoogleChronicle) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputGoogleChronicle) GetExtraLogTypes() []ExtraLogType {
	if o == nil {
		return nil
	}
	return o.ExtraLogTypes
}

func (o *OutputGoogleChronicle) GetLogType() *string {
	if o == nil {
		return nil
	}
	return o.LogType
}

func (o *OutputGoogleChronicle) GetLogTextField() *string {
	if o == nil {
		return nil
	}
	return o.LogTextField
}

func (o *OutputGoogleChronicle) GetCustomerID() *string {
	if o == nil {
		return nil
	}
	return o.CustomerID
}

func (o *OutputGoogleChronicle) GetNamespace() *string {
	if o == nil {
		return nil
	}
	return o.Namespace
}

func (o *OutputGoogleChronicle) GetCustomLabels() []CustomLabel {
	if o == nil {
		return nil
	}
	return o.CustomLabels
}

func (o *OutputGoogleChronicle) GetAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.APIKey
}

func (o *OutputGoogleChronicle) GetAPIKeySecret() *string {
	if o == nil {
		return nil
	}
	return o.APIKeySecret
}

func (o *OutputGoogleChronicle) GetServiceAccountCredentials() *string {
	if o == nil {
		return nil
	}
	return o.ServiceAccountCredentials
}

func (o *OutputGoogleChronicle) GetServiceAccountCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.ServiceAccountCredentialsSecret
}

func (o *OutputGoogleChronicle) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputGoogleChronicle) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputGoogleChronicle) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputGoogleChronicle) GetPqCompress() *CompressionGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputGoogleChronicle) GetPqOnBackpressure() *QueueFullBehaviorGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputGoogleChronicle) GetPqMode() *ModeGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputGoogleChronicle) GetPqControls() *PqControlsGoogleChronicle {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputGoogleChronicle) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeAzureEventhub string

const (
	TypeAzureEventhubAzureEventhub TypeAzureEventhub = "azure_eventhub"
)

func (e TypeAzureEventhub) ToPointer() *TypeAzureEventhub {
	return &e
}
func (e *TypeAzureEventhub) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "azure_eventhub":
		*e = TypeAzureEventhub(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeAzureEventhub: %v", v)
	}
}

// AcknowledgmentsAzureEventhub - Control the number of required acknowledgments
type AcknowledgmentsAzureEventhub int64

const (
	AcknowledgmentsAzureEventhubOne    AcknowledgmentsAzureEventhub = 1
	AcknowledgmentsAzureEventhubZero   AcknowledgmentsAzureEventhub = 0
	AcknowledgmentsAzureEventhubMinus1 AcknowledgmentsAzureEventhub = -1
)

func (e AcknowledgmentsAzureEventhub) ToPointer() *AcknowledgmentsAzureEventhub {
	return &e
}
func (e *AcknowledgmentsAzureEventhub) UnmarshalJSON(data []byte) error {
	var v int64
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case 1:
		fallthrough
	case 0:
		fallthrough
	case -1:
		*e = AcknowledgmentsAzureEventhub(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AcknowledgmentsAzureEventhub: %v", v)
	}
}

// RecordDataFormatAzureEventhub - Format to use to serialize events before writing to the Event Hubs Kafka brokers.
type RecordDataFormatAzureEventhub string

const (
	RecordDataFormatAzureEventhubJSON RecordDataFormatAzureEventhub = "json"
	RecordDataFormatAzureEventhubRaw  RecordDataFormatAzureEventhub = "raw"
)

func (e RecordDataFormatAzureEventhub) ToPointer() *RecordDataFormatAzureEventhub {
	return &e
}
func (e *RecordDataFormatAzureEventhub) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "json":
		fallthrough
	case "raw":
		*e = RecordDataFormatAzureEventhub(v)
		return nil
	default:
		return fmt.Errorf("invalid value for RecordDataFormatAzureEventhub: %v", v)
	}
}

// SASLMechanismAzureEventhub - SASL authentication mechanism to use
type SASLMechanismAzureEventhub string

const (
	SASLMechanismAzureEventhubPlain       SASLMechanismAzureEventhub = "plain"
	SASLMechanismAzureEventhubOauthbearer SASLMechanismAzureEventhub = "oauthbearer"
)

func (e SASLMechanismAzureEventhub) ToPointer() *SASLMechanismAzureEventhub {
	return &e
}
func (e *SASLMechanismAzureEventhub) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "plain":
		fallthrough
	case "oauthbearer":
		*e = SASLMechanismAzureEventhub(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SASLMechanismAzureEventhub: %v", v)
	}
}

// AuthenticationAzureEventhub - Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
type AuthenticationAzureEventhub struct {
	// Enable authentication.
	Disabled *bool `default:"false" json:"disabled"`
	// SASL authentication mechanism to use
	Mechanism *SASLMechanismAzureEventhub `default:"plain" json:"mechanism"`
}

func (a AuthenticationAzureEventhub) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(a, "", false)
}

func (a *AuthenticationAzureEventhub) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &a, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *AuthenticationAzureEventhub) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *AuthenticationAzureEventhub) GetMechanism() *SASLMechanismAzureEventhub {
	if o == nil {
		return nil
	}
	return o.Mechanism
}

type TLSSettingsClientSideAzureEventhub struct {
	Disabled *bool `default:"false" json:"disabled"`
	// Reject certs that are not authorized by a CA in the CA certificate path, or by another trusted CA (e.g., the system's CA).
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
}

func (t TLSSettingsClientSideAzureEventhub) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideAzureEventhub) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TLSSettingsClientSideAzureEventhub) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *TLSSettingsClientSideAzureEventhub) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

// BackpressureBehaviorAzureEventhub - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorAzureEventhub string

const (
	BackpressureBehaviorAzureEventhubBlock BackpressureBehaviorAzureEventhub = "block"
	BackpressureBehaviorAzureEventhubDrop  BackpressureBehaviorAzureEventhub = "drop"
	BackpressureBehaviorAzureEventhubQueue BackpressureBehaviorAzureEventhub = "queue"
)

func (e BackpressureBehaviorAzureEventhub) ToPointer() *BackpressureBehaviorAzureEventhub {
	return &e
}
func (e *BackpressureBehaviorAzureEventhub) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorAzureEventhub(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorAzureEventhub: %v", v)
	}
}

// CompressionAzureEventhub - Codec to use to compress the persisted data.
type CompressionAzureEventhub string

const (
	CompressionAzureEventhubNone CompressionAzureEventhub = "none"
	CompressionAzureEventhubGzip CompressionAzureEventhub = "gzip"
)

func (e CompressionAzureEventhub) ToPointer() *CompressionAzureEventhub {
	return &e
}
func (e *CompressionAzureEventhub) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionAzureEventhub(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionAzureEventhub: %v", v)
	}
}

// QueueFullBehaviorAzureEventhub - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorAzureEventhub string

const (
	QueueFullBehaviorAzureEventhubBlock QueueFullBehaviorAzureEventhub = "block"
	QueueFullBehaviorAzureEventhubDrop  QueueFullBehaviorAzureEventhub = "drop"
)

func (e QueueFullBehaviorAzureEventhub) ToPointer() *QueueFullBehaviorAzureEventhub {
	return &e
}
func (e *QueueFullBehaviorAzureEventhub) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorAzureEventhub(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorAzureEventhub: %v", v)
	}
}

// ModeAzureEventhub - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type ModeAzureEventhub string

const (
	ModeAzureEventhubError        ModeAzureEventhub = "error"
	ModeAzureEventhubBackpressure ModeAzureEventhub = "backpressure"
	ModeAzureEventhubAlways       ModeAzureEventhub = "always"
)

func (e ModeAzureEventhub) ToPointer() *ModeAzureEventhub {
	return &e
}
func (e *ModeAzureEventhub) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeAzureEventhub(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeAzureEventhub: %v", v)
	}
}

type PqControlsAzureEventhub struct {
}

type OutputAzureEventhub struct {
	// Unique ID for this output
	ID   *string            `json:"id,omitempty"`
	Type *TypeAzureEventhub `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// List of Event Hubs Kafka brokers to connect to, eg. yourdomain.servicebus.windows.net:9093. The hostname can be found in the host portion of the primary or secondary connection string in Shared Access Policies.
	Brokers []string `json:"brokers"`
	// The name of the Event Hub (a.k.a. Kafka Topic) to publish events. Can be overwritten using field __topicOut.
	Topic string `json:"topic"`
	// Control the number of required acknowledgments
	Ack *AcknowledgmentsAzureEventhub `default:"1" json:"ack"`
	// Format to use to serialize events before writing to the Event Hubs Kafka brokers.
	Format *RecordDataFormatAzureEventhub `default:"json" json:"format"`
	// Maximum size (KB) of each record batch before compression. Setting should be < message.max.bytes settings in Event Hubs brokers.
	MaxRecordSizeKB *float64 `default:"768" json:"maxRecordSizeKB"`
	// Maximum number of events in a batch before forcing a flush.
	FlushEventCount *float64 `default:"1000" json:"flushEventCount"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Maximum time to wait for a connection to complete successfully
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Maximum time to wait for Kafka to respond to a request
	RequestTimeout *float64 `default:"60000" json:"requestTimeout"`
	// If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
	MaxRetries *float64 `default:"5" json:"maxRetries"`
	// The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackOff *float64 `default:"30000" json:"maxBackOff"`
	// Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"300" json:"initialBackoff"`
	// Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// Maximum time to wait for Kafka to respond to an authentication request
	AuthenticationTimeout *float64 `default:"10000" json:"authenticationTimeout"`
	// Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backwards from the moment when credentials are set to expire.
	ReauthenticationThreshold *float64 `default:"10000" json:"reauthenticationThreshold"`
	// Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
	Sasl *AuthenticationAzureEventhub        `json:"sasl,omitempty"`
	TLS  *TLSSettingsClientSideAzureEventhub `json:"tls,omitempty"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorAzureEventhub `default:"block" json:"onBackpressure"`
	Description    *string                            `json:"description,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *CompressionAzureEventhub `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorAzureEventhub `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeAzureEventhub       `default:"error" json:"pqMode"`
	PqControls *PqControlsAzureEventhub `json:"pqControls,omitempty"`
	Status     *TFStatus                `json:"status,omitempty"`
}

func (o OutputAzureEventhub) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputAzureEventhub) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputAzureEventhub) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputAzureEventhub) GetType() *TypeAzureEventhub {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputAzureEventhub) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputAzureEventhub) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputAzureEventhub) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputAzureEventhub) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputAzureEventhub) GetBrokers() []string {
	if o == nil {
		return []string{}
	}
	return o.Brokers
}

func (o *OutputAzureEventhub) GetTopic() string {
	if o == nil {
		return ""
	}
	return o.Topic
}

func (o *OutputAzureEventhub) GetAck() *AcknowledgmentsAzureEventhub {
	if o == nil {
		return nil
	}
	return o.Ack
}

func (o *OutputAzureEventhub) GetFormat() *RecordDataFormatAzureEventhub {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputAzureEventhub) GetMaxRecordSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSizeKB
}

func (o *OutputAzureEventhub) GetFlushEventCount() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushEventCount
}

func (o *OutputAzureEventhub) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputAzureEventhub) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputAzureEventhub) GetRequestTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.RequestTimeout
}

func (o *OutputAzureEventhub) GetMaxRetries() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetries
}

func (o *OutputAzureEventhub) GetMaxBackOff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackOff
}

func (o *OutputAzureEventhub) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *OutputAzureEventhub) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *OutputAzureEventhub) GetAuthenticationTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.AuthenticationTimeout
}

func (o *OutputAzureEventhub) GetReauthenticationThreshold() *float64 {
	if o == nil {
		return nil
	}
	return o.ReauthenticationThreshold
}

func (o *OutputAzureEventhub) GetSasl() *AuthenticationAzureEventhub {
	if o == nil {
		return nil
	}
	return o.Sasl
}

func (o *OutputAzureEventhub) GetTLS() *TLSSettingsClientSideAzureEventhub {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputAzureEventhub) GetOnBackpressure() *BackpressureBehaviorAzureEventhub {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputAzureEventhub) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputAzureEventhub) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputAzureEventhub) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputAzureEventhub) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputAzureEventhub) GetPqCompress() *CompressionAzureEventhub {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputAzureEventhub) GetPqOnBackpressure() *QueueFullBehaviorAzureEventhub {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputAzureEventhub) GetPqMode() *ModeAzureEventhub {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputAzureEventhub) GetPqControls() *PqControlsAzureEventhub {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputAzureEventhub) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeHoneycomb string

const (
	TypeHoneycombHoneycomb TypeHoneycomb = "honeycomb"
)

func (e TypeHoneycomb) ToPointer() *TypeHoneycomb {
	return &e
}
func (e *TypeHoneycomb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "honeycomb":
		*e = TypeHoneycomb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeHoneycomb: %v", v)
	}
}

type ExtraHTTPHeaderHoneycomb struct {
	// Field name
	Name *string `json:"name,omitempty"`
	// Field value
	Value string `json:"value"`
}

func (o *ExtraHTTPHeaderHoneycomb) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderHoneycomb) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeHoneycomb - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeHoneycomb string

const (
	FailedRequestLoggingModeHoneycombPayload           FailedRequestLoggingModeHoneycomb = "payload"
	FailedRequestLoggingModeHoneycombPayloadAndHeaders FailedRequestLoggingModeHoneycomb = "payloadAndHeaders"
	FailedRequestLoggingModeHoneycombNone              FailedRequestLoggingModeHoneycomb = "none"
)

func (e FailedRequestLoggingModeHoneycomb) ToPointer() *FailedRequestLoggingModeHoneycomb {
	return &e
}
func (e *FailedRequestLoggingModeHoneycomb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeHoneycomb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeHoneycomb: %v", v)
	}
}

type ResponseRetrySettingHoneycomb struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingHoneycomb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingHoneycomb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingHoneycomb) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingHoneycomb) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingHoneycomb) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingHoneycomb) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsHoneycomb struct {
	// Enable to retry on request timeout
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsHoneycomb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsHoneycomb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsHoneycomb) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsHoneycomb) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsHoneycomb) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsHoneycomb) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorHoneycomb - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorHoneycomb string

const (
	BackpressureBehaviorHoneycombBlock BackpressureBehaviorHoneycomb = "block"
	BackpressureBehaviorHoneycombDrop  BackpressureBehaviorHoneycomb = "drop"
	BackpressureBehaviorHoneycombQueue BackpressureBehaviorHoneycomb = "queue"
)

func (e BackpressureBehaviorHoneycomb) ToPointer() *BackpressureBehaviorHoneycomb {
	return &e
}
func (e *BackpressureBehaviorHoneycomb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorHoneycomb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorHoneycomb: %v", v)
	}
}

// AuthenticationMethodHoneycomb - Enter API key directly, or select a stored secret
type AuthenticationMethodHoneycomb string

const (
	AuthenticationMethodHoneycombManual AuthenticationMethodHoneycomb = "manual"
	AuthenticationMethodHoneycombSecret AuthenticationMethodHoneycomb = "secret"
)

func (e AuthenticationMethodHoneycomb) ToPointer() *AuthenticationMethodHoneycomb {
	return &e
}
func (e *AuthenticationMethodHoneycomb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		*e = AuthenticationMethodHoneycomb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodHoneycomb: %v", v)
	}
}

// CompressionHoneycomb - Codec to use to compress the persisted data.
type CompressionHoneycomb string

const (
	CompressionHoneycombNone CompressionHoneycomb = "none"
	CompressionHoneycombGzip CompressionHoneycomb = "gzip"
)

func (e CompressionHoneycomb) ToPointer() *CompressionHoneycomb {
	return &e
}
func (e *CompressionHoneycomb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionHoneycomb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionHoneycomb: %v", v)
	}
}

// QueueFullBehaviorHoneycomb - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorHoneycomb string

const (
	QueueFullBehaviorHoneycombBlock QueueFullBehaviorHoneycomb = "block"
	QueueFullBehaviorHoneycombDrop  QueueFullBehaviorHoneycomb = "drop"
)

func (e QueueFullBehaviorHoneycomb) ToPointer() *QueueFullBehaviorHoneycomb {
	return &e
}
func (e *QueueFullBehaviorHoneycomb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorHoneycomb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorHoneycomb: %v", v)
	}
}

// ModeHoneycomb - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type ModeHoneycomb string

const (
	ModeHoneycombError        ModeHoneycomb = "error"
	ModeHoneycombBackpressure ModeHoneycomb = "backpressure"
	ModeHoneycombAlways       ModeHoneycomb = "always"
)

func (e ModeHoneycomb) ToPointer() *ModeHoneycomb {
	return &e
}
func (e *ModeHoneycomb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeHoneycomb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeHoneycomb: %v", v)
	}
}

type PqControlsHoneycomb struct {
}

type OutputHoneycomb struct {
	// Unique ID for this output
	ID   *string       `json:"id,omitempty"`
	Type TypeHoneycomb `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Name of the dataset to send events to  e.g., observability
	Dataset string `json:"dataset"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Defaults to Yes. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max body size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events.
	ExtraHTTPHeaders []ExtraHTTPHeaderHoneycomb `json:"extraHttpHeaders,omitempty"`
	// Enables round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeHoneycomb `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable).
	ResponseRetrySettings []ResponseRetrySettingHoneycomb `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsHoneycomb  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorHoneycomb `default:"block" json:"onBackpressure"`
	// Enter API key directly, or select a stored secret
	AuthType    *AuthenticationMethodHoneycomb `default:"manual" json:"authType"`
	Description *string                        `json:"description,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *CompressionHoneycomb `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorHoneycomb `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeHoneycomb       `default:"error" json:"pqMode"`
	PqControls *PqControlsHoneycomb `json:"pqControls,omitempty"`
	// Team API key where the dataset belongs
	Team *string `json:"team,omitempty"`
	// Select or create a stored text secret
	TextSecret *string   `json:"textSecret,omitempty"`
	Status     *TFStatus `json:"status,omitempty"`
}

func (o OutputHoneycomb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputHoneycomb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputHoneycomb) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputHoneycomb) GetType() TypeHoneycomb {
	if o == nil {
		return TypeHoneycomb("")
	}
	return o.Type
}

func (o *OutputHoneycomb) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputHoneycomb) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputHoneycomb) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputHoneycomb) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputHoneycomb) GetDataset() string {
	if o == nil {
		return ""
	}
	return o.Dataset
}

func (o *OutputHoneycomb) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputHoneycomb) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputHoneycomb) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputHoneycomb) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputHoneycomb) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputHoneycomb) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputHoneycomb) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputHoneycomb) GetExtraHTTPHeaders() []ExtraHTTPHeaderHoneycomb {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputHoneycomb) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputHoneycomb) GetFailedRequestLoggingMode() *FailedRequestLoggingModeHoneycomb {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputHoneycomb) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputHoneycomb) GetResponseRetrySettings() []ResponseRetrySettingHoneycomb {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputHoneycomb) GetTimeoutRetrySettings() *TimeoutRetrySettingsHoneycomb {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputHoneycomb) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputHoneycomb) GetOnBackpressure() *BackpressureBehaviorHoneycomb {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputHoneycomb) GetAuthType() *AuthenticationMethodHoneycomb {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputHoneycomb) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputHoneycomb) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputHoneycomb) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputHoneycomb) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputHoneycomb) GetPqCompress() *CompressionHoneycomb {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputHoneycomb) GetPqOnBackpressure() *QueueFullBehaviorHoneycomb {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputHoneycomb) GetPqMode() *ModeHoneycomb {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputHoneycomb) GetPqControls() *PqControlsHoneycomb {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputHoneycomb) GetTeam() *string {
	if o == nil {
		return nil
	}
	return o.Team
}

func (o *OutputHoneycomb) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputHoneycomb) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type OutputTypeKinesis string

const (
	OutputTypeKinesisKinesis OutputTypeKinesis = "kinesis"
)

func (e OutputTypeKinesis) ToPointer() *OutputTypeKinesis {
	return &e
}
func (e *OutputTypeKinesis) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "kinesis":
		*e = OutputTypeKinesis(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeKinesis: %v", v)
	}
}

// OutputAuthenticationMethodKinesis - AWS authentication method. Choose Auto to use IAM roles.
type OutputAuthenticationMethodKinesis string

const (
	OutputAuthenticationMethodKinesisAuto   OutputAuthenticationMethodKinesis = "auto"
	OutputAuthenticationMethodKinesisManual OutputAuthenticationMethodKinesis = "manual"
	OutputAuthenticationMethodKinesisSecret OutputAuthenticationMethodKinesis = "secret"
)

func (e OutputAuthenticationMethodKinesis) ToPointer() *OutputAuthenticationMethodKinesis {
	return &e
}
func (e *OutputAuthenticationMethodKinesis) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "auto":
		fallthrough
	case "manual":
		fallthrough
	case "secret":
		*e = OutputAuthenticationMethodKinesis(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputAuthenticationMethodKinesis: %v", v)
	}
}

// OutputSignatureVersionKinesis - Signature version to use for signing Kinesis stream requests
type OutputSignatureVersionKinesis string

const (
	OutputSignatureVersionKinesisV2 OutputSignatureVersionKinesis = "v2"
	OutputSignatureVersionKinesisV4 OutputSignatureVersionKinesis = "v4"
)

func (e OutputSignatureVersionKinesis) ToPointer() *OutputSignatureVersionKinesis {
	return &e
}
func (e *OutputSignatureVersionKinesis) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "v2":
		fallthrough
	case "v4":
		*e = OutputSignatureVersionKinesis(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputSignatureVersionKinesis: %v", v)
	}
}

// OutputCompressionKinesis - Compression type to use for records
type OutputCompressionKinesis string

const (
	OutputCompressionKinesisNone OutputCompressionKinesis = "none"
	OutputCompressionKinesisGzip OutputCompressionKinesis = "gzip"
)

func (e OutputCompressionKinesis) ToPointer() *OutputCompressionKinesis {
	return &e
}
func (e *OutputCompressionKinesis) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = OutputCompressionKinesis(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputCompressionKinesis: %v", v)
	}
}

// BackpressureBehaviorKinesis - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorKinesis string

const (
	BackpressureBehaviorKinesisBlock BackpressureBehaviorKinesis = "block"
	BackpressureBehaviorKinesisDrop  BackpressureBehaviorKinesis = "drop"
	BackpressureBehaviorKinesisQueue BackpressureBehaviorKinesis = "queue"
)

func (e BackpressureBehaviorKinesis) ToPointer() *BackpressureBehaviorKinesis {
	return &e
}
func (e *BackpressureBehaviorKinesis) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorKinesis(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorKinesis: %v", v)
	}
}

// PqCompressCompressionKinesis - Codec to use to compress the persisted data.
type PqCompressCompressionKinesis string

const (
	PqCompressCompressionKinesisNone PqCompressCompressionKinesis = "none"
	PqCompressCompressionKinesisGzip PqCompressCompressionKinesis = "gzip"
)

func (e PqCompressCompressionKinesis) ToPointer() *PqCompressCompressionKinesis {
	return &e
}
func (e *PqCompressCompressionKinesis) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = PqCompressCompressionKinesis(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PqCompressCompressionKinesis: %v", v)
	}
}

// QueueFullBehaviorKinesis - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorKinesis string

const (
	QueueFullBehaviorKinesisBlock QueueFullBehaviorKinesis = "block"
	QueueFullBehaviorKinesisDrop  QueueFullBehaviorKinesis = "drop"
)

func (e QueueFullBehaviorKinesis) ToPointer() *QueueFullBehaviorKinesis {
	return &e
}
func (e *QueueFullBehaviorKinesis) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorKinesis(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorKinesis: %v", v)
	}
}

// OutputModeKinesis - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type OutputModeKinesis string

const (
	OutputModeKinesisError        OutputModeKinesis = "error"
	OutputModeKinesisBackpressure OutputModeKinesis = "backpressure"
	OutputModeKinesisAlways       OutputModeKinesis = "always"
)

func (e OutputModeKinesis) ToPointer() *OutputModeKinesis {
	return &e
}
func (e *OutputModeKinesis) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = OutputModeKinesis(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputModeKinesis: %v", v)
	}
}

type PqControlsKinesis struct {
}

type OutputKinesis struct {
	// Unique ID for this output
	ID   *string            `json:"id,omitempty"`
	Type *OutputTypeKinesis `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Kinesis stream name to send events to.
	StreamName string `json:"streamName"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *OutputAuthenticationMethodKinesis `default:"auto" json:"awsAuthenticationMethod"`
	AwsSecretKey            *string                            `json:"awsSecretKey,omitempty"`
	// Region where the Kinesis stream is located
	Region string `json:"region"`
	// Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint.
	Endpoint *string `json:"endpoint,omitempty"`
	// Signature version to use for signing Kinesis stream requests
	SignatureVersion *OutputSignatureVersionKinesis `default:"v4" json:"signatureVersion"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Use Assume Role credentials to access Kinesis stream
	EnableAssumeRole *bool `default:"false" json:"enableAssumeRole"`
	// Amazon Resource Name (ARN) of the role to assume
	AssumeRoleArn *string `json:"assumeRoleArn,omitempty"`
	// External ID to use when assuming role
	AssumeRoleExternalID *string `json:"assumeRoleExternalId,omitempty"`
	// Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
	DurationSeconds *float64 `default:"3600" json:"durationSeconds"`
	// Maximum number of ongoing put requests before blocking.
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size (KB) of each individual record before compression. For uncompressed or non-compressible data 1MB is the max recommended size
	MaxRecordSizeKB *float64 `default:"1024" json:"maxRecordSizeKB"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Compression type to use for records
	Compression *OutputCompressionKinesis `default:"gzip" json:"compression"`
	// Provides higher stream rate limits, improving delivery speed and reliability by minimizing throttling. See the [ListShards API](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html) documentation for details.
	UseListShards *bool `default:"false" json:"useListShards"`
	// Batch events into a single record as NDJSON
	AsNdjson *bool `default:"true" json:"asNdjson"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorKinesis `default:"block" json:"onBackpressure"`
	Description    *string                      `json:"description,omitempty"`
	AwsAPIKey      *string                      `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *PqCompressCompressionKinesis `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorKinesis `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *OutputModeKinesis `default:"error" json:"pqMode"`
	PqControls *PqControlsKinesis `json:"pqControls,omitempty"`
	Status     *TFStatus          `json:"status,omitempty"`
}

func (o OutputKinesis) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputKinesis) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputKinesis) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputKinesis) GetType() *OutputTypeKinesis {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputKinesis) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputKinesis) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputKinesis) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputKinesis) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputKinesis) GetStreamName() string {
	if o == nil {
		return ""
	}
	return o.StreamName
}

func (o *OutputKinesis) GetAwsAuthenticationMethod() *OutputAuthenticationMethodKinesis {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputKinesis) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputKinesis) GetRegion() string {
	if o == nil {
		return ""
	}
	return o.Region
}

func (o *OutputKinesis) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputKinesis) GetSignatureVersion() *OutputSignatureVersionKinesis {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputKinesis) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputKinesis) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputKinesis) GetEnableAssumeRole() *bool {
	if o == nil {
		return nil
	}
	return o.EnableAssumeRole
}

func (o *OutputKinesis) GetAssumeRoleArn() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleArn
}

func (o *OutputKinesis) GetAssumeRoleExternalID() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleExternalID
}

func (o *OutputKinesis) GetDurationSeconds() *float64 {
	if o == nil {
		return nil
	}
	return o.DurationSeconds
}

func (o *OutputKinesis) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputKinesis) GetMaxRecordSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSizeKB
}

func (o *OutputKinesis) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputKinesis) GetCompression() *OutputCompressionKinesis {
	if o == nil {
		return nil
	}
	return o.Compression
}

func (o *OutputKinesis) GetUseListShards() *bool {
	if o == nil {
		return nil
	}
	return o.UseListShards
}

func (o *OutputKinesis) GetAsNdjson() *bool {
	if o == nil {
		return nil
	}
	return o.AsNdjson
}

func (o *OutputKinesis) GetOnBackpressure() *BackpressureBehaviorKinesis {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputKinesis) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputKinesis) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputKinesis) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputKinesis) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputKinesis) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputKinesis) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputKinesis) GetPqCompress() *PqCompressCompressionKinesis {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputKinesis) GetPqOnBackpressure() *QueueFullBehaviorKinesis {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputKinesis) GetPqMode() *OutputModeKinesis {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputKinesis) GetPqControls() *PqControlsKinesis {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputKinesis) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeAzureLogs string

const (
	TypeAzureLogsAzureLogs TypeAzureLogs = "azure_logs"
)

func (e TypeAzureLogs) ToPointer() *TypeAzureLogs {
	return &e
}
func (e *TypeAzureLogs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "azure_logs":
		*e = TypeAzureLogs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeAzureLogs: %v", v)
	}
}

type ExtraHTTPHeaderAzureLogs struct {
	// Field name
	Name *string `json:"name,omitempty"`
	// Field value
	Value string `json:"value"`
}

func (o *ExtraHTTPHeaderAzureLogs) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderAzureLogs) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeAzureLogs - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeAzureLogs string

const (
	FailedRequestLoggingModeAzureLogsPayload           FailedRequestLoggingModeAzureLogs = "payload"
	FailedRequestLoggingModeAzureLogsPayloadAndHeaders FailedRequestLoggingModeAzureLogs = "payloadAndHeaders"
	FailedRequestLoggingModeAzureLogsNone              FailedRequestLoggingModeAzureLogs = "none"
)

func (e FailedRequestLoggingModeAzureLogs) ToPointer() *FailedRequestLoggingModeAzureLogs {
	return &e
}
func (e *FailedRequestLoggingModeAzureLogs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeAzureLogs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeAzureLogs: %v", v)
	}
}

type ResponseRetrySettingAzureLogs struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingAzureLogs) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingAzureLogs) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingAzureLogs) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingAzureLogs) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingAzureLogs) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingAzureLogs) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsAzureLogs struct {
	// Enable to retry on request timeout
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsAzureLogs) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsAzureLogs) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsAzureLogs) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsAzureLogs) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsAzureLogs) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsAzureLogs) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorAzureLogs - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorAzureLogs string

const (
	BackpressureBehaviorAzureLogsBlock BackpressureBehaviorAzureLogs = "block"
	BackpressureBehaviorAzureLogsDrop  BackpressureBehaviorAzureLogs = "drop"
	BackpressureBehaviorAzureLogsQueue BackpressureBehaviorAzureLogs = "queue"
)

func (e BackpressureBehaviorAzureLogs) ToPointer() *BackpressureBehaviorAzureLogs {
	return &e
}
func (e *BackpressureBehaviorAzureLogs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorAzureLogs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorAzureLogs: %v", v)
	}
}

// AuthenticationMethodAzureLogs - Enter workspace ID and workspace key directly, or select a stored secret
type AuthenticationMethodAzureLogs string

const (
	AuthenticationMethodAzureLogsManual AuthenticationMethodAzureLogs = "manual"
	AuthenticationMethodAzureLogsSecret AuthenticationMethodAzureLogs = "secret"
)

func (e AuthenticationMethodAzureLogs) ToPointer() *AuthenticationMethodAzureLogs {
	return &e
}
func (e *AuthenticationMethodAzureLogs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		*e = AuthenticationMethodAzureLogs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodAzureLogs: %v", v)
	}
}

// CompressionAzureLogs - Codec to use to compress the persisted data.
type CompressionAzureLogs string

const (
	CompressionAzureLogsNone CompressionAzureLogs = "none"
	CompressionAzureLogsGzip CompressionAzureLogs = "gzip"
)

func (e CompressionAzureLogs) ToPointer() *CompressionAzureLogs {
	return &e
}
func (e *CompressionAzureLogs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionAzureLogs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionAzureLogs: %v", v)
	}
}

// QueueFullBehaviorAzureLogs - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorAzureLogs string

const (
	QueueFullBehaviorAzureLogsBlock QueueFullBehaviorAzureLogs = "block"
	QueueFullBehaviorAzureLogsDrop  QueueFullBehaviorAzureLogs = "drop"
)

func (e QueueFullBehaviorAzureLogs) ToPointer() *QueueFullBehaviorAzureLogs {
	return &e
}
func (e *QueueFullBehaviorAzureLogs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorAzureLogs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorAzureLogs: %v", v)
	}
}

// ModeAzureLogs - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type ModeAzureLogs string

const (
	ModeAzureLogsError        ModeAzureLogs = "error"
	ModeAzureLogsBackpressure ModeAzureLogs = "backpressure"
	ModeAzureLogsAlways       ModeAzureLogs = "always"
)

func (e ModeAzureLogs) ToPointer() *ModeAzureLogs {
	return &e
}
func (e *ModeAzureLogs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeAzureLogs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeAzureLogs: %v", v)
	}
}

type PqControlsAzureLogs struct {
}

type OutputAzureLogs struct {
	// Unique ID for this output
	ID   *string       `json:"id,omitempty"`
	Type TypeAzureLogs `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The Log Type of events sent to this LogAnalytics workspace. Defaults to `Cribl`. Use only letters, numbers, and `_` characters, and can't exceed 100 characters. Can be overwritten by event field __logType.
	LogType *string `default:"Cribl" json:"logType"`
	// Optional Resource ID of the Azure resource to associate the data with. Can be overridden by the __resourceId event field. This ID populates the _ResourceId property, allowing the data to be included in resource-centric queries. If the ID is neither specified nor overridden, resource-centric queries will omit the data.
	ResourceID *string `json:"resourceId,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"1024" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	Compress         *bool    `json:"compress,omitempty"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Defaults to Yes. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max body size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events.
	ExtraHTTPHeaders []ExtraHTTPHeaderAzureLogs `json:"extraHttpHeaders,omitempty"`
	// Enables round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeAzureLogs `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Enter the DNS name of the Log API endpoint that sends log data to a Log Analytics workspace in Azure Monitor. Defaults to .ods.opinsights.azure.com. @{product} will add a prefix and suffix around this DNS name to construct a URI in this format: <https://<Workspace_ID><your_DNS_name>/api/logs?api-version=<API version>.
	APIURL *string `default:".ods.opinsights.azure.com" json:"apiUrl"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable).
	ResponseRetrySettings []ResponseRetrySettingAzureLogs `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsAzureLogs  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorAzureLogs `default:"block" json:"onBackpressure"`
	// Enter workspace ID and workspace key directly, or select a stored secret
	AuthType    *AuthenticationMethodAzureLogs `default:"manual" json:"authType"`
	Description *string                        `json:"description,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *CompressionAzureLogs `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorAzureLogs `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeAzureLogs       `default:"error" json:"pqMode"`
	PqControls *PqControlsAzureLogs `json:"pqControls,omitempty"`
	// Azure Log Analytics Workspace ID. See Azure Dashboard Workspace > Advanced settings.
	WorkspaceID *string `json:"workspaceId,omitempty"`
	// Azure Log Analytics Workspace Primary or Secondary Shared Key. See Azure Dashboard Workspace > Advanced settings.
	WorkspaceKey *string `json:"workspaceKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	KeypairSecret *string   `json:"keypairSecret,omitempty"`
	Status        *TFStatus `json:"status,omitempty"`
}

func (o OutputAzureLogs) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputAzureLogs) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputAzureLogs) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputAzureLogs) GetType() TypeAzureLogs {
	if o == nil {
		return TypeAzureLogs("")
	}
	return o.Type
}

func (o *OutputAzureLogs) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputAzureLogs) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputAzureLogs) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputAzureLogs) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputAzureLogs) GetLogType() *string {
	if o == nil {
		return nil
	}
	return o.LogType
}

func (o *OutputAzureLogs) GetResourceID() *string {
	if o == nil {
		return nil
	}
	return o.ResourceID
}

func (o *OutputAzureLogs) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputAzureLogs) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputAzureLogs) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputAzureLogs) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputAzureLogs) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputAzureLogs) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputAzureLogs) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputAzureLogs) GetExtraHTTPHeaders() []ExtraHTTPHeaderAzureLogs {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputAzureLogs) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputAzureLogs) GetFailedRequestLoggingMode() *FailedRequestLoggingModeAzureLogs {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputAzureLogs) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputAzureLogs) GetAPIURL() *string {
	if o == nil {
		return nil
	}
	return o.APIURL
}

func (o *OutputAzureLogs) GetResponseRetrySettings() []ResponseRetrySettingAzureLogs {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputAzureLogs) GetTimeoutRetrySettings() *TimeoutRetrySettingsAzureLogs {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputAzureLogs) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputAzureLogs) GetOnBackpressure() *BackpressureBehaviorAzureLogs {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputAzureLogs) GetAuthType() *AuthenticationMethodAzureLogs {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputAzureLogs) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputAzureLogs) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputAzureLogs) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputAzureLogs) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputAzureLogs) GetPqCompress() *CompressionAzureLogs {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputAzureLogs) GetPqOnBackpressure() *QueueFullBehaviorAzureLogs {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputAzureLogs) GetPqMode() *ModeAzureLogs {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputAzureLogs) GetPqControls() *PqControlsAzureLogs {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputAzureLogs) GetWorkspaceID() *string {
	if o == nil {
		return nil
	}
	return o.WorkspaceID
}

func (o *OutputAzureLogs) GetWorkspaceKey() *string {
	if o == nil {
		return nil
	}
	return o.WorkspaceKey
}

func (o *OutputAzureLogs) GetKeypairSecret() *string {
	if o == nil {
		return nil
	}
	return o.KeypairSecret
}

func (o *OutputAzureLogs) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeAzureDataExplorer string

const (
	TypeAzureDataExplorerAzureDataExplorer TypeAzureDataExplorer = "azure_data_explorer"
)

func (e TypeAzureDataExplorer) ToPointer() *TypeAzureDataExplorer {
	return &e
}
func (e *TypeAzureDataExplorer) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "azure_data_explorer":
		*e = TypeAzureDataExplorer(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeAzureDataExplorer: %v", v)
	}
}

// IngestionMode - Method to use for ingesting data.
type IngestionMode string

const (
	IngestionModeBatching  IngestionMode = "batching"
	IngestionModeStreaming IngestionMode = "streaming"
)

func (e IngestionMode) ToPointer() *IngestionMode {
	return &e
}
func (e *IngestionMode) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "batching":
		fallthrough
	case "streaming":
		*e = IngestionMode(v)
		return nil
	default:
		return fmt.Errorf("invalid value for IngestionMode: %v", v)
	}
}

// AzureADAuthenticationEndpoint - Endpoint used to acquire authentication tokens from Azure.
type AzureADAuthenticationEndpoint string

const (
	AzureADAuthenticationEndpointHTTPSLoginMicrosoftonlineCom       AzureADAuthenticationEndpoint = "https://login.microsoftonline.com"
	AzureADAuthenticationEndpointHTTPSLoginMicrosoftonlineUs        AzureADAuthenticationEndpoint = "https://login.microsoftonline.us"
	AzureADAuthenticationEndpointHTTPSLoginPartnerMicrosoftonlineCn AzureADAuthenticationEndpoint = "https://login.partner.microsoftonline.cn"
)

func (e AzureADAuthenticationEndpoint) ToPointer() *AzureADAuthenticationEndpoint {
	return &e
}
func (e *AzureADAuthenticationEndpoint) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "https://login.microsoftonline.com":
		fallthrough
	case "https://login.microsoftonline.us":
		fallthrough
	case "https://login.partner.microsoftonline.cn":
		*e = AzureADAuthenticationEndpoint(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AzureADAuthenticationEndpoint: %v", v)
	}
}

// OauthTypeAuthenticationMethod - The type of OAuth 2.0 client credentials grant flow to use.
type OauthTypeAuthenticationMethod string

const (
	OauthTypeAuthenticationMethodClientSecret     OauthTypeAuthenticationMethod = "clientSecret"
	OauthTypeAuthenticationMethodClientTextSecret OauthTypeAuthenticationMethod = "clientTextSecret"
	OauthTypeAuthenticationMethodCertificate      OauthTypeAuthenticationMethod = "certificate"
)

func (e OauthTypeAuthenticationMethod) ToPointer() *OauthTypeAuthenticationMethod {
	return &e
}
func (e *OauthTypeAuthenticationMethod) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "clientSecret":
		fallthrough
	case "clientTextSecret":
		fallthrough
	case "certificate":
		*e = OauthTypeAuthenticationMethod(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OauthTypeAuthenticationMethod: %v", v)
	}
}

type CertificateAzureDataExplorer struct {
	// The certificate you registered as credentials for your app in the Azure portal.
	CertificateName *string `json:"certificateName,omitempty"`
}

func (o *CertificateAzureDataExplorer) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

// BackpressureBehaviorAzureDataExplorer - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorAzureDataExplorer string

const (
	BackpressureBehaviorAzureDataExplorerBlock BackpressureBehaviorAzureDataExplorer = "block"
	BackpressureBehaviorAzureDataExplorerDrop  BackpressureBehaviorAzureDataExplorer = "drop"
	BackpressureBehaviorAzureDataExplorerQueue BackpressureBehaviorAzureDataExplorer = "queue"
)

func (e BackpressureBehaviorAzureDataExplorer) ToPointer() *BackpressureBehaviorAzureDataExplorer {
	return &e
}
func (e *BackpressureBehaviorAzureDataExplorer) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorAzureDataExplorer(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorAzureDataExplorer: %v", v)
	}
}

// DataFormatAzureDataExplorer - Format of the output data
type DataFormatAzureDataExplorer string

const (
	DataFormatAzureDataExplorerJSON    DataFormatAzureDataExplorer = "json"
	DataFormatAzureDataExplorerRaw     DataFormatAzureDataExplorer = "raw"
	DataFormatAzureDataExplorerParquet DataFormatAzureDataExplorer = "parquet"
)

func (e DataFormatAzureDataExplorer) ToPointer() *DataFormatAzureDataExplorer {
	return &e
}
func (e *DataFormatAzureDataExplorer) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "json":
		fallthrough
	case "raw":
		fallthrough
	case "parquet":
		*e = DataFormatAzureDataExplorer(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DataFormatAzureDataExplorer: %v", v)
	}
}

// DiskSpaceProtectionAzureDataExplorer - Whether to block or drop events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionAzureDataExplorer string

const (
	DiskSpaceProtectionAzureDataExplorerBlock DiskSpaceProtectionAzureDataExplorer = "block"
	DiskSpaceProtectionAzureDataExplorerDrop  DiskSpaceProtectionAzureDataExplorer = "drop"
)

func (e DiskSpaceProtectionAzureDataExplorer) ToPointer() *DiskSpaceProtectionAzureDataExplorer {
	return &e
}
func (e *DiskSpaceProtectionAzureDataExplorer) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = DiskSpaceProtectionAzureDataExplorer(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DiskSpaceProtectionAzureDataExplorer: %v", v)
	}
}

type PrefixOptional string

const (
	PrefixOptionalDropBy   PrefixOptional = "dropBy"
	PrefixOptionalIngestBy PrefixOptional = "ingestBy"
)

func (e PrefixOptional) ToPointer() *PrefixOptional {
	return &e
}
func (e *PrefixOptional) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "dropBy":
		fallthrough
	case "ingestBy":
		*e = PrefixOptional(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PrefixOptional: %v", v)
	}
}

type ExtentTag struct {
	Prefix *PrefixOptional `json:"prefix,omitempty"`
	Value  string          `json:"value"`
}

func (o *ExtentTag) GetPrefix() *PrefixOptional {
	if o == nil {
		return nil
	}
	return o.Prefix
}

func (o *ExtentTag) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type IngestIfNotExist struct {
	Value string `json:"value"`
}

func (o *IngestIfNotExist) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// ReportLevel - Level of ingestion status reporting. Defaults to FailuresOnly.
type ReportLevel string

const (
	ReportLevelFailuresOnly         ReportLevel = "failuresOnly"
	ReportLevelDoNotReport          ReportLevel = "doNotReport"
	ReportLevelFailuresAndSuccesses ReportLevel = "failuresAndSuccesses"
)

func (e ReportLevel) ToPointer() *ReportLevel {
	return &e
}
func (e *ReportLevel) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "failuresOnly":
		fallthrough
	case "doNotReport":
		fallthrough
	case "failuresAndSuccesses":
		*e = ReportLevel(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ReportLevel: %v", v)
	}
}

// ReportMethod - Target of the ingestion status reporting. Defaults to Queue.
type ReportMethod string

const (
	ReportMethodQueue         ReportMethod = "queue"
	ReportMethodTable         ReportMethod = "table"
	ReportMethodQueueAndTable ReportMethod = "queueAndTable"
)

func (e ReportMethod) ToPointer() *ReportMethod {
	return &e
}
func (e *ReportMethod) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "queue":
		fallthrough
	case "table":
		fallthrough
	case "queueAndTable":
		*e = ReportMethod(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ReportMethod: %v", v)
	}
}

type AdditionalProperty struct {
	Key   string `json:"key"`
	Value string `json:"value"`
}

func (o *AdditionalProperty) GetKey() string {
	if o == nil {
		return ""
	}
	return o.Key
}

func (o *AdditionalProperty) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type ResponseRetrySettingAzureDataExplorer struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingAzureDataExplorer) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingAzureDataExplorer) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingAzureDataExplorer) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingAzureDataExplorer) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingAzureDataExplorer) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingAzureDataExplorer) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsAzureDataExplorer struct {
	// Enable to retry on request timeout
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsAzureDataExplorer) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsAzureDataExplorer) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsAzureDataExplorer) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsAzureDataExplorer) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsAzureDataExplorer) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsAzureDataExplorer) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// CompressAzureDataExplorer - Choose data compression format to apply to HTTP content before it is delivered.
type CompressAzureDataExplorer string

const (
	CompressAzureDataExplorerNone CompressAzureDataExplorer = "none"
	CompressAzureDataExplorerGzip CompressAzureDataExplorer = "gzip"
)

func (e CompressAzureDataExplorer) ToPointer() *CompressAzureDataExplorer {
	return &e
}
func (e *CompressAzureDataExplorer) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressAzureDataExplorer(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressAzureDataExplorer: %v", v)
	}
}

// CompressionAzureDataExplorer - Codec to use to compress the persisted data.
type CompressionAzureDataExplorer string

const (
	CompressionAzureDataExplorerNone CompressionAzureDataExplorer = "none"
	CompressionAzureDataExplorerGzip CompressionAzureDataExplorer = "gzip"
)

func (e CompressionAzureDataExplorer) ToPointer() *CompressionAzureDataExplorer {
	return &e
}
func (e *CompressionAzureDataExplorer) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionAzureDataExplorer(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionAzureDataExplorer: %v", v)
	}
}

// QueueFullBehaviorAzureDataExplorer - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorAzureDataExplorer string

const (
	QueueFullBehaviorAzureDataExplorerBlock QueueFullBehaviorAzureDataExplorer = "block"
	QueueFullBehaviorAzureDataExplorerDrop  QueueFullBehaviorAzureDataExplorer = "drop"
)

func (e QueueFullBehaviorAzureDataExplorer) ToPointer() *QueueFullBehaviorAzureDataExplorer {
	return &e
}
func (e *QueueFullBehaviorAzureDataExplorer) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorAzureDataExplorer(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorAzureDataExplorer: %v", v)
	}
}

// ModeAzureDataExplorer - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type ModeAzureDataExplorer string

const (
	ModeAzureDataExplorerError        ModeAzureDataExplorer = "error"
	ModeAzureDataExplorerBackpressure ModeAzureDataExplorer = "backpressure"
	ModeAzureDataExplorerAlways       ModeAzureDataExplorer = "always"
)

func (e ModeAzureDataExplorer) ToPointer() *ModeAzureDataExplorer {
	return &e
}
func (e *ModeAzureDataExplorer) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeAzureDataExplorer(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeAzureDataExplorer: %v", v)
	}
}

type PqControlsAzureDataExplorer struct {
}

type OutputAzureDataExplorer struct {
	// Unique ID for this output
	ID   *string                `json:"id,omitempty"`
	Type *TypeAzureDataExplorer `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The base URI for your cluster. Typically, `https://<cluster>.<region>.kusto.windows.net`.
	ClusterURL string `json:"clusterUrl"`
	// Name of the database containing the table where data will be ingested.
	Database string `json:"database"`
	// Name of the table to ingest data into.
	Table string `json:"table"`
	// When you save or start the Destination, validates database name and credentials; also validates table name except when creating a new table. Disable if your Azure app does not have both the Database Viewer and the Table Viewer role.
	ValidateDatabaseSettings *bool `default:"true" json:"validateDatabaseSettings"`
	// Method to use for ingesting data.
	IngestMode *IngestionMode `default:"batching" json:"ingestMode"`
	// Endpoint used to acquire authentication tokens from Azure.
	OauthEndpoint *AzureADAuthenticationEndpoint `default:"https://login.microsoftonline.com" json:"oauthEndpoint"`
	// Directory ID (tenant identifier) in Azure Active Directory.
	TenantID string `json:"tenantId"`
	// client_id to pass in the OAuth request parameter.
	ClientID string `json:"clientId"`
	// Scope to pass in the OAuth request parameter.
	Scope string `json:"scope"`
	// The type of OAuth 2.0 client credentials grant flow to use.
	OauthType   *OauthTypeAuthenticationMethod `default:"clientSecret" json:"oauthType"`
	Description *string                        `json:"description,omitempty"`
	// The client secret that you generated for your app in the Azure portal.
	ClientSecret *string `json:"clientSecret,omitempty"`
	// Select or create a stored text secret
	TextSecret  *string                       `json:"textSecret,omitempty"`
	Certificate *CertificateAzureDataExplorer `json:"certificate,omitempty"`
	// The ingestion service URI for your cluster. Typically, `https://ingest-<cluster>.<region>.kusto.windows.net`.
	IngestURL *string `json:"ingestUrl,omitempty"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorAzureDataExplorer `default:"block" json:"onBackpressure"`
	// Enable if you want to send a (JSON) mapping object instead of specifying an existing named data mapping.
	IsMappingObj *bool `default:"false" json:"isMappingObj"`
	// Format of the output data
	Format *DataFormatAzureDataExplorer `default:"json" json:"format"`
	// Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant stable storage.
	StagePath *string `default:"\\$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// Maximum number of parts to upload in parallel per file.
	MaxConcurrentFileParts *float64 `default:"1" json:"maxConcurrentFileParts"`
	// Whether to block or drop events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionAzureDataExplorer `default:"block" json:"onDiskFullBackpressure"`
	// Append output's ID to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// If a file fails to move to its final destination after the maximum number of retries, dead-letter it to prevent further errors.
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Enable to bypass the data management service's aggregation mechanism.
	FlushImmediately *bool `default:"false" json:"flushImmediately"`
	// Enable to prevent blob deletion after ingestion is complete.
	RetainBlobOnSuccess *bool `default:"false" json:"retainBlobOnSuccess"`
	// Strings or tags associated with the extent (ingested data shard).
	ExtentTags []ExtentTag `json:"extentTags,omitempty"`
	// Prevents duplicate ingestion by checking if an extent with the specified ingest-by tag already exists.
	IngestIfNotExists []IngestIfNotExist `json:"ingestIfNotExists,omitempty"`
	// Level of ingestion status reporting. Defaults to FailuresOnly.
	ReportLevel *ReportLevel `default:"failuresOnly" json:"reportLevel"`
	// Target of the ingestion status reporting. Defaults to Queue.
	ReportMethod *ReportMethod `default:"queue" json:"reportMethod"`
	// Optionally, enter additional configuration properties to send to the ingestion service.
	AdditionalProperties []AdditionalProperty `json:"additionalProperties,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable).
	ResponseRetrySettings []ResponseRetrySettingAzureDataExplorer `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsAzureDataExplorer  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// Choose data compression format to apply to HTTP content before it is delivered.
	Compress *CompressAzureDataExplorer `default:"gzip" json:"compress"`
	// Enter the name of a data mapping associated with your target table. Or, if incoming event and target table fields match exactly, you can leave the field empty.
	MappingRef *string `json:"mappingRef,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max body size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Defaults to Yes. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Enables round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Disable to close the connection immediately after sending the outgoing request.
	KeepAlive *bool `default:"true" json:"keepAlive"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *CompressionAzureDataExplorer `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorAzureDataExplorer `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeAzureDataExplorer       `default:"error" json:"pqMode"`
	PqControls *PqControlsAzureDataExplorer `json:"pqControls,omitempty"`
	Status     *TFStatus                    `json:"status,omitempty"`
}

func (o OutputAzureDataExplorer) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputAzureDataExplorer) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputAzureDataExplorer) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputAzureDataExplorer) GetType() *TypeAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputAzureDataExplorer) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputAzureDataExplorer) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputAzureDataExplorer) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputAzureDataExplorer) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputAzureDataExplorer) GetClusterURL() string {
	if o == nil {
		return ""
	}
	return o.ClusterURL
}

func (o *OutputAzureDataExplorer) GetDatabase() string {
	if o == nil {
		return ""
	}
	return o.Database
}

func (o *OutputAzureDataExplorer) GetTable() string {
	if o == nil {
		return ""
	}
	return o.Table
}

func (o *OutputAzureDataExplorer) GetValidateDatabaseSettings() *bool {
	if o == nil {
		return nil
	}
	return o.ValidateDatabaseSettings
}

func (o *OutputAzureDataExplorer) GetIngestMode() *IngestionMode {
	if o == nil {
		return nil
	}
	return o.IngestMode
}

func (o *OutputAzureDataExplorer) GetOauthEndpoint() *AzureADAuthenticationEndpoint {
	if o == nil {
		return nil
	}
	return o.OauthEndpoint
}

func (o *OutputAzureDataExplorer) GetTenantID() string {
	if o == nil {
		return ""
	}
	return o.TenantID
}

func (o *OutputAzureDataExplorer) GetClientID() string {
	if o == nil {
		return ""
	}
	return o.ClientID
}

func (o *OutputAzureDataExplorer) GetScope() string {
	if o == nil {
		return ""
	}
	return o.Scope
}

func (o *OutputAzureDataExplorer) GetOauthType() *OauthTypeAuthenticationMethod {
	if o == nil {
		return nil
	}
	return o.OauthType
}

func (o *OutputAzureDataExplorer) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputAzureDataExplorer) GetClientSecret() *string {
	if o == nil {
		return nil
	}
	return o.ClientSecret
}

func (o *OutputAzureDataExplorer) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputAzureDataExplorer) GetCertificate() *CertificateAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.Certificate
}

func (o *OutputAzureDataExplorer) GetIngestURL() *string {
	if o == nil {
		return nil
	}
	return o.IngestURL
}

func (o *OutputAzureDataExplorer) GetOnBackpressure() *BackpressureBehaviorAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputAzureDataExplorer) GetIsMappingObj() *bool {
	if o == nil {
		return nil
	}
	return o.IsMappingObj
}

func (o *OutputAzureDataExplorer) GetFormat() *DataFormatAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputAzureDataExplorer) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputAzureDataExplorer) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputAzureDataExplorer) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputAzureDataExplorer) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputAzureDataExplorer) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputAzureDataExplorer) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputAzureDataExplorer) GetMaxConcurrentFileParts() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentFileParts
}

func (o *OutputAzureDataExplorer) GetOnDiskFullBackpressure() *DiskSpaceProtectionAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputAzureDataExplorer) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputAzureDataExplorer) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputAzureDataExplorer) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputAzureDataExplorer) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputAzureDataExplorer) GetFlushImmediately() *bool {
	if o == nil {
		return nil
	}
	return o.FlushImmediately
}

func (o *OutputAzureDataExplorer) GetRetainBlobOnSuccess() *bool {
	if o == nil {
		return nil
	}
	return o.RetainBlobOnSuccess
}

func (o *OutputAzureDataExplorer) GetExtentTags() []ExtentTag {
	if o == nil {
		return nil
	}
	return o.ExtentTags
}

func (o *OutputAzureDataExplorer) GetIngestIfNotExists() []IngestIfNotExist {
	if o == nil {
		return nil
	}
	return o.IngestIfNotExists
}

func (o *OutputAzureDataExplorer) GetReportLevel() *ReportLevel {
	if o == nil {
		return nil
	}
	return o.ReportLevel
}

func (o *OutputAzureDataExplorer) GetReportMethod() *ReportMethod {
	if o == nil {
		return nil
	}
	return o.ReportMethod
}

func (o *OutputAzureDataExplorer) GetAdditionalProperties() []AdditionalProperty {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

func (o *OutputAzureDataExplorer) GetResponseRetrySettings() []ResponseRetrySettingAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputAzureDataExplorer) GetTimeoutRetrySettings() *TimeoutRetrySettingsAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputAzureDataExplorer) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputAzureDataExplorer) GetCompress() *CompressAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputAzureDataExplorer) GetMappingRef() *string {
	if o == nil {
		return nil
	}
	return o.MappingRef
}

func (o *OutputAzureDataExplorer) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputAzureDataExplorer) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputAzureDataExplorer) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputAzureDataExplorer) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputAzureDataExplorer) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputAzureDataExplorer) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputAzureDataExplorer) GetKeepAlive() *bool {
	if o == nil {
		return nil
	}
	return o.KeepAlive
}

func (o *OutputAzureDataExplorer) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputAzureDataExplorer) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputAzureDataExplorer) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputAzureDataExplorer) GetPqCompress() *CompressionAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputAzureDataExplorer) GetPqOnBackpressure() *QueueFullBehaviorAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputAzureDataExplorer) GetPqMode() *ModeAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputAzureDataExplorer) GetPqControls() *PqControlsAzureDataExplorer {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputAzureDataExplorer) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type OutputTypeAzureBlob string

const (
	OutputTypeAzureBlobAzureBlob OutputTypeAzureBlob = "azure_blob"
)

func (e OutputTypeAzureBlob) ToPointer() *OutputTypeAzureBlob {
	return &e
}
func (e *OutputTypeAzureBlob) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "azure_blob":
		*e = OutputTypeAzureBlob(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeAzureBlob: %v", v)
	}
}

// DataFormatAzureBlob - Format of the output data
type DataFormatAzureBlob string

const (
	DataFormatAzureBlobJSON    DataFormatAzureBlob = "json"
	DataFormatAzureBlobRaw     DataFormatAzureBlob = "raw"
	DataFormatAzureBlobParquet DataFormatAzureBlob = "parquet"
)

func (e DataFormatAzureBlob) ToPointer() *DataFormatAzureBlob {
	return &e
}
func (e *DataFormatAzureBlob) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "json":
		fallthrough
	case "raw":
		fallthrough
	case "parquet":
		*e = DataFormatAzureBlob(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DataFormatAzureBlob: %v", v)
	}
}

// BackpressureBehaviorAzureBlob - Whether to block or drop events when all receivers are exerting backpressure
type BackpressureBehaviorAzureBlob string

const (
	BackpressureBehaviorAzureBlobBlock BackpressureBehaviorAzureBlob = "block"
	BackpressureBehaviorAzureBlobDrop  BackpressureBehaviorAzureBlob = "drop"
)

func (e BackpressureBehaviorAzureBlob) ToPointer() *BackpressureBehaviorAzureBlob {
	return &e
}
func (e *BackpressureBehaviorAzureBlob) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = BackpressureBehaviorAzureBlob(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorAzureBlob: %v", v)
	}
}

// DiskSpaceProtectionAzureBlob - Whether to block or drop events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionAzureBlob string

const (
	DiskSpaceProtectionAzureBlobBlock DiskSpaceProtectionAzureBlob = "block"
	DiskSpaceProtectionAzureBlobDrop  DiskSpaceProtectionAzureBlob = "drop"
)

func (e DiskSpaceProtectionAzureBlob) ToPointer() *DiskSpaceProtectionAzureBlob {
	return &e
}
func (e *DiskSpaceProtectionAzureBlob) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = DiskSpaceProtectionAzureBlob(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DiskSpaceProtectionAzureBlob: %v", v)
	}
}

// OutputAuthenticationMethodAzureBlob - Enter connection string directly, or select a stored secret
type OutputAuthenticationMethodAzureBlob string

const (
	OutputAuthenticationMethodAzureBlobManual       OutputAuthenticationMethodAzureBlob = "manual"
	OutputAuthenticationMethodAzureBlobSecret       OutputAuthenticationMethodAzureBlob = "secret"
	OutputAuthenticationMethodAzureBlobClientSecret OutputAuthenticationMethodAzureBlob = "clientSecret"
	OutputAuthenticationMethodAzureBlobClientCert   OutputAuthenticationMethodAzureBlob = "clientCert"
)

func (e OutputAuthenticationMethodAzureBlob) ToPointer() *OutputAuthenticationMethodAzureBlob {
	return &e
}
func (e *OutputAuthenticationMethodAzureBlob) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		fallthrough
	case "clientSecret":
		fallthrough
	case "clientCert":
		*e = OutputAuthenticationMethodAzureBlob(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputAuthenticationMethodAzureBlob: %v", v)
	}
}

type BlobAccessTier string

const (
	BlobAccessTierInferred BlobAccessTier = "Inferred"
	BlobAccessTierHot      BlobAccessTier = "Hot"
	BlobAccessTierCool     BlobAccessTier = "Cool"
	BlobAccessTierCold     BlobAccessTier = "Cold"
	BlobAccessTierArchive  BlobAccessTier = "Archive"
)

func (e BlobAccessTier) ToPointer() *BlobAccessTier {
	return &e
}
func (e *BlobAccessTier) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "Inferred":
		fallthrough
	case "Hot":
		fallthrough
	case "Cool":
		fallthrough
	case "Cold":
		fallthrough
	case "Archive":
		*e = BlobAccessTier(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BlobAccessTier: %v", v)
	}
}

// CompressAzureBlob - Choose data compression format to apply before moving files to final destination
type CompressAzureBlob string

const (
	CompressAzureBlobNone CompressAzureBlob = "none"
	CompressAzureBlobGzip CompressAzureBlob = "gzip"
)

func (e CompressAzureBlob) ToPointer() *CompressAzureBlob {
	return &e
}
func (e *CompressAzureBlob) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressAzureBlob(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressAzureBlob: %v", v)
	}
}

// CompressionLevelAzureBlob - Compression level to apply before moving files to final destination
type CompressionLevelAzureBlob string

const (
	CompressionLevelAzureBlobBestSpeed       CompressionLevelAzureBlob = "best_speed"
	CompressionLevelAzureBlobNormal          CompressionLevelAzureBlob = "normal"
	CompressionLevelAzureBlobBestCompression CompressionLevelAzureBlob = "best_compression"
)

func (e CompressionLevelAzureBlob) ToPointer() *CompressionLevelAzureBlob {
	return &e
}
func (e *CompressionLevelAzureBlob) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "best_speed":
		fallthrough
	case "normal":
		fallthrough
	case "best_compression":
		*e = CompressionLevelAzureBlob(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionLevelAzureBlob: %v", v)
	}
}

// ParquetVersionAzureBlob - Determines which data types are supported and how they are represented
type ParquetVersionAzureBlob string

const (
	ParquetVersionAzureBlobParquet10 ParquetVersionAzureBlob = "PARQUET_1_0"
	ParquetVersionAzureBlobParquet24 ParquetVersionAzureBlob = "PARQUET_2_4"
	ParquetVersionAzureBlobParquet26 ParquetVersionAzureBlob = "PARQUET_2_6"
)

func (e ParquetVersionAzureBlob) ToPointer() *ParquetVersionAzureBlob {
	return &e
}
func (e *ParquetVersionAzureBlob) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "PARQUET_1_0":
		fallthrough
	case "PARQUET_2_4":
		fallthrough
	case "PARQUET_2_6":
		*e = ParquetVersionAzureBlob(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ParquetVersionAzureBlob: %v", v)
	}
}

// DataPageVersionAzureBlob - Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
type DataPageVersionAzureBlob string

const (
	DataPageVersionAzureBlobDataPageV1 DataPageVersionAzureBlob = "DATA_PAGE_V1"
	DataPageVersionAzureBlobDataPageV2 DataPageVersionAzureBlob = "DATA_PAGE_V2"
)

func (e DataPageVersionAzureBlob) ToPointer() *DataPageVersionAzureBlob {
	return &e
}
func (e *DataPageVersionAzureBlob) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "DATA_PAGE_V1":
		fallthrough
	case "DATA_PAGE_V2":
		*e = DataPageVersionAzureBlob(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DataPageVersionAzureBlob: %v", v)
	}
}

type KeyValueMetadatumAzureBlob struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (k KeyValueMetadatumAzureBlob) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(k, "", false)
}

func (k *KeyValueMetadatumAzureBlob) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &k, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *KeyValueMetadatumAzureBlob) GetKey() *string {
	if o == nil {
		return nil
	}
	return o.Key
}

func (o *KeyValueMetadatumAzureBlob) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OutputCertificateAzureBlob struct {
	// The certificate you registered as credentials for your app in the Azure portal
	CertificateName string `json:"certificateName"`
}

func (o *OutputCertificateAzureBlob) GetCertificateName() string {
	if o == nil {
		return ""
	}
	return o.CertificateName
}

type OutputAzureBlob struct {
	// Unique ID for this output
	ID   *string              `json:"id,omitempty"`
	Type *OutputTypeAzureBlob `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// A container organizes a set of blobs, similar to a directory in a file system. Value can be a JavaScript expression enclosed in quotes or backticks. @{product} evaluates the expression at init time. The expression can evaluate to a constant value, and can reference Global Variables, e.g., `myContainer-${C.env["CRIBL_WORKER_ID"]}`
	ContainerName string `json:"containerName"`
	// Creates the configured container in Azure Blob Storage if it does not already exist.
	CreateContainer *bool `default:"false" json:"createContainer"`
	// Root directory prepended to path before uploading. Value can be a JavaScript expression enclosed in quotes or backticks. @{product} evaluates the expression at init time. The expression can evaluate to a constant value, and can reference Global Variables, e.g., `myBlobPrefix-${C.env["CRIBL_WORKER_ID"]}`
	DestPath *string `json:"destPath,omitempty"`
	// Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant stable storage.
	StagePath *string `default:"\\$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Append output's ID to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Maximum number of parts to upload in parallel per file.
	MaxConcurrentFileParts *float64 `default:"1" json:"maxConcurrentFileParts"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value  if present  otherwise to each location's root directory.
	PartitionExpr *string `default:"C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')" json:"partitionExpr"`
	// Format of the output data
	Format *DataFormatAzureBlob `default:"json" json:"format"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// Whether to block or drop events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorAzureBlob `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, dead-letter it to prevent further errors.
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// Whether to block or drop events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionAzureBlob `default:"block" json:"onDiskFullBackpressure"`
	// Enter connection string directly, or select a stored secret
	AuthType     *OutputAuthenticationMethodAzureBlob `default:"manual" json:"authType"`
	StorageClass *BlobAccessTier                      `default:"Inferred" json:"storageClass"`
	Description  *string                              `json:"description,omitempty"`
	// Choose data compression format to apply before moving files to final destination
	Compress *CompressAzureBlob `default:"gzip" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *CompressionLevelAzureBlob `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionAzureBlob `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *DataPageVersionAzureBlob `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging, e.g., "key":"OCSF Event Class", "value":"9001".
	KeyValueMetadata []KeyValueMetadatumAzureBlob `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// How frequently, in seconds, to clean up empty directories when 'Remove empty staging dirs' is enabled
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"\\$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64 `default:"20" json:"maxRetryNum"`
	// Enter your Azure Storage account connection string. If left blank, Stream will fall back to env.AZURE_STORAGE_CONNECTION_STRING.
	ConnectionString *string `json:"connectionString,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// The name of your Azure storage account
	StorageAccountName *string `json:"storageAccountName,omitempty"`
	// The service principal's tenant ID
	TenantID *string `json:"tenantId,omitempty"`
	// The service principal's client ID
	ClientID *string `json:"clientId,omitempty"`
	// Endpoint suffix for the service URL. Defaults to core.windows.net.
	EndpointSuffix *string `json:"endpointSuffix,omitempty"`
	// Select or create a stored text secret
	ClientTextSecret *string                     `json:"clientTextSecret,omitempty"`
	Certificate      *OutputCertificateAzureBlob `json:"certificate,omitempty"`
	Status           *TFStatus                   `json:"status,omitempty"`
}

func (o OutputAzureBlob) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputAzureBlob) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputAzureBlob) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputAzureBlob) GetType() *OutputTypeAzureBlob {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputAzureBlob) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputAzureBlob) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputAzureBlob) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputAzureBlob) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputAzureBlob) GetContainerName() string {
	if o == nil {
		return ""
	}
	return o.ContainerName
}

func (o *OutputAzureBlob) GetCreateContainer() *bool {
	if o == nil {
		return nil
	}
	return o.CreateContainer
}

func (o *OutputAzureBlob) GetDestPath() *string {
	if o == nil {
		return nil
	}
	return o.DestPath
}

func (o *OutputAzureBlob) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputAzureBlob) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputAzureBlob) GetMaxConcurrentFileParts() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentFileParts
}

func (o *OutputAzureBlob) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputAzureBlob) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputAzureBlob) GetFormat() *DataFormatAzureBlob {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputAzureBlob) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputAzureBlob) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputAzureBlob) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputAzureBlob) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputAzureBlob) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputAzureBlob) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputAzureBlob) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputAzureBlob) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputAzureBlob) GetOnBackpressure() *BackpressureBehaviorAzureBlob {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputAzureBlob) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputAzureBlob) GetOnDiskFullBackpressure() *DiskSpaceProtectionAzureBlob {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputAzureBlob) GetAuthType() *OutputAuthenticationMethodAzureBlob {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputAzureBlob) GetStorageClass() *BlobAccessTier {
	if o == nil {
		return nil
	}
	return o.StorageClass
}

func (o *OutputAzureBlob) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputAzureBlob) GetCompress() *CompressAzureBlob {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputAzureBlob) GetCompressionLevel() *CompressionLevelAzureBlob {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputAzureBlob) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputAzureBlob) GetParquetVersion() *ParquetVersionAzureBlob {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputAzureBlob) GetParquetDataPageVersion() *DataPageVersionAzureBlob {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputAzureBlob) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputAzureBlob) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputAzureBlob) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputAzureBlob) GetKeyValueMetadata() []KeyValueMetadatumAzureBlob {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputAzureBlob) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputAzureBlob) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputAzureBlob) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputAzureBlob) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputAzureBlob) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputAzureBlob) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

func (o *OutputAzureBlob) GetConnectionString() *string {
	if o == nil {
		return nil
	}
	return o.ConnectionString
}

func (o *OutputAzureBlob) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputAzureBlob) GetStorageAccountName() *string {
	if o == nil {
		return nil
	}
	return o.StorageAccountName
}

func (o *OutputAzureBlob) GetTenantID() *string {
	if o == nil {
		return nil
	}
	return o.TenantID
}

func (o *OutputAzureBlob) GetClientID() *string {
	if o == nil {
		return nil
	}
	return o.ClientID
}

func (o *OutputAzureBlob) GetEndpointSuffix() *string {
	if o == nil {
		return nil
	}
	return o.EndpointSuffix
}

func (o *OutputAzureBlob) GetClientTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.ClientTextSecret
}

func (o *OutputAzureBlob) GetCertificate() *OutputCertificateAzureBlob {
	if o == nil {
		return nil
	}
	return o.Certificate
}

func (o *OutputAzureBlob) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type OutputTypeS3 string

const (
	OutputTypeS3S3 OutputTypeS3 = "s3"
)

func (e OutputTypeS3) ToPointer() *OutputTypeS3 {
	return &e
}
func (e *OutputTypeS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "s3":
		*e = OutputTypeS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeS3: %v", v)
	}
}

// OutputAuthenticationMethodS3 - AWS authentication method. Choose Auto to use IAM roles.
type OutputAuthenticationMethodS3 string

const (
	OutputAuthenticationMethodS3Auto   OutputAuthenticationMethodS3 = "auto"
	OutputAuthenticationMethodS3Manual OutputAuthenticationMethodS3 = "manual"
	OutputAuthenticationMethodS3Secret OutputAuthenticationMethodS3 = "secret"
)

func (e OutputAuthenticationMethodS3) ToPointer() *OutputAuthenticationMethodS3 {
	return &e
}
func (e *OutputAuthenticationMethodS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "auto":
		fallthrough
	case "manual":
		fallthrough
	case "secret":
		*e = OutputAuthenticationMethodS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputAuthenticationMethodS3: %v", v)
	}
}

// OutputSignatureVersionS3 - Signature version to use for signing S3 requests
type OutputSignatureVersionS3 string

const (
	OutputSignatureVersionS3V2 OutputSignatureVersionS3 = "v2"
	OutputSignatureVersionS3V4 OutputSignatureVersionS3 = "v4"
)

func (e OutputSignatureVersionS3) ToPointer() *OutputSignatureVersionS3 {
	return &e
}
func (e *OutputSignatureVersionS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "v2":
		fallthrough
	case "v4":
		*e = OutputSignatureVersionS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputSignatureVersionS3: %v", v)
	}
}

// ObjectACLS3 - Object ACL to assign to uploaded objects.
type ObjectACLS3 string

const (
	ObjectACLS3Private                ObjectACLS3 = "private"
	ObjectACLS3PublicRead             ObjectACLS3 = "public-read"
	ObjectACLS3PublicReadWrite        ObjectACLS3 = "public-read-write"
	ObjectACLS3AuthenticatedRead      ObjectACLS3 = "authenticated-read"
	ObjectACLS3AwsExecRead            ObjectACLS3 = "aws-exec-read"
	ObjectACLS3BucketOwnerRead        ObjectACLS3 = "bucket-owner-read"
	ObjectACLS3BucketOwnerFullControl ObjectACLS3 = "bucket-owner-full-control"
)

func (e ObjectACLS3) ToPointer() *ObjectACLS3 {
	return &e
}
func (e *ObjectACLS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "private":
		fallthrough
	case "public-read":
		fallthrough
	case "public-read-write":
		fallthrough
	case "authenticated-read":
		fallthrough
	case "aws-exec-read":
		fallthrough
	case "bucket-owner-read":
		fallthrough
	case "bucket-owner-full-control":
		*e = ObjectACLS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ObjectACLS3: %v", v)
	}
}

// StorageClassS3 - Storage class to select for uploaded objects.
type StorageClassS3 string

const (
	StorageClassS3Standard           StorageClassS3 = "STANDARD"
	StorageClassS3ReducedRedundancy  StorageClassS3 = "REDUCED_REDUNDANCY"
	StorageClassS3StandardIa         StorageClassS3 = "STANDARD_IA"
	StorageClassS3OnezoneIa          StorageClassS3 = "ONEZONE_IA"
	StorageClassS3IntelligentTiering StorageClassS3 = "INTELLIGENT_TIERING"
	StorageClassS3Glacier            StorageClassS3 = "GLACIER"
	StorageClassS3GlacierIr          StorageClassS3 = "GLACIER_IR"
	StorageClassS3DeepArchive        StorageClassS3 = "DEEP_ARCHIVE"
)

func (e StorageClassS3) ToPointer() *StorageClassS3 {
	return &e
}
func (e *StorageClassS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "STANDARD":
		fallthrough
	case "REDUCED_REDUNDANCY":
		fallthrough
	case "STANDARD_IA":
		fallthrough
	case "ONEZONE_IA":
		fallthrough
	case "INTELLIGENT_TIERING":
		fallthrough
	case "GLACIER":
		fallthrough
	case "GLACIER_IR":
		fallthrough
	case "DEEP_ARCHIVE":
		*e = StorageClassS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for StorageClassS3: %v", v)
	}
}

// ServerSideEncryptionS3 - Server-side encryption for uploaded objects.
type ServerSideEncryptionS3 string

const (
	ServerSideEncryptionS3Aes256 ServerSideEncryptionS3 = "AES256"
	ServerSideEncryptionS3AwsKms ServerSideEncryptionS3 = "aws:kms"
)

func (e ServerSideEncryptionS3) ToPointer() *ServerSideEncryptionS3 {
	return &e
}
func (e *ServerSideEncryptionS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "AES256":
		fallthrough
	case "aws:kms":
		*e = ServerSideEncryptionS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ServerSideEncryptionS3: %v", v)
	}
}

// DataFormatS3 - Format of the output data
type DataFormatS3 string

const (
	DataFormatS3JSON    DataFormatS3 = "json"
	DataFormatS3Raw     DataFormatS3 = "raw"
	DataFormatS3Parquet DataFormatS3 = "parquet"
)

func (e DataFormatS3) ToPointer() *DataFormatS3 {
	return &e
}
func (e *DataFormatS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "json":
		fallthrough
	case "raw":
		fallthrough
	case "parquet":
		*e = DataFormatS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DataFormatS3: %v", v)
	}
}

// BackpressureBehaviorS3 - Whether to block or drop events when all receivers are exerting backpressure
type BackpressureBehaviorS3 string

const (
	BackpressureBehaviorS3Block BackpressureBehaviorS3 = "block"
	BackpressureBehaviorS3Drop  BackpressureBehaviorS3 = "drop"
)

func (e BackpressureBehaviorS3) ToPointer() *BackpressureBehaviorS3 {
	return &e
}
func (e *BackpressureBehaviorS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = BackpressureBehaviorS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorS3: %v", v)
	}
}

// DiskSpaceProtectionS3 - Whether to block or drop events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionS3 string

const (
	DiskSpaceProtectionS3Block DiskSpaceProtectionS3 = "block"
	DiskSpaceProtectionS3Drop  DiskSpaceProtectionS3 = "drop"
)

func (e DiskSpaceProtectionS3) ToPointer() *DiskSpaceProtectionS3 {
	return &e
}
func (e *DiskSpaceProtectionS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = DiskSpaceProtectionS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DiskSpaceProtectionS3: %v", v)
	}
}

// CompressS3 - Choose data compression format to apply before moving files to final destination
type CompressS3 string

const (
	CompressS3None CompressS3 = "none"
	CompressS3Gzip CompressS3 = "gzip"
)

func (e CompressS3) ToPointer() *CompressS3 {
	return &e
}
func (e *CompressS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressS3: %v", v)
	}
}

// CompressionLevelS3 - Compression level to apply before moving files to final destination
type CompressionLevelS3 string

const (
	CompressionLevelS3BestSpeed       CompressionLevelS3 = "best_speed"
	CompressionLevelS3Normal          CompressionLevelS3 = "normal"
	CompressionLevelS3BestCompression CompressionLevelS3 = "best_compression"
)

func (e CompressionLevelS3) ToPointer() *CompressionLevelS3 {
	return &e
}
func (e *CompressionLevelS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "best_speed":
		fallthrough
	case "normal":
		fallthrough
	case "best_compression":
		*e = CompressionLevelS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionLevelS3: %v", v)
	}
}

// ParquetVersionS3 - Determines which data types are supported and how they are represented
type ParquetVersionS3 string

const (
	ParquetVersionS3Parquet10 ParquetVersionS3 = "PARQUET_1_0"
	ParquetVersionS3Parquet24 ParquetVersionS3 = "PARQUET_2_4"
	ParquetVersionS3Parquet26 ParquetVersionS3 = "PARQUET_2_6"
)

func (e ParquetVersionS3) ToPointer() *ParquetVersionS3 {
	return &e
}
func (e *ParquetVersionS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "PARQUET_1_0":
		fallthrough
	case "PARQUET_2_4":
		fallthrough
	case "PARQUET_2_6":
		*e = ParquetVersionS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ParquetVersionS3: %v", v)
	}
}

// DataPageVersionS3 - Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
type DataPageVersionS3 string

const (
	DataPageVersionS3DataPageV1 DataPageVersionS3 = "DATA_PAGE_V1"
	DataPageVersionS3DataPageV2 DataPageVersionS3 = "DATA_PAGE_V2"
)

func (e DataPageVersionS3) ToPointer() *DataPageVersionS3 {
	return &e
}
func (e *DataPageVersionS3) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "DATA_PAGE_V1":
		fallthrough
	case "DATA_PAGE_V2":
		*e = DataPageVersionS3(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DataPageVersionS3: %v", v)
	}
}

type KeyValueMetadatumS3 struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (k KeyValueMetadatumS3) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(k, "", false)
}

func (k *KeyValueMetadatumS3) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &k, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *KeyValueMetadatumS3) GetKey() *string {
	if o == nil {
		return nil
	}
	return o.Key
}

func (o *KeyValueMetadatumS3) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OutputS3 struct {
	// Unique ID for this output
	ID   *string       `json:"id,omitempty"`
	Type *OutputTypeS3 `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. E.g., referencing a Global Variable: `myBucket-${C.vars.myVar}`.
	Bucket string `json:"bucket"`
	// Region where the S3 bucket is located.
	Region *string `json:"region,omitempty"`
	// Secret key. This value can be a constant or a JavaScript expression(e.g., `${C.env.SOME_SECRET}`).
	AwsSecretKey *string `json:"awsSecretKey,omitempty"`
	// AWS authentication method. Choose Auto to use IAM roles.
	AwsAuthenticationMethod *OutputAuthenticationMethodS3 `default:"auto" json:"awsAuthenticationMethod"`
	// S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.
	Endpoint *string `json:"endpoint,omitempty"`
	// Signature version to use for signing S3 requests
	SignatureVersion *OutputSignatureVersionS3 `default:"v4" json:"signatureVersion"`
	// Reuse connections between requests, which can improve performance
	ReuseConnections *bool `default:"true" json:"reuseConnections"`
	// Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Use Assume Role credentials to access S3
	EnableAssumeRole *bool `default:"false" json:"enableAssumeRole"`
	// Amazon Resource Name (ARN) of the role to assume
	AssumeRoleArn *string `json:"assumeRoleArn,omitempty"`
	// External ID to use when assuming role
	AssumeRoleExternalID *string `json:"assumeRoleExternalId,omitempty"`
	// Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
	DurationSeconds *float64 `default:"3600" json:"durationSeconds"`
	// Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant stable storage.
	StagePath *string `default:"\\$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// Append output's ID to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Prefix to append to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. E.g., referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`.
	DestPath *string `default:"" json:"destPath"`
	// Object ACL to assign to uploaded objects.
	ObjectACL *ObjectACLS3 `default:"private" json:"objectACL"`
	// Storage class to select for uploaded objects.
	StorageClass *StorageClassS3 `json:"storageClass,omitempty"`
	// Server-side encryption for uploaded objects.
	ServerSideEncryption *ServerSideEncryptionS3 `json:"serverSideEncryption,omitempty"`
	// ID or ARN of the KMS customer-managed key to use for encryption
	KmsKeyID *string `json:"kmsKeyId,omitempty"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value  if present  otherwise to each location's root directory.
	PartitionExpr *string `default:"C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')" json:"partitionExpr"`
	// Format of the output data
	Format *DataFormatS3 `default:"json" json:"format"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// Whether to block or drop events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorS3 `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, dead-letter it to prevent further errors.
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// Whether to block or drop events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionS3 `default:"block" json:"onDiskFullBackpressure"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.
	MaxConcurrentFileParts *float64 `default:"4" json:"maxConcurrentFileParts"`
	// Disable if you can access files within the bucket but not the bucket itself.
	VerifyPermissions *bool `default:"true" json:"verifyPermissions"`
	// Maximum number of files that can be waiting for upload before backpressure is applied
	MaxClosingFilesToBackpressure *float64 `default:"100" json:"maxClosingFilesToBackpressure"`
	Description                   *string  `json:"description,omitempty"`
	// This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)
	AwsAPIKey *string `json:"awsApiKey,omitempty"`
	// Select or create a stored secret that references your access key and secret key
	AwsSecret *string `json:"awsSecret,omitempty"`
	// Choose data compression format to apply before moving files to final destination
	Compress *CompressS3 `default:"gzip" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *CompressionLevelS3 `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionS3 `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *DataPageVersionS3 `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging, e.g., "key":"OCSF Event Class", "value":"9001".
	KeyValueMetadata []KeyValueMetadatumS3 `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// How frequently, in seconds, to clean up empty directories when 'Remove empty staging dirs' is enabled
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"\\$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64  `default:"20" json:"maxRetryNum"`
	Status      *TFStatus `json:"status,omitempty"`
}

func (o OutputS3) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputS3) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputS3) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputS3) GetType() *OutputTypeS3 {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputS3) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputS3) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputS3) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputS3) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputS3) GetBucket() string {
	if o == nil {
		return ""
	}
	return o.Bucket
}

func (o *OutputS3) GetRegion() *string {
	if o == nil {
		return nil
	}
	return o.Region
}

func (o *OutputS3) GetAwsSecretKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecretKey
}

func (o *OutputS3) GetAwsAuthenticationMethod() *OutputAuthenticationMethodS3 {
	if o == nil {
		return nil
	}
	return o.AwsAuthenticationMethod
}

func (o *OutputS3) GetEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.Endpoint
}

func (o *OutputS3) GetSignatureVersion() *OutputSignatureVersionS3 {
	if o == nil {
		return nil
	}
	return o.SignatureVersion
}

func (o *OutputS3) GetReuseConnections() *bool {
	if o == nil {
		return nil
	}
	return o.ReuseConnections
}

func (o *OutputS3) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputS3) GetEnableAssumeRole() *bool {
	if o == nil {
		return nil
	}
	return o.EnableAssumeRole
}

func (o *OutputS3) GetAssumeRoleArn() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleArn
}

func (o *OutputS3) GetAssumeRoleExternalID() *string {
	if o == nil {
		return nil
	}
	return o.AssumeRoleExternalID
}

func (o *OutputS3) GetDurationSeconds() *float64 {
	if o == nil {
		return nil
	}
	return o.DurationSeconds
}

func (o *OutputS3) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputS3) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputS3) GetDestPath() *string {
	if o == nil {
		return nil
	}
	return o.DestPath
}

func (o *OutputS3) GetObjectACL() *ObjectACLS3 {
	if o == nil {
		return nil
	}
	return o.ObjectACL
}

func (o *OutputS3) GetStorageClass() *StorageClassS3 {
	if o == nil {
		return nil
	}
	return o.StorageClass
}

func (o *OutputS3) GetServerSideEncryption() *ServerSideEncryptionS3 {
	if o == nil {
		return nil
	}
	return o.ServerSideEncryption
}

func (o *OutputS3) GetKmsKeyID() *string {
	if o == nil {
		return nil
	}
	return o.KmsKeyID
}

func (o *OutputS3) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputS3) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputS3) GetFormat() *DataFormatS3 {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputS3) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputS3) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputS3) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputS3) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputS3) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputS3) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputS3) GetOnBackpressure() *BackpressureBehaviorS3 {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputS3) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputS3) GetOnDiskFullBackpressure() *DiskSpaceProtectionS3 {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputS3) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputS3) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputS3) GetMaxConcurrentFileParts() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentFileParts
}

func (o *OutputS3) GetVerifyPermissions() *bool {
	if o == nil {
		return nil
	}
	return o.VerifyPermissions
}

func (o *OutputS3) GetMaxClosingFilesToBackpressure() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxClosingFilesToBackpressure
}

func (o *OutputS3) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputS3) GetAwsAPIKey() *string {
	if o == nil {
		return nil
	}
	return o.AwsAPIKey
}

func (o *OutputS3) GetAwsSecret() *string {
	if o == nil {
		return nil
	}
	return o.AwsSecret
}

func (o *OutputS3) GetCompress() *CompressS3 {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputS3) GetCompressionLevel() *CompressionLevelS3 {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputS3) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputS3) GetParquetVersion() *ParquetVersionS3 {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputS3) GetParquetDataPageVersion() *DataPageVersionS3 {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputS3) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputS3) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputS3) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputS3) GetKeyValueMetadata() []KeyValueMetadatumS3 {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputS3) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputS3) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputS3) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputS3) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputS3) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputS3) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

func (o *OutputS3) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeFilesystem string

const (
	TypeFilesystemFilesystem TypeFilesystem = "filesystem"
)

func (e TypeFilesystem) ToPointer() *TypeFilesystem {
	return &e
}
func (e *TypeFilesystem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "filesystem":
		*e = TypeFilesystem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeFilesystem: %v", v)
	}
}

// DataFormatFilesystem - Format of the output data
type DataFormatFilesystem string

const (
	DataFormatFilesystemJSON    DataFormatFilesystem = "json"
	DataFormatFilesystemRaw     DataFormatFilesystem = "raw"
	DataFormatFilesystemParquet DataFormatFilesystem = "parquet"
)

func (e DataFormatFilesystem) ToPointer() *DataFormatFilesystem {
	return &e
}
func (e *DataFormatFilesystem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "json":
		fallthrough
	case "raw":
		fallthrough
	case "parquet":
		*e = DataFormatFilesystem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DataFormatFilesystem: %v", v)
	}
}

// BackpressureBehaviorFilesystem - Whether to block or drop events when all receivers are exerting backpressure
type BackpressureBehaviorFilesystem string

const (
	BackpressureBehaviorFilesystemBlock BackpressureBehaviorFilesystem = "block"
	BackpressureBehaviorFilesystemDrop  BackpressureBehaviorFilesystem = "drop"
)

func (e BackpressureBehaviorFilesystem) ToPointer() *BackpressureBehaviorFilesystem {
	return &e
}
func (e *BackpressureBehaviorFilesystem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = BackpressureBehaviorFilesystem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorFilesystem: %v", v)
	}
}

// DiskSpaceProtectionFilesystem - Whether to block or drop events when disk space is below the global 'Min free disk space' limit
type DiskSpaceProtectionFilesystem string

const (
	DiskSpaceProtectionFilesystemBlock DiskSpaceProtectionFilesystem = "block"
	DiskSpaceProtectionFilesystemDrop  DiskSpaceProtectionFilesystem = "drop"
)

func (e DiskSpaceProtectionFilesystem) ToPointer() *DiskSpaceProtectionFilesystem {
	return &e
}
func (e *DiskSpaceProtectionFilesystem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = DiskSpaceProtectionFilesystem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DiskSpaceProtectionFilesystem: %v", v)
	}
}

// CompressFilesystem - Choose data compression format to apply before moving files to final destination
type CompressFilesystem string

const (
	CompressFilesystemNone CompressFilesystem = "none"
	CompressFilesystemGzip CompressFilesystem = "gzip"
)

func (e CompressFilesystem) ToPointer() *CompressFilesystem {
	return &e
}
func (e *CompressFilesystem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressFilesystem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressFilesystem: %v", v)
	}
}

// CompressionLevelFilesystem - Compression level to apply before moving files to final destination
type CompressionLevelFilesystem string

const (
	CompressionLevelFilesystemBestSpeed       CompressionLevelFilesystem = "best_speed"
	CompressionLevelFilesystemNormal          CompressionLevelFilesystem = "normal"
	CompressionLevelFilesystemBestCompression CompressionLevelFilesystem = "best_compression"
)

func (e CompressionLevelFilesystem) ToPointer() *CompressionLevelFilesystem {
	return &e
}
func (e *CompressionLevelFilesystem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "best_speed":
		fallthrough
	case "normal":
		fallthrough
	case "best_compression":
		*e = CompressionLevelFilesystem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionLevelFilesystem: %v", v)
	}
}

// ParquetVersionFilesystem - Determines which data types are supported and how they are represented
type ParquetVersionFilesystem string

const (
	ParquetVersionFilesystemParquet10 ParquetVersionFilesystem = "PARQUET_1_0"
	ParquetVersionFilesystemParquet24 ParquetVersionFilesystem = "PARQUET_2_4"
	ParquetVersionFilesystemParquet26 ParquetVersionFilesystem = "PARQUET_2_6"
)

func (e ParquetVersionFilesystem) ToPointer() *ParquetVersionFilesystem {
	return &e
}
func (e *ParquetVersionFilesystem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "PARQUET_1_0":
		fallthrough
	case "PARQUET_2_4":
		fallthrough
	case "PARQUET_2_6":
		*e = ParquetVersionFilesystem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ParquetVersionFilesystem: %v", v)
	}
}

// DataPageVersionFilesystem - Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
type DataPageVersionFilesystem string

const (
	DataPageVersionFilesystemDataPageV1 DataPageVersionFilesystem = "DATA_PAGE_V1"
	DataPageVersionFilesystemDataPageV2 DataPageVersionFilesystem = "DATA_PAGE_V2"
)

func (e DataPageVersionFilesystem) ToPointer() *DataPageVersionFilesystem {
	return &e
}
func (e *DataPageVersionFilesystem) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "DATA_PAGE_V1":
		fallthrough
	case "DATA_PAGE_V2":
		*e = DataPageVersionFilesystem(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DataPageVersionFilesystem: %v", v)
	}
}

type KeyValueMetadatumFilesystem struct {
	Key   *string `default:"" json:"key"`
	Value string  `json:"value"`
}

func (k KeyValueMetadatumFilesystem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(k, "", false)
}

func (k *KeyValueMetadatumFilesystem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &k, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *KeyValueMetadatumFilesystem) GetKey() *string {
	if o == nil {
		return nil
	}
	return o.Key
}

func (o *KeyValueMetadatumFilesystem) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OutputFilesystem struct {
	// Unique ID for this output
	ID   *string        `json:"id,omitempty"`
	Type TypeFilesystem `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Final destination for the output files
	DestPath string `json:"destPath"`
	// Filesystem location in which to buffer files before compressing and moving to final destination. Use performant, stable storage.
	StagePath *string `json:"stagePath,omitempty"`
	// Append output's ID to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value  if present  otherwise to each location's root directory.
	PartitionExpr *string `default:"C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')" json:"partitionExpr"`
	// Format of the output data
	Format *DataFormatFilesystem `default:"json" json:"format"`
	// JavaScript expression to define the output filename prefix (can be constant)
	BaseFileName *string `default:"CriblOut" json:"baseFileName"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// If set, this line will be written to the beginning of each output file
	HeaderLine *string `default:"" json:"headerLine"`
	// Buffer size used to write to a file
	WriteHighWaterMark *float64 `default:"64" json:"writeHighWaterMark"`
	// Whether to block or drop events when all receivers are exerting backpressure
	OnBackpressure *BackpressureBehaviorFilesystem `default:"block" json:"onBackpressure"`
	// If a file fails to move to its final destination after the maximum number of retries, dead-letter it to prevent further errors.
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// Whether to block or drop events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *DiskSpaceProtectionFilesystem `default:"block" json:"onDiskFullBackpressure"`
	Description            *string                        `json:"description,omitempty"`
	// Choose data compression format to apply before moving files to final destination
	Compress *CompressFilesystem `default:"gzip" json:"compress"`
	// Compression level to apply before moving files to final destination
	CompressionLevel *CompressionLevelFilesystem `default:"best_speed" json:"compressionLevel"`
	// Automatically calculate the schema based on the events of each Parquet file generated
	AutomaticSchema *bool `default:"false" json:"automaticSchema"`
	// Determines which data types are supported and how they are represented
	ParquetVersion *ParquetVersionFilesystem `default:"PARQUET_2_6" json:"parquetVersion"`
	// Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
	ParquetDataPageVersion *DataPageVersionFilesystem `default:"DATA_PAGE_V2" json:"parquetDataPageVersion"`
	// The number of rows that every group will contain. The final group can contain a smaller number of rows.
	ParquetRowGroupLength *float64 `default:"10000" json:"parquetRowGroupLength"`
	// Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
	ParquetPageSize *string `default:"1MB" json:"parquetPageSize"`
	// Log up to 3 rows that @{product} skips due to data mismatch
	ShouldLogInvalidRows *bool `json:"shouldLogInvalidRows,omitempty"`
	// The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging, e.g., "key":"OCSF Event Class", "value":"9001".
	KeyValueMetadata []KeyValueMetadatumFilesystem `json:"keyValueMetadata,omitempty"`
	// Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
	EnableStatistics *bool `default:"true" json:"enableStatistics"`
	// One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
	EnableWritePageIndex *bool `default:"true" json:"enableWritePageIndex"`
	// Parquet tools can use the checksum of a Parquet page to verify data integrity
	EnablePageChecksum *bool `default:"false" json:"enablePageChecksum"`
	// How frequently, in seconds, to clean up empty directories when 'Remove empty staging dirs' is enabled
	EmptyDirCleanupSec *float64 `default:"300" json:"emptyDirCleanupSec"`
	// Storage location for files that fail to reach their final destination after maximum retries are exceeded
	DeadletterPath *string `default:"\\$CRIBL_HOME/state/outputs/dead-letter" json:"deadletterPath"`
	// The maximum number of times a file will attempt to move to its final destination before being dead-lettered
	MaxRetryNum *float64  `default:"20" json:"maxRetryNum"`
	Status      *TFStatus `json:"status,omitempty"`
}

func (o OutputFilesystem) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputFilesystem) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputFilesystem) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputFilesystem) GetType() TypeFilesystem {
	if o == nil {
		return TypeFilesystem("")
	}
	return o.Type
}

func (o *OutputFilesystem) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputFilesystem) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputFilesystem) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputFilesystem) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputFilesystem) GetDestPath() string {
	if o == nil {
		return ""
	}
	return o.DestPath
}

func (o *OutputFilesystem) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputFilesystem) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputFilesystem) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputFilesystem) GetPartitionExpr() *string {
	if o == nil {
		return nil
	}
	return o.PartitionExpr
}

func (o *OutputFilesystem) GetFormat() *DataFormatFilesystem {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputFilesystem) GetBaseFileName() *string {
	if o == nil {
		return nil
	}
	return o.BaseFileName
}

func (o *OutputFilesystem) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputFilesystem) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputFilesystem) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputFilesystem) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputFilesystem) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputFilesystem) GetHeaderLine() *string {
	if o == nil {
		return nil
	}
	return o.HeaderLine
}

func (o *OutputFilesystem) GetWriteHighWaterMark() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteHighWaterMark
}

func (o *OutputFilesystem) GetOnBackpressure() *BackpressureBehaviorFilesystem {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputFilesystem) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputFilesystem) GetOnDiskFullBackpressure() *DiskSpaceProtectionFilesystem {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputFilesystem) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputFilesystem) GetCompress() *CompressFilesystem {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputFilesystem) GetCompressionLevel() *CompressionLevelFilesystem {
	if o == nil {
		return nil
	}
	return o.CompressionLevel
}

func (o *OutputFilesystem) GetAutomaticSchema() *bool {
	if o == nil {
		return nil
	}
	return o.AutomaticSchema
}

func (o *OutputFilesystem) GetParquetVersion() *ParquetVersionFilesystem {
	if o == nil {
		return nil
	}
	return o.ParquetVersion
}

func (o *OutputFilesystem) GetParquetDataPageVersion() *DataPageVersionFilesystem {
	if o == nil {
		return nil
	}
	return o.ParquetDataPageVersion
}

func (o *OutputFilesystem) GetParquetRowGroupLength() *float64 {
	if o == nil {
		return nil
	}
	return o.ParquetRowGroupLength
}

func (o *OutputFilesystem) GetParquetPageSize() *string {
	if o == nil {
		return nil
	}
	return o.ParquetPageSize
}

func (o *OutputFilesystem) GetShouldLogInvalidRows() *bool {
	if o == nil {
		return nil
	}
	return o.ShouldLogInvalidRows
}

func (o *OutputFilesystem) GetKeyValueMetadata() []KeyValueMetadatumFilesystem {
	if o == nil {
		return nil
	}
	return o.KeyValueMetadata
}

func (o *OutputFilesystem) GetEnableStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableStatistics
}

func (o *OutputFilesystem) GetEnableWritePageIndex() *bool {
	if o == nil {
		return nil
	}
	return o.EnableWritePageIndex
}

func (o *OutputFilesystem) GetEnablePageChecksum() *bool {
	if o == nil {
		return nil
	}
	return o.EnablePageChecksum
}

func (o *OutputFilesystem) GetEmptyDirCleanupSec() *float64 {
	if o == nil {
		return nil
	}
	return o.EmptyDirCleanupSec
}

func (o *OutputFilesystem) GetDeadletterPath() *string {
	if o == nil {
		return nil
	}
	return o.DeadletterPath
}

func (o *OutputFilesystem) GetMaxRetryNum() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRetryNum
}

func (o *OutputFilesystem) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeSignalfx string

const (
	TypeSignalfxSignalfx TypeSignalfx = "signalfx"
)

func (e TypeSignalfx) ToPointer() *TypeSignalfx {
	return &e
}
func (e *TypeSignalfx) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "signalfx":
		*e = TypeSignalfx(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeSignalfx: %v", v)
	}
}

// AuthenticationMethodSignalfx - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type AuthenticationMethodSignalfx string

const (
	AuthenticationMethodSignalfxManual AuthenticationMethodSignalfx = "manual"
	AuthenticationMethodSignalfxSecret AuthenticationMethodSignalfx = "secret"
)

func (e AuthenticationMethodSignalfx) ToPointer() *AuthenticationMethodSignalfx {
	return &e
}
func (e *AuthenticationMethodSignalfx) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		*e = AuthenticationMethodSignalfx(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodSignalfx: %v", v)
	}
}

type ExtraHTTPHeaderSignalfx struct {
	// Field name
	Name *string `json:"name,omitempty"`
	// Field value
	Value string `json:"value"`
}

func (o *ExtraHTTPHeaderSignalfx) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderSignalfx) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeSignalfx - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeSignalfx string

const (
	FailedRequestLoggingModeSignalfxPayload           FailedRequestLoggingModeSignalfx = "payload"
	FailedRequestLoggingModeSignalfxPayloadAndHeaders FailedRequestLoggingModeSignalfx = "payloadAndHeaders"
	FailedRequestLoggingModeSignalfxNone              FailedRequestLoggingModeSignalfx = "none"
)

func (e FailedRequestLoggingModeSignalfx) ToPointer() *FailedRequestLoggingModeSignalfx {
	return &e
}
func (e *FailedRequestLoggingModeSignalfx) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeSignalfx(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeSignalfx: %v", v)
	}
}

type ResponseRetrySettingSignalfx struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingSignalfx) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingSignalfx) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingSignalfx) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingSignalfx) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingSignalfx) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingSignalfx) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsSignalfx struct {
	// Enable to retry on request timeout
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsSignalfx) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsSignalfx) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsSignalfx) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsSignalfx) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsSignalfx) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsSignalfx) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorSignalfx - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorSignalfx string

const (
	BackpressureBehaviorSignalfxBlock BackpressureBehaviorSignalfx = "block"
	BackpressureBehaviorSignalfxDrop  BackpressureBehaviorSignalfx = "drop"
	BackpressureBehaviorSignalfxQueue BackpressureBehaviorSignalfx = "queue"
)

func (e BackpressureBehaviorSignalfx) ToPointer() *BackpressureBehaviorSignalfx {
	return &e
}
func (e *BackpressureBehaviorSignalfx) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorSignalfx(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorSignalfx: %v", v)
	}
}

// CompressionSignalfx - Codec to use to compress the persisted data.
type CompressionSignalfx string

const (
	CompressionSignalfxNone CompressionSignalfx = "none"
	CompressionSignalfxGzip CompressionSignalfx = "gzip"
)

func (e CompressionSignalfx) ToPointer() *CompressionSignalfx {
	return &e
}
func (e *CompressionSignalfx) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionSignalfx(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionSignalfx: %v", v)
	}
}

// QueueFullBehaviorSignalfx - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSignalfx string

const (
	QueueFullBehaviorSignalfxBlock QueueFullBehaviorSignalfx = "block"
	QueueFullBehaviorSignalfxDrop  QueueFullBehaviorSignalfx = "drop"
)

func (e QueueFullBehaviorSignalfx) ToPointer() *QueueFullBehaviorSignalfx {
	return &e
}
func (e *QueueFullBehaviorSignalfx) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorSignalfx(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorSignalfx: %v", v)
	}
}

// ModeSignalfx - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type ModeSignalfx string

const (
	ModeSignalfxError        ModeSignalfx = "error"
	ModeSignalfxBackpressure ModeSignalfx = "backpressure"
	ModeSignalfxAlways       ModeSignalfx = "always"
)

func (e ModeSignalfx) ToPointer() *ModeSignalfx {
	return &e
}
func (e *ModeSignalfx) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeSignalfx(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeSignalfx: %v", v)
	}
}

type PqControlsSignalfx struct {
}

type OutputSignalfx struct {
	// Unique ID for this output
	ID   *string      `json:"id,omitempty"`
	Type TypeSignalfx `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType *AuthenticationMethodSignalfx `default:"manual" json:"authType"`
	// SignalFx realm name, e.g. "us0". For a complete list of available SignalFx realm names, please check [here](https://docs.splunk.com/observability/en/get-started/service-description.html#sd-regions).
	Realm *string `default:"us0" json:"realm"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Defaults to Yes. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max body size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events.
	ExtraHTTPHeaders []ExtraHTTPHeaderSignalfx `json:"extraHttpHeaders,omitempty"`
	// Enables round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeSignalfx `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable).
	ResponseRetrySettings []ResponseRetrySettingSignalfx `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsSignalfx  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorSignalfx `default:"block" json:"onBackpressure"`
	Description    *string                       `json:"description,omitempty"`
	// SignalFx API access token (see [here](https://docs.signalfx.com/en/latest/admin-guide/tokens.html#working-with-access-tokens))
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *CompressionSignalfx `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorSignalfx `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeSignalfx       `default:"error" json:"pqMode"`
	PqControls *PqControlsSignalfx `json:"pqControls,omitempty"`
	Status     *TFStatus           `json:"status,omitempty"`
}

func (o OutputSignalfx) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSignalfx) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputSignalfx) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputSignalfx) GetType() TypeSignalfx {
	if o == nil {
		return TypeSignalfx("")
	}
	return o.Type
}

func (o *OutputSignalfx) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSignalfx) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSignalfx) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSignalfx) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSignalfx) GetAuthType() *AuthenticationMethodSignalfx {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputSignalfx) GetRealm() *string {
	if o == nil {
		return nil
	}
	return o.Realm
}

func (o *OutputSignalfx) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputSignalfx) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputSignalfx) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputSignalfx) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputSignalfx) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputSignalfx) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputSignalfx) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputSignalfx) GetExtraHTTPHeaders() []ExtraHTTPHeaderSignalfx {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputSignalfx) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputSignalfx) GetFailedRequestLoggingMode() *FailedRequestLoggingModeSignalfx {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputSignalfx) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputSignalfx) GetResponseRetrySettings() []ResponseRetrySettingSignalfx {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputSignalfx) GetTimeoutRetrySettings() *TimeoutRetrySettingsSignalfx {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputSignalfx) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputSignalfx) GetOnBackpressure() *BackpressureBehaviorSignalfx {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSignalfx) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSignalfx) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputSignalfx) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputSignalfx) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSignalfx) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSignalfx) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSignalfx) GetPqCompress() *CompressionSignalfx {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSignalfx) GetPqOnBackpressure() *QueueFullBehaviorSignalfx {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSignalfx) GetPqMode() *ModeSignalfx {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSignalfx) GetPqControls() *PqControlsSignalfx {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputSignalfx) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeWavefront string

const (
	TypeWavefrontWavefront TypeWavefront = "wavefront"
)

func (e TypeWavefront) ToPointer() *TypeWavefront {
	return &e
}
func (e *TypeWavefront) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "wavefront":
		*e = TypeWavefront(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeWavefront: %v", v)
	}
}

// AuthenticationMethodWavefront - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type AuthenticationMethodWavefront string

const (
	AuthenticationMethodWavefrontManual AuthenticationMethodWavefront = "manual"
	AuthenticationMethodWavefrontSecret AuthenticationMethodWavefront = "secret"
)

func (e AuthenticationMethodWavefront) ToPointer() *AuthenticationMethodWavefront {
	return &e
}
func (e *AuthenticationMethodWavefront) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		*e = AuthenticationMethodWavefront(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodWavefront: %v", v)
	}
}

type ExtraHTTPHeaderWavefront struct {
	// Field name
	Name *string `json:"name,omitempty"`
	// Field value
	Value string `json:"value"`
}

func (o *ExtraHTTPHeaderWavefront) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderWavefront) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeWavefront - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeWavefront string

const (
	FailedRequestLoggingModeWavefrontPayload           FailedRequestLoggingModeWavefront = "payload"
	FailedRequestLoggingModeWavefrontPayloadAndHeaders FailedRequestLoggingModeWavefront = "payloadAndHeaders"
	FailedRequestLoggingModeWavefrontNone              FailedRequestLoggingModeWavefront = "none"
)

func (e FailedRequestLoggingModeWavefront) ToPointer() *FailedRequestLoggingModeWavefront {
	return &e
}
func (e *FailedRequestLoggingModeWavefront) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeWavefront(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeWavefront: %v", v)
	}
}

type ResponseRetrySettingWavefront struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingWavefront) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingWavefront) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingWavefront) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingWavefront) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingWavefront) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingWavefront) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsWavefront struct {
	// Enable to retry on request timeout
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsWavefront) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsWavefront) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsWavefront) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsWavefront) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsWavefront) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsWavefront) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorWavefront - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorWavefront string

const (
	BackpressureBehaviorWavefrontBlock BackpressureBehaviorWavefront = "block"
	BackpressureBehaviorWavefrontDrop  BackpressureBehaviorWavefront = "drop"
	BackpressureBehaviorWavefrontQueue BackpressureBehaviorWavefront = "queue"
)

func (e BackpressureBehaviorWavefront) ToPointer() *BackpressureBehaviorWavefront {
	return &e
}
func (e *BackpressureBehaviorWavefront) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorWavefront(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorWavefront: %v", v)
	}
}

// CompressionWavefront - Codec to use to compress the persisted data.
type CompressionWavefront string

const (
	CompressionWavefrontNone CompressionWavefront = "none"
	CompressionWavefrontGzip CompressionWavefront = "gzip"
)

func (e CompressionWavefront) ToPointer() *CompressionWavefront {
	return &e
}
func (e *CompressionWavefront) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionWavefront(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionWavefront: %v", v)
	}
}

// QueueFullBehaviorWavefront - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorWavefront string

const (
	QueueFullBehaviorWavefrontBlock QueueFullBehaviorWavefront = "block"
	QueueFullBehaviorWavefrontDrop  QueueFullBehaviorWavefront = "drop"
)

func (e QueueFullBehaviorWavefront) ToPointer() *QueueFullBehaviorWavefront {
	return &e
}
func (e *QueueFullBehaviorWavefront) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorWavefront(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorWavefront: %v", v)
	}
}

// ModeWavefront - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type ModeWavefront string

const (
	ModeWavefrontError        ModeWavefront = "error"
	ModeWavefrontBackpressure ModeWavefront = "backpressure"
	ModeWavefrontAlways       ModeWavefront = "always"
)

func (e ModeWavefront) ToPointer() *ModeWavefront {
	return &e
}
func (e *ModeWavefront) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeWavefront(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeWavefront: %v", v)
	}
}

type PqControlsWavefront struct {
}

type OutputWavefront struct {
	// Unique ID for this output
	ID   *string       `json:"id,omitempty"`
	Type TypeWavefront `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType *AuthenticationMethodWavefront `default:"manual" json:"authType"`
	// WaveFront domain name, e.g. "longboard"
	Domain *string `default:"longboard" json:"domain"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Defaults to Yes. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max body size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events.
	ExtraHTTPHeaders []ExtraHTTPHeaderWavefront `json:"extraHttpHeaders,omitempty"`
	// Enables round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeWavefront `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable).
	ResponseRetrySettings []ResponseRetrySettingWavefront `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsWavefront  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorWavefront `default:"block" json:"onBackpressure"`
	Description    *string                        `json:"description,omitempty"`
	// WaveFront API authentication token (see [here](https://docs.wavefront.com/wavefront_api.html#generating-an-api-token))
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *CompressionWavefront `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorWavefront `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeWavefront       `default:"error" json:"pqMode"`
	PqControls *PqControlsWavefront `json:"pqControls,omitempty"`
	Status     *TFStatus            `json:"status,omitempty"`
}

func (o OutputWavefront) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputWavefront) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputWavefront) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputWavefront) GetType() TypeWavefront {
	if o == nil {
		return TypeWavefront("")
	}
	return o.Type
}

func (o *OutputWavefront) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputWavefront) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputWavefront) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputWavefront) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputWavefront) GetAuthType() *AuthenticationMethodWavefront {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputWavefront) GetDomain() *string {
	if o == nil {
		return nil
	}
	return o.Domain
}

func (o *OutputWavefront) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputWavefront) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputWavefront) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputWavefront) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputWavefront) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputWavefront) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputWavefront) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputWavefront) GetExtraHTTPHeaders() []ExtraHTTPHeaderWavefront {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputWavefront) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputWavefront) GetFailedRequestLoggingMode() *FailedRequestLoggingModeWavefront {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputWavefront) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputWavefront) GetResponseRetrySettings() []ResponseRetrySettingWavefront {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputWavefront) GetTimeoutRetrySettings() *TimeoutRetrySettingsWavefront {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputWavefront) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputWavefront) GetOnBackpressure() *BackpressureBehaviorWavefront {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputWavefront) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputWavefront) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputWavefront) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputWavefront) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputWavefront) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputWavefront) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputWavefront) GetPqCompress() *CompressionWavefront {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputWavefront) GetPqOnBackpressure() *QueueFullBehaviorWavefront {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputWavefront) GetPqMode() *ModeWavefront {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputWavefront) GetPqControls() *PqControlsWavefront {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputWavefront) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type OutputTypeTcpjson string

const (
	OutputTypeTcpjsonTcpjson OutputTypeTcpjson = "tcpjson"
)

func (e OutputTypeTcpjson) ToPointer() *OutputTypeTcpjson {
	return &e
}
func (e *OutputTypeTcpjson) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "tcpjson":
		*e = OutputTypeTcpjson(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeTcpjson: %v", v)
	}
}

// OutputCompressionTcpjson - Codec to use to compress the data before sending
type OutputCompressionTcpjson string

const (
	OutputCompressionTcpjsonNone OutputCompressionTcpjson = "none"
	OutputCompressionTcpjsonGzip OutputCompressionTcpjson = "gzip"
)

func (e OutputCompressionTcpjson) ToPointer() *OutputCompressionTcpjson {
	return &e
}
func (e *OutputCompressionTcpjson) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = OutputCompressionTcpjson(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputCompressionTcpjson: %v", v)
	}
}

// OutputMinimumTLSVersionTcpjson - Minimum TLS version to use when connecting
type OutputMinimumTLSVersionTcpjson string

const (
	OutputMinimumTLSVersionTcpjsonTlSv1  OutputMinimumTLSVersionTcpjson = "TLSv1"
	OutputMinimumTLSVersionTcpjsonTlSv11 OutputMinimumTLSVersionTcpjson = "TLSv1.1"
	OutputMinimumTLSVersionTcpjsonTlSv12 OutputMinimumTLSVersionTcpjson = "TLSv1.2"
	OutputMinimumTLSVersionTcpjsonTlSv13 OutputMinimumTLSVersionTcpjson = "TLSv1.3"
)

func (e OutputMinimumTLSVersionTcpjson) ToPointer() *OutputMinimumTLSVersionTcpjson {
	return &e
}
func (e *OutputMinimumTLSVersionTcpjson) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = OutputMinimumTLSVersionTcpjson(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMinimumTLSVersionTcpjson: %v", v)
	}
}

// OutputMaximumTLSVersionTcpjson - Maximum TLS version to use when connecting
type OutputMaximumTLSVersionTcpjson string

const (
	OutputMaximumTLSVersionTcpjsonTlSv1  OutputMaximumTLSVersionTcpjson = "TLSv1"
	OutputMaximumTLSVersionTcpjsonTlSv11 OutputMaximumTLSVersionTcpjson = "TLSv1.1"
	OutputMaximumTLSVersionTcpjsonTlSv12 OutputMaximumTLSVersionTcpjson = "TLSv1.2"
	OutputMaximumTLSVersionTcpjsonTlSv13 OutputMaximumTLSVersionTcpjson = "TLSv1.3"
)

func (e OutputMaximumTLSVersionTcpjson) ToPointer() *OutputMaximumTLSVersionTcpjson {
	return &e
}
func (e *OutputMaximumTLSVersionTcpjson) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = OutputMaximumTLSVersionTcpjson(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMaximumTLSVersionTcpjson: %v", v)
	}
}

type TLSSettingsClientSideTcpjson struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certs that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (e.g., the system's CA). Defaults to Yes. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate.
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key.
	Passphrase *string `json:"passphrase,omitempty"`
	// Minimum TLS version to use when connecting
	MinVersion *OutputMinimumTLSVersionTcpjson `json:"minVersion,omitempty"`
	// Maximum TLS version to use when connecting
	MaxVersion *OutputMaximumTLSVersionTcpjson `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideTcpjson) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideTcpjson) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TLSSettingsClientSideTcpjson) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *TLSSettingsClientSideTcpjson) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *TLSSettingsClientSideTcpjson) GetServername() *string {
	if o == nil {
		return nil
	}
	return o.Servername
}

func (o *TLSSettingsClientSideTcpjson) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

func (o *TLSSettingsClientSideTcpjson) GetCaPath() *string {
	if o == nil {
		return nil
	}
	return o.CaPath
}

func (o *TLSSettingsClientSideTcpjson) GetPrivKeyPath() *string {
	if o == nil {
		return nil
	}
	return o.PrivKeyPath
}

func (o *TLSSettingsClientSideTcpjson) GetCertPath() *string {
	if o == nil {
		return nil
	}
	return o.CertPath
}

func (o *TLSSettingsClientSideTcpjson) GetPassphrase() *string {
	if o == nil {
		return nil
	}
	return o.Passphrase
}

func (o *TLSSettingsClientSideTcpjson) GetMinVersion() *OutputMinimumTLSVersionTcpjson {
	if o == nil {
		return nil
	}
	return o.MinVersion
}

func (o *TLSSettingsClientSideTcpjson) GetMaxVersion() *OutputMaximumTLSVersionTcpjson {
	if o == nil {
		return nil
	}
	return o.MaxVersion
}

// BackpressureBehaviorTcpjson - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorTcpjson string

const (
	BackpressureBehaviorTcpjsonBlock BackpressureBehaviorTcpjson = "block"
	BackpressureBehaviorTcpjsonDrop  BackpressureBehaviorTcpjson = "drop"
	BackpressureBehaviorTcpjsonQueue BackpressureBehaviorTcpjson = "queue"
)

func (e BackpressureBehaviorTcpjson) ToPointer() *BackpressureBehaviorTcpjson {
	return &e
}
func (e *BackpressureBehaviorTcpjson) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorTcpjson(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorTcpjson: %v", v)
	}
}

// OutputAuthenticationMethodTcpjson - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type OutputAuthenticationMethodTcpjson string

const (
	OutputAuthenticationMethodTcpjsonManual OutputAuthenticationMethodTcpjson = "manual"
	OutputAuthenticationMethodTcpjsonSecret OutputAuthenticationMethodTcpjson = "secret"
)

func (e OutputAuthenticationMethodTcpjson) ToPointer() *OutputAuthenticationMethodTcpjson {
	return &e
}
func (e *OutputAuthenticationMethodTcpjson) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		*e = OutputAuthenticationMethodTcpjson(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputAuthenticationMethodTcpjson: %v", v)
	}
}

// TLSTcpjson - Whether to inherit TLS configs from group setting or disable TLS.
type TLSTcpjson string

const (
	TLSTcpjsonInherit TLSTcpjson = "inherit"
	TLSTcpjsonFalse   TLSTcpjson = "false"
)

func (e TLSTcpjson) ToPointer() *TLSTcpjson {
	return &e
}
func (e *TLSTcpjson) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "inherit":
		fallthrough
	case "false":
		*e = TLSTcpjson(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TLSTcpjson: %v", v)
	}
}

type HostTcpjson struct {
	// The hostname of the receiver.
	Host string `json:"host"`
	// The port to connect to on the provided host.
	Port float64 `json:"port"`
	// Whether to inherit TLS configs from group setting or disable TLS.
	TLS *TLSTcpjson `default:"inherit" json:"tls"`
	// Servername to use if establishing a TLS connection. If not specified, defaults to connection host (iff not an IP); otherwise, to the global TLS settings.
	Servername *string `json:"servername,omitempty"`
	// Assign a weight (>0) to each endpoint to indicate its traffic-handling capability
	Weight *float64 `default:"1" json:"weight"`
}

func (h HostTcpjson) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(h, "", false)
}

func (h *HostTcpjson) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &h, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *HostTcpjson) GetHost() string {
	if o == nil {
		return ""
	}
	return o.Host
}

func (o *HostTcpjson) GetPort() float64 {
	if o == nil {
		return 0.0
	}
	return o.Port
}

func (o *HostTcpjson) GetTLS() *TLSTcpjson {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *HostTcpjson) GetServername() *string {
	if o == nil {
		return nil
	}
	return o.Servername
}

func (o *HostTcpjson) GetWeight() *float64 {
	if o == nil {
		return nil
	}
	return o.Weight
}

// PqCompressCompressionTcpjson - Codec to use to compress the persisted data.
type PqCompressCompressionTcpjson string

const (
	PqCompressCompressionTcpjsonNone PqCompressCompressionTcpjson = "none"
	PqCompressCompressionTcpjsonGzip PqCompressCompressionTcpjson = "gzip"
)

func (e PqCompressCompressionTcpjson) ToPointer() *PqCompressCompressionTcpjson {
	return &e
}
func (e *PqCompressCompressionTcpjson) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = PqCompressCompressionTcpjson(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PqCompressCompressionTcpjson: %v", v)
	}
}

// QueueFullBehaviorTcpjson - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorTcpjson string

const (
	QueueFullBehaviorTcpjsonBlock QueueFullBehaviorTcpjson = "block"
	QueueFullBehaviorTcpjsonDrop  QueueFullBehaviorTcpjson = "drop"
)

func (e QueueFullBehaviorTcpjson) ToPointer() *QueueFullBehaviorTcpjson {
	return &e
}
func (e *QueueFullBehaviorTcpjson) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorTcpjson(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorTcpjson: %v", v)
	}
}

// OutputModeTcpjson - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type OutputModeTcpjson string

const (
	OutputModeTcpjsonError        OutputModeTcpjson = "error"
	OutputModeTcpjsonBackpressure OutputModeTcpjson = "backpressure"
	OutputModeTcpjsonAlways       OutputModeTcpjson = "always"
)

func (e OutputModeTcpjson) ToPointer() *OutputModeTcpjson {
	return &e
}
func (e *OutputModeTcpjson) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = OutputModeTcpjson(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputModeTcpjson: %v", v)
	}
}

type PqControlsTcpjson struct {
}

type OutputTcpjson struct {
	// Unique ID for this output
	ID   string            `json:"id"`
	Type OutputTypeTcpjson `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Use load-balanced destinations
	LoadBalanced *bool `default:"true" json:"loadBalanced"`
	// Codec to use to compress the data before sending
	Compression *OutputCompressionTcpjson `default:"gzip" json:"compression"`
	// Use to troubleshoot issues with sending data
	LogFailedRequests *bool `default:"false" json:"logFailedRequests"`
	// Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
	ThrottleRatePerSec *string                       `default:"0" json:"throttleRatePerSec"`
	TLS                *TLSSettingsClientSideTcpjson `json:"tls,omitempty"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead
	WriteTimeout *float64 `default:"60000" json:"writeTimeout"`
	// The number of minutes before the internally generated authentication token expires, valid values between 1 and 60
	TokenTTLMinutes *float64 `default:"60" json:"tokenTTLMinutes"`
	// Upon connection, send a header-like record containing the auth token and other metadata.This record will not contain an actual event  only subsequent records will.
	SendHeader *bool `default:"true" json:"sendHeader"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorTcpjson `default:"block" json:"onBackpressure"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType    *OutputAuthenticationMethodTcpjson `default:"manual" json:"authType"`
	Description *string                            `json:"description,omitempty"`
	// The hostname of the receiver
	Host *string `json:"host,omitempty"`
	// The port to connect to on the provided host
	Port *float64 `json:"port,omitempty"`
	// Exclude all IPs of the current host from the list of any resolved hostnames.
	ExcludeSelf *bool `default:"false" json:"excludeSelf"`
	// Set of hosts to load-balance data to.
	Hosts []HostTcpjson `json:"hosts,omitempty"`
	// Re-resolve any hostnames every this many seconds and pick up destinations from A records.
	DNSResolvePeriodSec *float64 `default:"600" json:"dnsResolvePeriodSec"`
	// How far back in time to keep traffic stats for load balancing purposes.
	LoadBalanceStatsPeriodSec *float64 `default:"300" json:"loadBalanceStatsPeriodSec"`
	// Maximum number of concurrent connections (per worker process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.
	MaxConcurrentSenders *float64 `default:"0" json:"maxConcurrentSenders"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *PqCompressCompressionTcpjson `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorTcpjson `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *OutputModeTcpjson `default:"error" json:"pqMode"`
	PqControls *PqControlsTcpjson `json:"pqControls,omitempty"`
	// Optional authentication token to include as part of the connection header
	AuthToken *string `default:"" json:"authToken"`
	// Select or create a stored text secret
	TextSecret *string   `json:"textSecret,omitempty"`
	Status     *TFStatus `json:"status,omitempty"`
}

func (o OutputTcpjson) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputTcpjson) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputTcpjson) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputTcpjson) GetType() OutputTypeTcpjson {
	if o == nil {
		return OutputTypeTcpjson("")
	}
	return o.Type
}

func (o *OutputTcpjson) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputTcpjson) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputTcpjson) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputTcpjson) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputTcpjson) GetLoadBalanced() *bool {
	if o == nil {
		return nil
	}
	return o.LoadBalanced
}

func (o *OutputTcpjson) GetCompression() *OutputCompressionTcpjson {
	if o == nil {
		return nil
	}
	return o.Compression
}

func (o *OutputTcpjson) GetLogFailedRequests() *bool {
	if o == nil {
		return nil
	}
	return o.LogFailedRequests
}

func (o *OutputTcpjson) GetThrottleRatePerSec() *string {
	if o == nil {
		return nil
	}
	return o.ThrottleRatePerSec
}

func (o *OutputTcpjson) GetTLS() *TLSSettingsClientSideTcpjson {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputTcpjson) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputTcpjson) GetWriteTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteTimeout
}

func (o *OutputTcpjson) GetTokenTTLMinutes() *float64 {
	if o == nil {
		return nil
	}
	return o.TokenTTLMinutes
}

func (o *OutputTcpjson) GetSendHeader() *bool {
	if o == nil {
		return nil
	}
	return o.SendHeader
}

func (o *OutputTcpjson) GetOnBackpressure() *BackpressureBehaviorTcpjson {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputTcpjson) GetAuthType() *OutputAuthenticationMethodTcpjson {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputTcpjson) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputTcpjson) GetHost() *string {
	if o == nil {
		return nil
	}
	return o.Host
}

func (o *OutputTcpjson) GetPort() *float64 {
	if o == nil {
		return nil
	}
	return o.Port
}

func (o *OutputTcpjson) GetExcludeSelf() *bool {
	if o == nil {
		return nil
	}
	return o.ExcludeSelf
}

func (o *OutputTcpjson) GetHosts() []HostTcpjson {
	if o == nil {
		return nil
	}
	return o.Hosts
}

func (o *OutputTcpjson) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputTcpjson) GetLoadBalanceStatsPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.LoadBalanceStatsPeriodSec
}

func (o *OutputTcpjson) GetMaxConcurrentSenders() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentSenders
}

func (o *OutputTcpjson) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputTcpjson) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputTcpjson) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputTcpjson) GetPqCompress() *PqCompressCompressionTcpjson {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputTcpjson) GetPqOnBackpressure() *QueueFullBehaviorTcpjson {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputTcpjson) GetPqMode() *OutputModeTcpjson {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputTcpjson) GetPqControls() *PqControlsTcpjson {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputTcpjson) GetAuthToken() *string {
	if o == nil {
		return nil
	}
	return o.AuthToken
}

func (o *OutputTcpjson) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputTcpjson) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type OutputTypeSplunkHec string

const (
	OutputTypeSplunkHecSplunkHec OutputTypeSplunkHec = "splunk_hec"
)

func (e OutputTypeSplunkHec) ToPointer() *OutputTypeSplunkHec {
	return &e
}
func (e *OutputTypeSplunkHec) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "splunk_hec":
		*e = OutputTypeSplunkHec(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeSplunkHec: %v", v)
	}
}

type ExtraHTTPHeaderSplunkHec struct {
	// Field name
	Name *string `json:"name,omitempty"`
	// Field value
	Value string `json:"value"`
}

func (o *ExtraHTTPHeaderSplunkHec) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderSplunkHec) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeSplunkHec - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeSplunkHec string

const (
	FailedRequestLoggingModeSplunkHecPayload           FailedRequestLoggingModeSplunkHec = "payload"
	FailedRequestLoggingModeSplunkHecPayloadAndHeaders FailedRequestLoggingModeSplunkHec = "payloadAndHeaders"
	FailedRequestLoggingModeSplunkHecNone              FailedRequestLoggingModeSplunkHec = "none"
)

func (e FailedRequestLoggingModeSplunkHec) ToPointer() *FailedRequestLoggingModeSplunkHec {
	return &e
}
func (e *FailedRequestLoggingModeSplunkHec) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeSplunkHec(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeSplunkHec: %v", v)
	}
}

// OutputAuthenticationMethodSplunkHec - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type OutputAuthenticationMethodSplunkHec string

const (
	OutputAuthenticationMethodSplunkHecManual OutputAuthenticationMethodSplunkHec = "manual"
	OutputAuthenticationMethodSplunkHecSecret OutputAuthenticationMethodSplunkHec = "secret"
)

func (e OutputAuthenticationMethodSplunkHec) ToPointer() *OutputAuthenticationMethodSplunkHec {
	return &e
}
func (e *OutputAuthenticationMethodSplunkHec) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		*e = OutputAuthenticationMethodSplunkHec(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputAuthenticationMethodSplunkHec: %v", v)
	}
}

type ResponseRetrySettingSplunkHec struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingSplunkHec) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingSplunkHec) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingSplunkHec) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingSplunkHec) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingSplunkHec) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingSplunkHec) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsSplunkHec struct {
	// Enable to retry on request timeout
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsSplunkHec) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsSplunkHec) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsSplunkHec) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsSplunkHec) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsSplunkHec) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsSplunkHec) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorSplunkHec - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorSplunkHec string

const (
	BackpressureBehaviorSplunkHecBlock BackpressureBehaviorSplunkHec = "block"
	BackpressureBehaviorSplunkHecDrop  BackpressureBehaviorSplunkHec = "drop"
	BackpressureBehaviorSplunkHecQueue BackpressureBehaviorSplunkHec = "queue"
)

func (e BackpressureBehaviorSplunkHec) ToPointer() *BackpressureBehaviorSplunkHec {
	return &e
}
func (e *BackpressureBehaviorSplunkHec) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorSplunkHec(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorSplunkHec: %v", v)
	}
}

type URLSplunkHec struct {
	// URL to a Splunk HEC endpoint to send events to, e.g., http://localhost:8088/services/collector/event
	URL *string `default:"http://localhost:8088/services/collector/event" json:"url"`
	// Assign a weight (>0) to each endpoint to indicate its traffic-handling capability
	Weight *float64 `default:"1" json:"weight"`
}

func (u URLSplunkHec) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(u, "", false)
}

func (u *URLSplunkHec) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &u, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *URLSplunkHec) GetURL() *string {
	if o == nil {
		return nil
	}
	return o.URL
}

func (o *URLSplunkHec) GetWeight() *float64 {
	if o == nil {
		return nil
	}
	return o.Weight
}

// PqCompressCompressionSplunkHec - Codec to use to compress the persisted data.
type PqCompressCompressionSplunkHec string

const (
	PqCompressCompressionSplunkHecNone PqCompressCompressionSplunkHec = "none"
	PqCompressCompressionSplunkHecGzip PqCompressCompressionSplunkHec = "gzip"
)

func (e PqCompressCompressionSplunkHec) ToPointer() *PqCompressCompressionSplunkHec {
	return &e
}
func (e *PqCompressCompressionSplunkHec) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = PqCompressCompressionSplunkHec(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PqCompressCompressionSplunkHec: %v", v)
	}
}

// QueueFullBehaviorSplunkHec - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSplunkHec string

const (
	QueueFullBehaviorSplunkHecBlock QueueFullBehaviorSplunkHec = "block"
	QueueFullBehaviorSplunkHecDrop  QueueFullBehaviorSplunkHec = "drop"
)

func (e QueueFullBehaviorSplunkHec) ToPointer() *QueueFullBehaviorSplunkHec {
	return &e
}
func (e *QueueFullBehaviorSplunkHec) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorSplunkHec(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorSplunkHec: %v", v)
	}
}

// OutputModeSplunkHec - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type OutputModeSplunkHec string

const (
	OutputModeSplunkHecError        OutputModeSplunkHec = "error"
	OutputModeSplunkHecBackpressure OutputModeSplunkHec = "backpressure"
	OutputModeSplunkHecAlways       OutputModeSplunkHec = "always"
)

func (e OutputModeSplunkHec) ToPointer() *OutputModeSplunkHec {
	return &e
}
func (e *OutputModeSplunkHec) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = OutputModeSplunkHec(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputModeSplunkHec: %v", v)
	}
}

type PqControlsSplunkHec struct {
}

type OutputSplunkHec struct {
	// Unique ID for this output
	ID   string              `json:"id"`
	Type OutputTypeSplunkHec `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.
	LoadBalanced *bool `default:"true" json:"loadBalanced"`
	// In the Splunk app, define which Splunk processing queue to send the events after HEC processing.
	NextQueue *string `default:"indexQueue" json:"nextQueue"`
	// In the Splunk app, set the value of _TCP_ROUTING for events that do not have _ctrl._TCP_ROUTING set.
	TCPRouting *string `default:"nowhere" json:"tcpRouting"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Defaults to Yes. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max body size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events.
	ExtraHTTPHeaders []ExtraHTTPHeaderSplunkHec `json:"extraHttpHeaders,omitempty"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeSplunkHec `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Output metrics in multiple-metric format, supported in Splunk 8.0 and above to allow multiple metrics in a single event.
	EnableMultiMetrics *bool `default:"false" json:"enableMultiMetrics"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType *OutputAuthenticationMethodSplunkHec `default:"manual" json:"authType"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable).
	ResponseRetrySettings []ResponseRetrySettingSplunkHec `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsSplunkHec  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorSplunkHec `default:"block" json:"onBackpressure"`
	Description    *string                        `json:"description,omitempty"`
	// URL to a Splunk HEC endpoint to send events to, e.g., http://localhost:8088/services/collector/event
	URL *string `default:"http://localhost:8088/services/collector/event" json:"url"`
	// Enables round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Exclude all IPs of the current host from the list of any resolved hostnames.
	ExcludeSelf *bool          `default:"false" json:"excludeSelf"`
	Urls        []URLSplunkHec `json:"urls,omitempty"`
	// Re-resolve any hostnames every this many seconds and pick up destinations from A records.
	DNSResolvePeriodSec *float64 `default:"600" json:"dnsResolvePeriodSec"`
	// How far back in time to keep traffic stats for load balancing purposes.
	LoadBalanceStatsPeriodSec *float64 `default:"300" json:"loadBalanceStatsPeriodSec"`
	// Splunk HEC authentication token
	Token *string `json:"token,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *PqCompressCompressionSplunkHec `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorSplunkHec `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *OutputModeSplunkHec `default:"error" json:"pqMode"`
	PqControls *PqControlsSplunkHec `json:"pqControls,omitempty"`
	Status     *TFStatus            `json:"status,omitempty"`
}

func (o OutputSplunkHec) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSplunkHec) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputSplunkHec) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputSplunkHec) GetType() OutputTypeSplunkHec {
	if o == nil {
		return OutputTypeSplunkHec("")
	}
	return o.Type
}

func (o *OutputSplunkHec) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSplunkHec) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSplunkHec) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSplunkHec) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSplunkHec) GetLoadBalanced() *bool {
	if o == nil {
		return nil
	}
	return o.LoadBalanced
}

func (o *OutputSplunkHec) GetNextQueue() *string {
	if o == nil {
		return nil
	}
	return o.NextQueue
}

func (o *OutputSplunkHec) GetTCPRouting() *string {
	if o == nil {
		return nil
	}
	return o.TCPRouting
}

func (o *OutputSplunkHec) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputSplunkHec) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputSplunkHec) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputSplunkHec) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputSplunkHec) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputSplunkHec) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputSplunkHec) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputSplunkHec) GetExtraHTTPHeaders() []ExtraHTTPHeaderSplunkHec {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputSplunkHec) GetFailedRequestLoggingMode() *FailedRequestLoggingModeSplunkHec {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputSplunkHec) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputSplunkHec) GetEnableMultiMetrics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableMultiMetrics
}

func (o *OutputSplunkHec) GetAuthType() *OutputAuthenticationMethodSplunkHec {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputSplunkHec) GetResponseRetrySettings() []ResponseRetrySettingSplunkHec {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputSplunkHec) GetTimeoutRetrySettings() *TimeoutRetrySettingsSplunkHec {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputSplunkHec) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputSplunkHec) GetOnBackpressure() *BackpressureBehaviorSplunkHec {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSplunkHec) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSplunkHec) GetURL() *string {
	if o == nil {
		return nil
	}
	return o.URL
}

func (o *OutputSplunkHec) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputSplunkHec) GetExcludeSelf() *bool {
	if o == nil {
		return nil
	}
	return o.ExcludeSelf
}

func (o *OutputSplunkHec) GetUrls() []URLSplunkHec {
	if o == nil {
		return nil
	}
	return o.Urls
}

func (o *OutputSplunkHec) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputSplunkHec) GetLoadBalanceStatsPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.LoadBalanceStatsPeriodSec
}

func (o *OutputSplunkHec) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputSplunkHec) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputSplunkHec) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSplunkHec) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSplunkHec) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSplunkHec) GetPqCompress() *PqCompressCompressionSplunkHec {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSplunkHec) GetPqOnBackpressure() *QueueFullBehaviorSplunkHec {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSplunkHec) GetPqMode() *OutputModeSplunkHec {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSplunkHec) GetPqControls() *PqControlsSplunkHec {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputSplunkHec) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeSplunkLb string

const (
	TypeSplunkLbSplunkLb TypeSplunkLb = "splunk_lb"
)

func (e TypeSplunkLb) ToPointer() *TypeSplunkLb {
	return &e
}
func (e *TypeSplunkLb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "splunk_lb":
		*e = TypeSplunkLb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeSplunkLb: %v", v)
	}
}

// NestedFieldSerializationSplunkLb - How to serialize nested fields into index-time fields
type NestedFieldSerializationSplunkLb string

const (
	NestedFieldSerializationSplunkLbJSON NestedFieldSerializationSplunkLb = "json"
	NestedFieldSerializationSplunkLbNone NestedFieldSerializationSplunkLb = "none"
)

func (e NestedFieldSerializationSplunkLb) ToPointer() *NestedFieldSerializationSplunkLb {
	return &e
}
func (e *NestedFieldSerializationSplunkLb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "json":
		fallthrough
	case "none":
		*e = NestedFieldSerializationSplunkLb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for NestedFieldSerializationSplunkLb: %v", v)
	}
}

// MinimumTLSVersionSplunkLb - Minimum TLS version to use when connecting
type MinimumTLSVersionSplunkLb string

const (
	MinimumTLSVersionSplunkLbTlSv1  MinimumTLSVersionSplunkLb = "TLSv1"
	MinimumTLSVersionSplunkLbTlSv11 MinimumTLSVersionSplunkLb = "TLSv1.1"
	MinimumTLSVersionSplunkLbTlSv12 MinimumTLSVersionSplunkLb = "TLSv1.2"
	MinimumTLSVersionSplunkLbTlSv13 MinimumTLSVersionSplunkLb = "TLSv1.3"
)

func (e MinimumTLSVersionSplunkLb) ToPointer() *MinimumTLSVersionSplunkLb {
	return &e
}
func (e *MinimumTLSVersionSplunkLb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = MinimumTLSVersionSplunkLb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for MinimumTLSVersionSplunkLb: %v", v)
	}
}

// MaximumTLSVersionSplunkLb - Maximum TLS version to use when connecting
type MaximumTLSVersionSplunkLb string

const (
	MaximumTLSVersionSplunkLbTlSv1  MaximumTLSVersionSplunkLb = "TLSv1"
	MaximumTLSVersionSplunkLbTlSv11 MaximumTLSVersionSplunkLb = "TLSv1.1"
	MaximumTLSVersionSplunkLbTlSv12 MaximumTLSVersionSplunkLb = "TLSv1.2"
	MaximumTLSVersionSplunkLbTlSv13 MaximumTLSVersionSplunkLb = "TLSv1.3"
)

func (e MaximumTLSVersionSplunkLb) ToPointer() *MaximumTLSVersionSplunkLb {
	return &e
}
func (e *MaximumTLSVersionSplunkLb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = MaximumTLSVersionSplunkLb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for MaximumTLSVersionSplunkLb: %v", v)
	}
}

type TLSSettingsClientSideSplunkLb struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certs that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (e.g., the system's CA). Defaults to Yes. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate.
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key.
	Passphrase *string `json:"passphrase,omitempty"`
	// Minimum TLS version to use when connecting
	MinVersion *MinimumTLSVersionSplunkLb `json:"minVersion,omitempty"`
	// Maximum TLS version to use when connecting
	MaxVersion *MaximumTLSVersionSplunkLb `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideSplunkLb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideSplunkLb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TLSSettingsClientSideSplunkLb) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *TLSSettingsClientSideSplunkLb) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *TLSSettingsClientSideSplunkLb) GetServername() *string {
	if o == nil {
		return nil
	}
	return o.Servername
}

func (o *TLSSettingsClientSideSplunkLb) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

func (o *TLSSettingsClientSideSplunkLb) GetCaPath() *string {
	if o == nil {
		return nil
	}
	return o.CaPath
}

func (o *TLSSettingsClientSideSplunkLb) GetPrivKeyPath() *string {
	if o == nil {
		return nil
	}
	return o.PrivKeyPath
}

func (o *TLSSettingsClientSideSplunkLb) GetCertPath() *string {
	if o == nil {
		return nil
	}
	return o.CertPath
}

func (o *TLSSettingsClientSideSplunkLb) GetPassphrase() *string {
	if o == nil {
		return nil
	}
	return o.Passphrase
}

func (o *TLSSettingsClientSideSplunkLb) GetMinVersion() *MinimumTLSVersionSplunkLb {
	if o == nil {
		return nil
	}
	return o.MinVersion
}

func (o *TLSSettingsClientSideSplunkLb) GetMaxVersion() *MaximumTLSVersionSplunkLb {
	if o == nil {
		return nil
	}
	return o.MaxVersion
}

// MaxS2SVersionSplunkLb - The highest S2S protocol version to advertise during handshake
type MaxS2SVersionSplunkLb string

const (
	MaxS2SVersionSplunkLbV3 MaxS2SVersionSplunkLb = "v3"
	MaxS2SVersionSplunkLbV4 MaxS2SVersionSplunkLb = "v4"
)

func (e MaxS2SVersionSplunkLb) ToPointer() *MaxS2SVersionSplunkLb {
	return &e
}
func (e *MaxS2SVersionSplunkLb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "v3":
		fallthrough
	case "v4":
		*e = MaxS2SVersionSplunkLb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for MaxS2SVersionSplunkLb: %v", v)
	}
}

// BackpressureBehaviorSplunkLb - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorSplunkLb string

const (
	BackpressureBehaviorSplunkLbBlock BackpressureBehaviorSplunkLb = "block"
	BackpressureBehaviorSplunkLbDrop  BackpressureBehaviorSplunkLb = "drop"
	BackpressureBehaviorSplunkLbQueue BackpressureBehaviorSplunkLb = "queue"
)

func (e BackpressureBehaviorSplunkLb) ToPointer() *BackpressureBehaviorSplunkLb {
	return &e
}
func (e *BackpressureBehaviorSplunkLb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorSplunkLb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorSplunkLb: %v", v)
	}
}

// AuthenticationMethodSplunkLb - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type AuthenticationMethodSplunkLb string

const (
	AuthenticationMethodSplunkLbManual AuthenticationMethodSplunkLb = "manual"
	AuthenticationMethodSplunkLbSecret AuthenticationMethodSplunkLb = "secret"
)

func (e AuthenticationMethodSplunkLb) ToPointer() *AuthenticationMethodSplunkLb {
	return &e
}
func (e *AuthenticationMethodSplunkLb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		*e = AuthenticationMethodSplunkLb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodSplunkLb: %v", v)
	}
}

// CompressCompressionSplunkLb - Controls whether the sender should send compressed data to the server. Select 'Disabled' to reject compressed connections or 'Always' to ignore server's configuration and send compressed data.
type CompressCompressionSplunkLb string

const (
	CompressCompressionSplunkLbDisabled CompressCompressionSplunkLb = "disabled"
	CompressCompressionSplunkLbAuto     CompressCompressionSplunkLb = "auto"
	CompressCompressionSplunkLbAlways   CompressCompressionSplunkLb = "always"
)

func (e CompressCompressionSplunkLb) ToPointer() *CompressCompressionSplunkLb {
	return &e
}
func (e *CompressCompressionSplunkLb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "disabled":
		fallthrough
	case "auto":
		fallthrough
	case "always":
		*e = CompressCompressionSplunkLb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressCompressionSplunkLb: %v", v)
	}
}

// IndexerDiscoveryConfigsAuthTokenAuthenticationMethod - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type IndexerDiscoveryConfigsAuthTokenAuthenticationMethod string

const (
	IndexerDiscoveryConfigsAuthTokenAuthenticationMethodManual IndexerDiscoveryConfigsAuthTokenAuthenticationMethod = "manual"
	IndexerDiscoveryConfigsAuthTokenAuthenticationMethodSecret IndexerDiscoveryConfigsAuthTokenAuthenticationMethod = "secret"
)

func (e IndexerDiscoveryConfigsAuthTokenAuthenticationMethod) ToPointer() *IndexerDiscoveryConfigsAuthTokenAuthenticationMethod {
	return &e
}
func (e *IndexerDiscoveryConfigsAuthTokenAuthenticationMethod) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		*e = IndexerDiscoveryConfigsAuthTokenAuthenticationMethod(v)
		return nil
	default:
		return fmt.Errorf("invalid value for IndexerDiscoveryConfigsAuthTokenAuthenticationMethod: %v", v)
	}
}

type OutputAuthToken struct {
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType *IndexerDiscoveryConfigsAuthTokenAuthenticationMethod `default:"manual" json:"authType"`
}

func (o OutputAuthToken) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputAuthToken) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *OutputAuthToken) GetAuthType() *IndexerDiscoveryConfigsAuthTokenAuthenticationMethod {
	if o == nil {
		return nil
	}
	return o.AuthType
}

// IndexerDiscoveryConfigsAuthenticationMethod - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type IndexerDiscoveryConfigsAuthenticationMethod string

const (
	IndexerDiscoveryConfigsAuthenticationMethodManual IndexerDiscoveryConfigsAuthenticationMethod = "manual"
	IndexerDiscoveryConfigsAuthenticationMethodSecret IndexerDiscoveryConfigsAuthenticationMethod = "secret"
)

func (e IndexerDiscoveryConfigsAuthenticationMethod) ToPointer() *IndexerDiscoveryConfigsAuthenticationMethod {
	return &e
}
func (e *IndexerDiscoveryConfigsAuthenticationMethod) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		*e = IndexerDiscoveryConfigsAuthenticationMethod(v)
		return nil
	default:
		return fmt.Errorf("invalid value for IndexerDiscoveryConfigsAuthenticationMethod: %v", v)
	}
}

// IndexerDiscoveryConfigs - List of configurations to set up indexer discovery in Splunk Indexer clustering environment.
type IndexerDiscoveryConfigs struct {
	// Clustering site of the indexers from where indexers need to be discovered. In case of single site cluster, it defaults to 'default' site.
	Site *string `default:"default" json:"site"`
	// Full URI of Splunk cluster manager (scheme://host:port). Example: https://managerAddress:8089
	MasterURI string `json:"masterUri"`
	// Time interval, in seconds, between two consecutive indexer list fetches from cluster manager
	RefreshIntervalSec *float64 `default:"300" json:"refreshIntervalSec"`
	// During indexer discovery, reject cluster manager certificates that are not authorized by the system's CA. Disable to allow untrusted (for example, self-signed) certificates.
	RejectUnauthorized *bool `default:"false" json:"rejectUnauthorized"`
	// Tokens required to authenticate to cluster manager for indexer discovery
	AuthTokens []OutputAuthToken `json:"authTokens,omitempty"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType *IndexerDiscoveryConfigsAuthenticationMethod `default:"manual" json:"authType"`
	// Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.
	AuthToken *string `default:"" json:"authToken"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
}

func (i IndexerDiscoveryConfigs) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(i, "", false)
}

func (i *IndexerDiscoveryConfigs) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &i, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *IndexerDiscoveryConfigs) GetSite() *string {
	if o == nil {
		return nil
	}
	return o.Site
}

func (o *IndexerDiscoveryConfigs) GetMasterURI() string {
	if o == nil {
		return ""
	}
	return o.MasterURI
}

func (o *IndexerDiscoveryConfigs) GetRefreshIntervalSec() *float64 {
	if o == nil {
		return nil
	}
	return o.RefreshIntervalSec
}

func (o *IndexerDiscoveryConfigs) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *IndexerDiscoveryConfigs) GetAuthTokens() []OutputAuthToken {
	if o == nil {
		return nil
	}
	return o.AuthTokens
}

func (o *IndexerDiscoveryConfigs) GetAuthType() *IndexerDiscoveryConfigsAuthenticationMethod {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *IndexerDiscoveryConfigs) GetAuthToken() *string {
	if o == nil {
		return nil
	}
	return o.AuthToken
}

func (o *IndexerDiscoveryConfigs) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

// TLSSplunkLb - Whether to inherit TLS configs from group setting or disable TLS.
type TLSSplunkLb string

const (
	TLSSplunkLbInherit TLSSplunkLb = "inherit"
	TLSSplunkLbFalse   TLSSplunkLb = "false"
)

func (e TLSSplunkLb) ToPointer() *TLSSplunkLb {
	return &e
}
func (e *TLSSplunkLb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "inherit":
		fallthrough
	case "false":
		*e = TLSSplunkLb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TLSSplunkLb: %v", v)
	}
}

type HostSplunkLb struct {
	// The hostname of the receiver.
	Host string `json:"host"`
	// The port to connect to on the provided host.
	Port *float64 `default:"9997" json:"port"`
	// Whether to inherit TLS configs from group setting or disable TLS.
	TLS *TLSSplunkLb `default:"inherit" json:"tls"`
	// Servername to use if establishing a TLS connection. If not specified, defaults to connection host (iff not an IP); otherwise, to the global TLS settings.
	Servername *string `json:"servername,omitempty"`
	// Assign a weight (>0) to each endpoint to indicate its traffic-handling capability
	Weight *float64 `default:"1" json:"weight"`
}

func (h HostSplunkLb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(h, "", false)
}

func (h *HostSplunkLb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &h, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *HostSplunkLb) GetHost() string {
	if o == nil {
		return ""
	}
	return o.Host
}

func (o *HostSplunkLb) GetPort() *float64 {
	if o == nil {
		return nil
	}
	return o.Port
}

func (o *HostSplunkLb) GetTLS() *TLSSplunkLb {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *HostSplunkLb) GetServername() *string {
	if o == nil {
		return nil
	}
	return o.Servername
}

func (o *HostSplunkLb) GetWeight() *float64 {
	if o == nil {
		return nil
	}
	return o.Weight
}

// PqCompressCompressionSplunkLb - Codec to use to compress the persisted data.
type PqCompressCompressionSplunkLb string

const (
	PqCompressCompressionSplunkLbNone PqCompressCompressionSplunkLb = "none"
	PqCompressCompressionSplunkLbGzip PqCompressCompressionSplunkLb = "gzip"
)

func (e PqCompressCompressionSplunkLb) ToPointer() *PqCompressCompressionSplunkLb {
	return &e
}
func (e *PqCompressCompressionSplunkLb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = PqCompressCompressionSplunkLb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PqCompressCompressionSplunkLb: %v", v)
	}
}

// QueueFullBehaviorSplunkLb - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSplunkLb string

const (
	QueueFullBehaviorSplunkLbBlock QueueFullBehaviorSplunkLb = "block"
	QueueFullBehaviorSplunkLbDrop  QueueFullBehaviorSplunkLb = "drop"
)

func (e QueueFullBehaviorSplunkLb) ToPointer() *QueueFullBehaviorSplunkLb {
	return &e
}
func (e *QueueFullBehaviorSplunkLb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorSplunkLb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorSplunkLb: %v", v)
	}
}

// ModeSplunkLb - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type ModeSplunkLb string

const (
	ModeSplunkLbError        ModeSplunkLb = "error"
	ModeSplunkLbBackpressure ModeSplunkLb = "backpressure"
	ModeSplunkLbAlways       ModeSplunkLb = "always"
)

func (e ModeSplunkLb) ToPointer() *ModeSplunkLb {
	return &e
}
func (e *ModeSplunkLb) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeSplunkLb(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeSplunkLb: %v", v)
	}
}

type PqControlsSplunkLb struct {
}

type OutputSplunkLb struct {
	// Unique ID for this output
	ID   *string      `json:"id,omitempty"`
	Type TypeSplunkLb `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Re-resolve any hostnames every this many seconds and pick up destinations from A records.
	DNSResolvePeriodSec *float64 `default:"600" json:"dnsResolvePeriodSec"`
	// How far back in time to keep traffic stats for load balancing purposes.
	LoadBalanceStatsPeriodSec *float64 `default:"300" json:"loadBalanceStatsPeriodSec"`
	// Maximum number of concurrent connections (per worker process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.
	MaxConcurrentSenders *float64 `default:"0" json:"maxConcurrentSenders"`
	// How to serialize nested fields into index-time fields
	NestedFields *NestedFieldSerializationSplunkLb `default:"none" json:"nestedFields"`
	// Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
	ThrottleRatePerSec *string `default:"0" json:"throttleRatePerSec"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead
	WriteTimeout *float64                       `default:"60000" json:"writeTimeout"`
	TLS          *TLSSettingsClientSideSplunkLb `json:"tls,omitempty"`
	// Output metrics in multiple-metric format in a single event. Supported in Splunk 8.0 and above.
	EnableMultiMetrics *bool `default:"false" json:"enableMultiMetrics"`
	// Check if indexer is shutting down and stop sending data. This helps minimize data loss during shutdown.
	EnableACK *bool `default:"true" json:"enableACK"`
	// Use to troubleshoot issues with sending data
	LogFailedRequests *bool `default:"false" json:"logFailedRequests"`
	// The highest S2S protocol version to advertise during handshake
	MaxS2Sversion *MaxS2SVersionSplunkLb `default:"v3" json:"maxS2Sversion"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorSplunkLb `default:"block" json:"onBackpressure"`
	// Automatically discover indexers in indexer clustering environment.
	IndexerDiscovery *bool `default:"false" json:"indexerDiscovery"`
	// How long (in milliseconds) each LB endpoint can report blocked before the Destination reports unhealthy, blocking the sender. (Grace period for fluctuations.) Use 0 to disable; max 1 minute.
	SenderUnhealthyTimeAllowance *float64 `default:"100" json:"senderUnhealthyTimeAllowance"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType    *AuthenticationMethodSplunkLb `default:"manual" json:"authType"`
	Description *string                       `json:"description,omitempty"`
	// Maximum number of times healthcheck can fail before we close connection. If set to 0 (disabled), and the connection to Splunk is forcibly closed, some data loss might occur.
	MaxFailedHealthChecks *float64 `default:"1" json:"maxFailedHealthChecks"`
	// Controls whether the sender should send compressed data to the server. Select 'Disabled' to reject compressed connections or 'Always' to ignore server's configuration and send compressed data.
	Compress *CompressCompressionSplunkLb `default:"disabled" json:"compress"`
	// List of configurations to set up indexer discovery in Splunk Indexer clustering environment.
	IndexerDiscoveryConfigs *IndexerDiscoveryConfigs `json:"indexerDiscoveryConfigs,omitempty"`
	// Exclude all IPs of the current host from the list of any resolved hostnames.
	ExcludeSelf *bool `default:"false" json:"excludeSelf"`
	// Set of Splunk indexers to load-balance data to.
	Hosts []HostSplunkLb `json:"hosts"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *PqCompressCompressionSplunkLb `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorSplunkLb `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeSplunkLb       `default:"error" json:"pqMode"`
	PqControls *PqControlsSplunkLb `json:"pqControls,omitempty"`
	// Shared secret token to use when establishing a connection to a Splunk indexer.
	AuthToken *string `default:"" json:"authToken"`
	// Select or create a stored text secret
	TextSecret *string   `json:"textSecret,omitempty"`
	Status     *TFStatus `json:"status,omitempty"`
}

func (o OutputSplunkLb) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSplunkLb) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputSplunkLb) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputSplunkLb) GetType() TypeSplunkLb {
	if o == nil {
		return TypeSplunkLb("")
	}
	return o.Type
}

func (o *OutputSplunkLb) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSplunkLb) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSplunkLb) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSplunkLb) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSplunkLb) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputSplunkLb) GetLoadBalanceStatsPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.LoadBalanceStatsPeriodSec
}

func (o *OutputSplunkLb) GetMaxConcurrentSenders() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentSenders
}

func (o *OutputSplunkLb) GetNestedFields() *NestedFieldSerializationSplunkLb {
	if o == nil {
		return nil
	}
	return o.NestedFields
}

func (o *OutputSplunkLb) GetThrottleRatePerSec() *string {
	if o == nil {
		return nil
	}
	return o.ThrottleRatePerSec
}

func (o *OutputSplunkLb) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputSplunkLb) GetWriteTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteTimeout
}

func (o *OutputSplunkLb) GetTLS() *TLSSettingsClientSideSplunkLb {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputSplunkLb) GetEnableMultiMetrics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableMultiMetrics
}

func (o *OutputSplunkLb) GetEnableACK() *bool {
	if o == nil {
		return nil
	}
	return o.EnableACK
}

func (o *OutputSplunkLb) GetLogFailedRequests() *bool {
	if o == nil {
		return nil
	}
	return o.LogFailedRequests
}

func (o *OutputSplunkLb) GetMaxS2Sversion() *MaxS2SVersionSplunkLb {
	if o == nil {
		return nil
	}
	return o.MaxS2Sversion
}

func (o *OutputSplunkLb) GetOnBackpressure() *BackpressureBehaviorSplunkLb {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSplunkLb) GetIndexerDiscovery() *bool {
	if o == nil {
		return nil
	}
	return o.IndexerDiscovery
}

func (o *OutputSplunkLb) GetSenderUnhealthyTimeAllowance() *float64 {
	if o == nil {
		return nil
	}
	return o.SenderUnhealthyTimeAllowance
}

func (o *OutputSplunkLb) GetAuthType() *AuthenticationMethodSplunkLb {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputSplunkLb) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSplunkLb) GetMaxFailedHealthChecks() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFailedHealthChecks
}

func (o *OutputSplunkLb) GetCompress() *CompressCompressionSplunkLb {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputSplunkLb) GetIndexerDiscoveryConfigs() *IndexerDiscoveryConfigs {
	if o == nil {
		return nil
	}
	return o.IndexerDiscoveryConfigs
}

func (o *OutputSplunkLb) GetExcludeSelf() *bool {
	if o == nil {
		return nil
	}
	return o.ExcludeSelf
}

func (o *OutputSplunkLb) GetHosts() []HostSplunkLb {
	if o == nil {
		return []HostSplunkLb{}
	}
	return o.Hosts
}

func (o *OutputSplunkLb) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSplunkLb) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSplunkLb) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSplunkLb) GetPqCompress() *PqCompressCompressionSplunkLb {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSplunkLb) GetPqOnBackpressure() *QueueFullBehaviorSplunkLb {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSplunkLb) GetPqMode() *ModeSplunkLb {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSplunkLb) GetPqControls() *PqControlsSplunkLb {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputSplunkLb) GetAuthToken() *string {
	if o == nil {
		return nil
	}
	return o.AuthToken
}

func (o *OutputSplunkLb) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputSplunkLb) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type OutputTypeSplunk string

const (
	OutputTypeSplunkSplunk OutputTypeSplunk = "splunk"
)

func (e OutputTypeSplunk) ToPointer() *OutputTypeSplunk {
	return &e
}
func (e *OutputTypeSplunk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "splunk":
		*e = OutputTypeSplunk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeSplunk: %v", v)
	}
}

// NestedFieldSerializationSplunk - How to serialize nested fields into index-time fields
type NestedFieldSerializationSplunk string

const (
	NestedFieldSerializationSplunkJSON NestedFieldSerializationSplunk = "json"
	NestedFieldSerializationSplunkNone NestedFieldSerializationSplunk = "none"
)

func (e NestedFieldSerializationSplunk) ToPointer() *NestedFieldSerializationSplunk {
	return &e
}
func (e *NestedFieldSerializationSplunk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "json":
		fallthrough
	case "none":
		*e = NestedFieldSerializationSplunk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for NestedFieldSerializationSplunk: %v", v)
	}
}

// OutputMinimumTLSVersionSplunk - Minimum TLS version to use when connecting
type OutputMinimumTLSVersionSplunk string

const (
	OutputMinimumTLSVersionSplunkTlSv1  OutputMinimumTLSVersionSplunk = "TLSv1"
	OutputMinimumTLSVersionSplunkTlSv11 OutputMinimumTLSVersionSplunk = "TLSv1.1"
	OutputMinimumTLSVersionSplunkTlSv12 OutputMinimumTLSVersionSplunk = "TLSv1.2"
	OutputMinimumTLSVersionSplunkTlSv13 OutputMinimumTLSVersionSplunk = "TLSv1.3"
)

func (e OutputMinimumTLSVersionSplunk) ToPointer() *OutputMinimumTLSVersionSplunk {
	return &e
}
func (e *OutputMinimumTLSVersionSplunk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = OutputMinimumTLSVersionSplunk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMinimumTLSVersionSplunk: %v", v)
	}
}

// OutputMaximumTLSVersionSplunk - Maximum TLS version to use when connecting
type OutputMaximumTLSVersionSplunk string

const (
	OutputMaximumTLSVersionSplunkTlSv1  OutputMaximumTLSVersionSplunk = "TLSv1"
	OutputMaximumTLSVersionSplunkTlSv11 OutputMaximumTLSVersionSplunk = "TLSv1.1"
	OutputMaximumTLSVersionSplunkTlSv12 OutputMaximumTLSVersionSplunk = "TLSv1.2"
	OutputMaximumTLSVersionSplunkTlSv13 OutputMaximumTLSVersionSplunk = "TLSv1.3"
)

func (e OutputMaximumTLSVersionSplunk) ToPointer() *OutputMaximumTLSVersionSplunk {
	return &e
}
func (e *OutputMaximumTLSVersionSplunk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = OutputMaximumTLSVersionSplunk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMaximumTLSVersionSplunk: %v", v)
	}
}

type TLSSettingsClientSideSplunk struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certs that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (e.g., the system's CA). Defaults to Yes. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate.
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key.
	Passphrase *string `json:"passphrase,omitempty"`
	// Minimum TLS version to use when connecting
	MinVersion *OutputMinimumTLSVersionSplunk `json:"minVersion,omitempty"`
	// Maximum TLS version to use when connecting
	MaxVersion *OutputMaximumTLSVersionSplunk `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideSplunk) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideSplunk) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TLSSettingsClientSideSplunk) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *TLSSettingsClientSideSplunk) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *TLSSettingsClientSideSplunk) GetServername() *string {
	if o == nil {
		return nil
	}
	return o.Servername
}

func (o *TLSSettingsClientSideSplunk) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

func (o *TLSSettingsClientSideSplunk) GetCaPath() *string {
	if o == nil {
		return nil
	}
	return o.CaPath
}

func (o *TLSSettingsClientSideSplunk) GetPrivKeyPath() *string {
	if o == nil {
		return nil
	}
	return o.PrivKeyPath
}

func (o *TLSSettingsClientSideSplunk) GetCertPath() *string {
	if o == nil {
		return nil
	}
	return o.CertPath
}

func (o *TLSSettingsClientSideSplunk) GetPassphrase() *string {
	if o == nil {
		return nil
	}
	return o.Passphrase
}

func (o *TLSSettingsClientSideSplunk) GetMinVersion() *OutputMinimumTLSVersionSplunk {
	if o == nil {
		return nil
	}
	return o.MinVersion
}

func (o *TLSSettingsClientSideSplunk) GetMaxVersion() *OutputMaximumTLSVersionSplunk {
	if o == nil {
		return nil
	}
	return o.MaxVersion
}

// OutputMaxS2SVersionSplunk - The highest S2S protocol version to advertise during handshake
type OutputMaxS2SVersionSplunk string

const (
	OutputMaxS2SVersionSplunkV3 OutputMaxS2SVersionSplunk = "v3"
	OutputMaxS2SVersionSplunkV4 OutputMaxS2SVersionSplunk = "v4"
)

func (e OutputMaxS2SVersionSplunk) ToPointer() *OutputMaxS2SVersionSplunk {
	return &e
}
func (e *OutputMaxS2SVersionSplunk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "v3":
		fallthrough
	case "v4":
		*e = OutputMaxS2SVersionSplunk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputMaxS2SVersionSplunk: %v", v)
	}
}

// BackpressureBehaviorSplunk - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorSplunk string

const (
	BackpressureBehaviorSplunkBlock BackpressureBehaviorSplunk = "block"
	BackpressureBehaviorSplunkDrop  BackpressureBehaviorSplunk = "drop"
	BackpressureBehaviorSplunkQueue BackpressureBehaviorSplunk = "queue"
)

func (e BackpressureBehaviorSplunk) ToPointer() *BackpressureBehaviorSplunk {
	return &e
}
func (e *BackpressureBehaviorSplunk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorSplunk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorSplunk: %v", v)
	}
}

// AuthenticationMethodSplunk - Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
type AuthenticationMethodSplunk string

const (
	AuthenticationMethodSplunkManual AuthenticationMethodSplunk = "manual"
	AuthenticationMethodSplunkSecret AuthenticationMethodSplunk = "secret"
)

func (e AuthenticationMethodSplunk) ToPointer() *AuthenticationMethodSplunk {
	return &e
}
func (e *AuthenticationMethodSplunk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		fallthrough
	case "secret":
		*e = AuthenticationMethodSplunk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationMethodSplunk: %v", v)
	}
}

// OutputCompressCompressionSplunk - Controls whether the sender should send compressed data to the server. Select 'Disabled' to reject compressed connections or 'Always' to ignore server's configuration and send compressed data.
type OutputCompressCompressionSplunk string

const (
	OutputCompressCompressionSplunkDisabled OutputCompressCompressionSplunk = "disabled"
	OutputCompressCompressionSplunkAuto     OutputCompressCompressionSplunk = "auto"
	OutputCompressCompressionSplunkAlways   OutputCompressCompressionSplunk = "always"
)

func (e OutputCompressCompressionSplunk) ToPointer() *OutputCompressCompressionSplunk {
	return &e
}
func (e *OutputCompressCompressionSplunk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "disabled":
		fallthrough
	case "auto":
		fallthrough
	case "always":
		*e = OutputCompressCompressionSplunk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputCompressCompressionSplunk: %v", v)
	}
}

// PqCompressCompressionSplunk - Codec to use to compress the persisted data.
type PqCompressCompressionSplunk string

const (
	PqCompressCompressionSplunkNone PqCompressCompressionSplunk = "none"
	PqCompressCompressionSplunkGzip PqCompressCompressionSplunk = "gzip"
)

func (e PqCompressCompressionSplunk) ToPointer() *PqCompressCompressionSplunk {
	return &e
}
func (e *PqCompressCompressionSplunk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = PqCompressCompressionSplunk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PqCompressCompressionSplunk: %v", v)
	}
}

// QueueFullBehaviorSplunk - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSplunk string

const (
	QueueFullBehaviorSplunkBlock QueueFullBehaviorSplunk = "block"
	QueueFullBehaviorSplunkDrop  QueueFullBehaviorSplunk = "drop"
)

func (e QueueFullBehaviorSplunk) ToPointer() *QueueFullBehaviorSplunk {
	return &e
}
func (e *QueueFullBehaviorSplunk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorSplunk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorSplunk: %v", v)
	}
}

// OutputModeSplunk - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type OutputModeSplunk string

const (
	OutputModeSplunkError        OutputModeSplunk = "error"
	OutputModeSplunkBackpressure OutputModeSplunk = "backpressure"
	OutputModeSplunkAlways       OutputModeSplunk = "always"
)

func (e OutputModeSplunk) ToPointer() *OutputModeSplunk {
	return &e
}
func (e *OutputModeSplunk) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = OutputModeSplunk(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputModeSplunk: %v", v)
	}
}

type PqControlsSplunk struct {
}

type OutputSplunk struct {
	// Unique ID for this output
	ID   *string           `json:"id,omitempty"`
	Type *OutputTypeSplunk `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The hostname of the receiver
	Host string `json:"host"`
	// The port to connect to on the provided host
	Port *float64 `default:"9997" json:"port"`
	// How to serialize nested fields into index-time fields
	NestedFields *NestedFieldSerializationSplunk `default:"none" json:"nestedFields"`
	// Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
	ThrottleRatePerSec *string `default:"0" json:"throttleRatePerSec"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead
	WriteTimeout *float64                     `default:"60000" json:"writeTimeout"`
	TLS          *TLSSettingsClientSideSplunk `json:"tls,omitempty"`
	// Output metrics in multiple-metric format in a single event. Supported in Splunk 8.0 and above.
	EnableMultiMetrics *bool `default:"false" json:"enableMultiMetrics"`
	// Check if indexer is shutting down and stop sending data. This helps minimize data loss during shutdown.
	EnableACK *bool `default:"true" json:"enableACK"`
	// Use to troubleshoot issues with sending data
	LogFailedRequests *bool `default:"false" json:"logFailedRequests"`
	// The highest S2S protocol version to advertise during handshake
	MaxS2Sversion *OutputMaxS2SVersionSplunk `default:"v3" json:"maxS2Sversion"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorSplunk `default:"block" json:"onBackpressure"`
	// Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
	AuthType    *AuthenticationMethodSplunk `default:"manual" json:"authType"`
	Description *string                     `json:"description,omitempty"`
	// Maximum number of times healthcheck can fail before we close connection. If set to 0 (disabled), and the connection to Splunk is forcibly closed, some data loss might occur.
	MaxFailedHealthChecks *float64 `default:"1" json:"maxFailedHealthChecks"`
	// Controls whether the sender should send compressed data to the server. Select 'Disabled' to reject compressed connections or 'Always' to ignore server's configuration and send compressed data.
	Compress *OutputCompressCompressionSplunk `default:"disabled" json:"compress"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *PqCompressCompressionSplunk `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorSplunk `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *OutputModeSplunk `default:"error" json:"pqMode"`
	PqControls *PqControlsSplunk `json:"pqControls,omitempty"`
	// Shared secret token to use when establishing a connection to a Splunk indexer.
	AuthToken *string `default:"" json:"authToken"`
	// Select or create a stored text secret
	TextSecret *string   `json:"textSecret,omitempty"`
	Status     *TFStatus `json:"status,omitempty"`
}

func (o OutputSplunk) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSplunk) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputSplunk) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputSplunk) GetType() *OutputTypeSplunk {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputSplunk) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSplunk) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSplunk) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSplunk) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSplunk) GetHost() string {
	if o == nil {
		return ""
	}
	return o.Host
}

func (o *OutputSplunk) GetPort() *float64 {
	if o == nil {
		return nil
	}
	return o.Port
}

func (o *OutputSplunk) GetNestedFields() *NestedFieldSerializationSplunk {
	if o == nil {
		return nil
	}
	return o.NestedFields
}

func (o *OutputSplunk) GetThrottleRatePerSec() *string {
	if o == nil {
		return nil
	}
	return o.ThrottleRatePerSec
}

func (o *OutputSplunk) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputSplunk) GetWriteTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteTimeout
}

func (o *OutputSplunk) GetTLS() *TLSSettingsClientSideSplunk {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputSplunk) GetEnableMultiMetrics() *bool {
	if o == nil {
		return nil
	}
	return o.EnableMultiMetrics
}

func (o *OutputSplunk) GetEnableACK() *bool {
	if o == nil {
		return nil
	}
	return o.EnableACK
}

func (o *OutputSplunk) GetLogFailedRequests() *bool {
	if o == nil {
		return nil
	}
	return o.LogFailedRequests
}

func (o *OutputSplunk) GetMaxS2Sversion() *OutputMaxS2SVersionSplunk {
	if o == nil {
		return nil
	}
	return o.MaxS2Sversion
}

func (o *OutputSplunk) GetOnBackpressure() *BackpressureBehaviorSplunk {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSplunk) GetAuthType() *AuthenticationMethodSplunk {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputSplunk) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSplunk) GetMaxFailedHealthChecks() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFailedHealthChecks
}

func (o *OutputSplunk) GetCompress() *OutputCompressCompressionSplunk {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputSplunk) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSplunk) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSplunk) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSplunk) GetPqCompress() *PqCompressCompressionSplunk {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSplunk) GetPqOnBackpressure() *QueueFullBehaviorSplunk {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSplunk) GetPqMode() *OutputModeSplunk {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSplunk) GetPqControls() *PqControlsSplunk {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputSplunk) GetAuthToken() *string {
	if o == nil {
		return nil
	}
	return o.AuthToken
}

func (o *OutputSplunk) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputSplunk) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type OutputTypeSyslog string

const (
	OutputTypeSyslogSyslog OutputTypeSyslog = "syslog"
)

func (e OutputTypeSyslog) ToPointer() *OutputTypeSyslog {
	return &e
}
func (e *OutputTypeSyslog) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "syslog":
		*e = OutputTypeSyslog(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputTypeSyslog: %v", v)
	}
}

// ProtocolSyslog - The network protocol to use for sending out syslog messages
type ProtocolSyslog string

const (
	ProtocolSyslogTCP ProtocolSyslog = "tcp"
	ProtocolSyslogUDP ProtocolSyslog = "udp"
)

func (e ProtocolSyslog) ToPointer() *ProtocolSyslog {
	return &e
}
func (e *ProtocolSyslog) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "tcp":
		fallthrough
	case "udp":
		*e = ProtocolSyslog(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ProtocolSyslog: %v", v)
	}
}

// Facility - Default value for message facility. Will be overwritten by value of __facility if set. Defaults to user.
type Facility int64

const (
	FacilityZero      Facility = 0
	FacilityOne       Facility = 1
	FacilityTwo       Facility = 2
	FacilityThree     Facility = 3
	FacilityFour      Facility = 4
	FacilityFive      Facility = 5
	FacilitySix       Facility = 6
	FacilitySeven     Facility = 7
	FacilityEight     Facility = 8
	FacilityNine      Facility = 9
	FacilityTen       Facility = 10
	FacilityEleven    Facility = 11
	FacilityTwelve    Facility = 12
	FacilityThirteen  Facility = 13
	FacilityFourteen  Facility = 14
	FacilityFifteen   Facility = 15
	FacilitySixteen   Facility = 16
	FacilitySeventeen Facility = 17
	FacilityEighteen  Facility = 18
	FacilityNineteen  Facility = 19
	FacilityTwenty    Facility = 20
	FacilityTwentyOne Facility = 21
)

func (e Facility) ToPointer() *Facility {
	return &e
}
func (e *Facility) UnmarshalJSON(data []byte) error {
	var v int64
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case 0:
		fallthrough
	case 1:
		fallthrough
	case 2:
		fallthrough
	case 3:
		fallthrough
	case 4:
		fallthrough
	case 5:
		fallthrough
	case 6:
		fallthrough
	case 7:
		fallthrough
	case 8:
		fallthrough
	case 9:
		fallthrough
	case 10:
		fallthrough
	case 11:
		fallthrough
	case 12:
		fallthrough
	case 13:
		fallthrough
	case 14:
		fallthrough
	case 15:
		fallthrough
	case 16:
		fallthrough
	case 17:
		fallthrough
	case 18:
		fallthrough
	case 19:
		fallthrough
	case 20:
		fallthrough
	case 21:
		*e = Facility(v)
		return nil
	default:
		return fmt.Errorf("invalid value for Facility: %v", v)
	}
}

// SeveritySyslog - Default value for message severity. Will be overwritten by value of __severity if set. Defaults to notice.
type SeveritySyslog int64

const (
	SeveritySyslogZero  SeveritySyslog = 0
	SeveritySyslogOne   SeveritySyslog = 1
	SeveritySyslogTwo   SeveritySyslog = 2
	SeveritySyslogThree SeveritySyslog = 3
	SeveritySyslogFour  SeveritySyslog = 4
	SeveritySyslogFive  SeveritySyslog = 5
	SeveritySyslogSix   SeveritySyslog = 6
	SeveritySyslogSeven SeveritySyslog = 7
)

func (e SeveritySyslog) ToPointer() *SeveritySyslog {
	return &e
}
func (e *SeveritySyslog) UnmarshalJSON(data []byte) error {
	var v int64
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case 0:
		fallthrough
	case 1:
		fallthrough
	case 2:
		fallthrough
	case 3:
		fallthrough
	case 4:
		fallthrough
	case 5:
		fallthrough
	case 6:
		fallthrough
	case 7:
		*e = SeveritySyslog(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SeveritySyslog: %v", v)
	}
}

// MessageFormatSyslog - The syslog message format depending on the receiver's support
type MessageFormatSyslog string

const (
	MessageFormatSyslogRfc3164 MessageFormatSyslog = "rfc3164"
	MessageFormatSyslogRfc5424 MessageFormatSyslog = "rfc5424"
)

func (e MessageFormatSyslog) ToPointer() *MessageFormatSyslog {
	return &e
}
func (e *MessageFormatSyslog) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "rfc3164":
		fallthrough
	case "rfc5424":
		*e = MessageFormatSyslog(v)
		return nil
	default:
		return fmt.Errorf("invalid value for MessageFormatSyslog: %v", v)
	}
}

// TimestampFormatEnum - Timestamp format to use when serializing event's time field
type TimestampFormatEnum string

const (
	TimestampFormatEnumSyslog  TimestampFormatEnum = "syslog"
	TimestampFormatEnumIso8601 TimestampFormatEnum = "iso8601"
)

func (e TimestampFormatEnum) ToPointer() *TimestampFormatEnum {
	return &e
}
func (e *TimestampFormatEnum) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "syslog":
		fallthrough
	case "iso8601":
		*e = TimestampFormatEnum(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TimestampFormatEnum: %v", v)
	}
}

// MinimumTLSVersionSyslog - Minimum TLS version to use when connecting
type MinimumTLSVersionSyslog string

const (
	MinimumTLSVersionSyslogTlSv1  MinimumTLSVersionSyslog = "TLSv1"
	MinimumTLSVersionSyslogTlSv11 MinimumTLSVersionSyslog = "TLSv1.1"
	MinimumTLSVersionSyslogTlSv12 MinimumTLSVersionSyslog = "TLSv1.2"
	MinimumTLSVersionSyslogTlSv13 MinimumTLSVersionSyslog = "TLSv1.3"
)

func (e MinimumTLSVersionSyslog) ToPointer() *MinimumTLSVersionSyslog {
	return &e
}
func (e *MinimumTLSVersionSyslog) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = MinimumTLSVersionSyslog(v)
		return nil
	default:
		return fmt.Errorf("invalid value for MinimumTLSVersionSyslog: %v", v)
	}
}

// MaximumTLSVersionSyslog - Maximum TLS version to use when connecting
type MaximumTLSVersionSyslog string

const (
	MaximumTLSVersionSyslogTlSv1  MaximumTLSVersionSyslog = "TLSv1"
	MaximumTLSVersionSyslogTlSv11 MaximumTLSVersionSyslog = "TLSv1.1"
	MaximumTLSVersionSyslogTlSv12 MaximumTLSVersionSyslog = "TLSv1.2"
	MaximumTLSVersionSyslogTlSv13 MaximumTLSVersionSyslog = "TLSv1.3"
)

func (e MaximumTLSVersionSyslog) ToPointer() *MaximumTLSVersionSyslog {
	return &e
}
func (e *MaximumTLSVersionSyslog) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = MaximumTLSVersionSyslog(v)
		return nil
	default:
		return fmt.Errorf("invalid value for MaximumTLSVersionSyslog: %v", v)
	}
}

type TLSSettingsClientSideSyslog struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Reject certs that are not authorized by a CA in the CA certificate path, or by another
	//                     trusted CA (e.g., the system's CA). Defaults to Yes. Overrides the toggle from Advanced Settings, when also present.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate.
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key.
	Passphrase *string `json:"passphrase,omitempty"`
	// Minimum TLS version to use when connecting
	MinVersion *MinimumTLSVersionSyslog `json:"minVersion,omitempty"`
	// Maximum TLS version to use when connecting
	MaxVersion *MaximumTLSVersionSyslog `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideSyslog) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideSyslog) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TLSSettingsClientSideSyslog) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *TLSSettingsClientSideSyslog) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *TLSSettingsClientSideSyslog) GetServername() *string {
	if o == nil {
		return nil
	}
	return o.Servername
}

func (o *TLSSettingsClientSideSyslog) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

func (o *TLSSettingsClientSideSyslog) GetCaPath() *string {
	if o == nil {
		return nil
	}
	return o.CaPath
}

func (o *TLSSettingsClientSideSyslog) GetPrivKeyPath() *string {
	if o == nil {
		return nil
	}
	return o.PrivKeyPath
}

func (o *TLSSettingsClientSideSyslog) GetCertPath() *string {
	if o == nil {
		return nil
	}
	return o.CertPath
}

func (o *TLSSettingsClientSideSyslog) GetPassphrase() *string {
	if o == nil {
		return nil
	}
	return o.Passphrase
}

func (o *TLSSettingsClientSideSyslog) GetMinVersion() *MinimumTLSVersionSyslog {
	if o == nil {
		return nil
	}
	return o.MinVersion
}

func (o *TLSSettingsClientSideSyslog) GetMaxVersion() *MaximumTLSVersionSyslog {
	if o == nil {
		return nil
	}
	return o.MaxVersion
}

// BackpressureBehaviorSyslog - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorSyslog string

const (
	BackpressureBehaviorSyslogBlock BackpressureBehaviorSyslog = "block"
	BackpressureBehaviorSyslogDrop  BackpressureBehaviorSyslog = "drop"
	BackpressureBehaviorSyslogQueue BackpressureBehaviorSyslog = "queue"
)

func (e BackpressureBehaviorSyslog) ToPointer() *BackpressureBehaviorSyslog {
	return &e
}
func (e *BackpressureBehaviorSyslog) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorSyslog(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorSyslog: %v", v)
	}
}

// CompressionSyslog - Codec to use to compress the persisted data.
type CompressionSyslog string

const (
	CompressionSyslogNone CompressionSyslog = "none"
	CompressionSyslogGzip CompressionSyslog = "gzip"
)

func (e CompressionSyslog) ToPointer() *CompressionSyslog {
	return &e
}
func (e *CompressionSyslog) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionSyslog(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionSyslog: %v", v)
	}
}

// QueueFullBehaviorSyslog - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSyslog string

const (
	QueueFullBehaviorSyslogBlock QueueFullBehaviorSyslog = "block"
	QueueFullBehaviorSyslogDrop  QueueFullBehaviorSyslog = "drop"
)

func (e QueueFullBehaviorSyslog) ToPointer() *QueueFullBehaviorSyslog {
	return &e
}
func (e *QueueFullBehaviorSyslog) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorSyslog(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorSyslog: %v", v)
	}
}

// ModeSyslog - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type ModeSyslog string

const (
	ModeSyslogError        ModeSyslog = "error"
	ModeSyslogBackpressure ModeSyslog = "backpressure"
	ModeSyslogAlways       ModeSyslog = "always"
)

func (e ModeSyslog) ToPointer() *ModeSyslog {
	return &e
}
func (e *ModeSyslog) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeSyslog(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeSyslog: %v", v)
	}
}

type PqControlsSyslog struct {
}

type OutputSyslog struct {
	// Unique ID for this output
	ID   string           `json:"id"`
	Type OutputTypeSyslog `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The network protocol to use for sending out syslog messages
	Protocol *ProtocolSyslog `default:"tcp" json:"protocol"`
	// Default value for message facility. Will be overwritten by value of __facility if set. Defaults to user.
	Facility *Facility `default:"1" json:"facility"`
	// Default value for message severity. Will be overwritten by value of __severity if set. Defaults to notice.
	Severity *SeveritySyslog `default:"5" json:"severity"`
	// Default name for device or application that originated the message. Defaults to Cribl, but will be overwritten by value of __appname if set.
	AppName *string `default:"Cribl" json:"appName"`
	// The syslog message format depending on the receiver's support
	MessageFormat *MessageFormatSyslog `default:"rfc3164" json:"messageFormat"`
	// Timestamp format to use when serializing event's time field
	TimestampFormat *TimestampFormatEnum `default:"syslog" json:"timestampFormat"`
	// Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
	ThrottleRatePerSec *string `default:"0" json:"throttleRatePerSec"`
	// When enabled, messages will be prefixed with the byte count of the message. Otherwise, no prefix will be set, and the message will be appended with a \n.
	OctetCountFraming *bool `json:"octetCountFraming,omitempty"`
	// Use to troubleshoot issues with sending data
	LogFailedRequests *bool   `default:"false" json:"logFailedRequests"`
	Description       *string `json:"description,omitempty"`
	// For optimal performance, enable load balancing even if you have one hostname, as it can expand to multiple IPs.  If this setting is disabled, consider enabling round-robin DNS.
	LoadBalanced *bool `default:"true" json:"loadBalanced"`
	// Amount of time (milliseconds) to wait for the connection to establish before retrying
	ConnectionTimeout *float64 `default:"10000" json:"connectionTimeout"`
	// Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead
	WriteTimeout *float64                     `default:"60000" json:"writeTimeout"`
	TLS          *TLSSettingsClientSideSyslog `json:"tls,omitempty"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorSyslog `default:"block" json:"onBackpressure"`
	// The hostname of the receiver
	Host *string `json:"host,omitempty"`
	// The port to connect to on the provided host
	Port *float64 `json:"port,omitempty"`
	// Maximum size of syslog messages. Make sure this value is less than or equal to the MTU to avoid UDP packet fragmentation.
	MaxRecordSize *float64 `default:"1500" json:"maxRecordSize"`
	// How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every message sent will incur a DNS lookup.
	UDPDNSResolvePeriodSec *float64 `default:"0" json:"udpDnsResolvePeriodSec"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *CompressionSyslog `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorSyslog `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeSyslog       `default:"error" json:"pqMode"`
	PqControls *PqControlsSyslog `json:"pqControls,omitempty"`
	Status     *TFStatus         `json:"status,omitempty"`
}

func (o OutputSyslog) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSyslog) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputSyslog) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputSyslog) GetType() OutputTypeSyslog {
	if o == nil {
		return OutputTypeSyslog("")
	}
	return o.Type
}

func (o *OutputSyslog) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSyslog) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSyslog) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSyslog) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSyslog) GetProtocol() *ProtocolSyslog {
	if o == nil {
		return nil
	}
	return o.Protocol
}

func (o *OutputSyslog) GetFacility() *Facility {
	if o == nil {
		return nil
	}
	return o.Facility
}

func (o *OutputSyslog) GetSeverity() *SeveritySyslog {
	if o == nil {
		return nil
	}
	return o.Severity
}

func (o *OutputSyslog) GetAppName() *string {
	if o == nil {
		return nil
	}
	return o.AppName
}

func (o *OutputSyslog) GetMessageFormat() *MessageFormatSyslog {
	if o == nil {
		return nil
	}
	return o.MessageFormat
}

func (o *OutputSyslog) GetTimestampFormat() *TimestampFormatEnum {
	if o == nil {
		return nil
	}
	return o.TimestampFormat
}

func (o *OutputSyslog) GetThrottleRatePerSec() *string {
	if o == nil {
		return nil
	}
	return o.ThrottleRatePerSec
}

func (o *OutputSyslog) GetOctetCountFraming() *bool {
	if o == nil {
		return nil
	}
	return o.OctetCountFraming
}

func (o *OutputSyslog) GetLogFailedRequests() *bool {
	if o == nil {
		return nil
	}
	return o.LogFailedRequests
}

func (o *OutputSyslog) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSyslog) GetLoadBalanced() *bool {
	if o == nil {
		return nil
	}
	return o.LoadBalanced
}

func (o *OutputSyslog) GetConnectionTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.ConnectionTimeout
}

func (o *OutputSyslog) GetWriteTimeout() *float64 {
	if o == nil {
		return nil
	}
	return o.WriteTimeout
}

func (o *OutputSyslog) GetTLS() *TLSSettingsClientSideSyslog {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputSyslog) GetOnBackpressure() *BackpressureBehaviorSyslog {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSyslog) GetHost() *string {
	if o == nil {
		return nil
	}
	return o.Host
}

func (o *OutputSyslog) GetPort() *float64 {
	if o == nil {
		return nil
	}
	return o.Port
}

func (o *OutputSyslog) GetMaxRecordSize() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxRecordSize
}

func (o *OutputSyslog) GetUDPDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.UDPDNSResolvePeriodSec
}

func (o *OutputSyslog) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSyslog) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSyslog) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSyslog) GetPqCompress() *CompressionSyslog {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSyslog) GetPqOnBackpressure() *QueueFullBehaviorSyslog {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSyslog) GetPqMode() *ModeSyslog {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSyslog) GetPqControls() *PqControlsSyslog {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputSyslog) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeDevnull string

const (
	TypeDevnullDevnull TypeDevnull = "devnull"
)

func (e TypeDevnull) ToPointer() *TypeDevnull {
	return &e
}
func (e *TypeDevnull) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "devnull":
		*e = TypeDevnull(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeDevnull: %v", v)
	}
}

type OutputDevnull struct {
	// Unique ID for this output
	ID   string      `json:"id"`
	Type TypeDevnull `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string  `json:"streamtags,omitempty"`
	Status     *TFStatus `json:"status,omitempty"`
}

func (o *OutputDevnull) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputDevnull) GetType() TypeDevnull {
	if o == nil {
		return TypeDevnull("")
	}
	return o.Type
}

func (o *OutputDevnull) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDevnull) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDevnull) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDevnull) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputDevnull) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeSentinel string

const (
	TypeSentinelSentinel TypeSentinel = "sentinel"
)

func (e TypeSentinel) ToPointer() *TypeSentinel {
	return &e
}
func (e *TypeSentinel) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "sentinel":
		*e = TypeSentinel(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeSentinel: %v", v)
	}
}

type ExtraHTTPHeaderSentinel struct {
	// Field name
	Name *string `json:"name,omitempty"`
	// Field value
	Value string `json:"value"`
}

func (o *ExtraHTTPHeaderSentinel) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderSentinel) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeSentinel - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeSentinel string

const (
	FailedRequestLoggingModeSentinelPayload           FailedRequestLoggingModeSentinel = "payload"
	FailedRequestLoggingModeSentinelPayloadAndHeaders FailedRequestLoggingModeSentinel = "payloadAndHeaders"
	FailedRequestLoggingModeSentinelNone              FailedRequestLoggingModeSentinel = "none"
)

func (e FailedRequestLoggingModeSentinel) ToPointer() *FailedRequestLoggingModeSentinel {
	return &e
}
func (e *FailedRequestLoggingModeSentinel) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeSentinel(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeSentinel: %v", v)
	}
}

type ResponseRetrySettingSentinel struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingSentinel) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingSentinel) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingSentinel) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingSentinel) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingSentinel) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingSentinel) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsSentinel struct {
	// Enable to retry on request timeout
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsSentinel) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsSentinel) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsSentinel) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsSentinel) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsSentinel) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsSentinel) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorSentinel - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorSentinel string

const (
	BackpressureBehaviorSentinelBlock BackpressureBehaviorSentinel = "block"
	BackpressureBehaviorSentinelDrop  BackpressureBehaviorSentinel = "drop"
	BackpressureBehaviorSentinelQueue BackpressureBehaviorSentinel = "queue"
)

func (e BackpressureBehaviorSentinel) ToPointer() *BackpressureBehaviorSentinel {
	return &e
}
func (e *BackpressureBehaviorSentinel) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorSentinel(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorSentinel: %v", v)
	}
}

type OutputAuthType string

const (
	OutputAuthTypeOauth OutputAuthType = "oauth"
)

func (e OutputAuthType) ToPointer() *OutputAuthType {
	return &e
}
func (e *OutputAuthType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "oauth":
		*e = OutputAuthType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputAuthType: %v", v)
	}
}

type EndpointConfiguration string

const (
	EndpointConfigurationURL EndpointConfiguration = "url"
	EndpointConfigurationID  EndpointConfiguration = "ID"
)

func (e EndpointConfiguration) ToPointer() *EndpointConfiguration {
	return &e
}
func (e *EndpointConfiguration) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "url":
		fallthrough
	case "ID":
		*e = EndpointConfiguration(v)
		return nil
	default:
		return fmt.Errorf("invalid value for EndpointConfiguration: %v", v)
	}
}

type FormatSentinel string

const (
	FormatSentinelNdjson    FormatSentinel = "ndjson"
	FormatSentinelJSONArray FormatSentinel = "json_array"
	FormatSentinelCustom    FormatSentinel = "custom"
	FormatSentinelAdvanced  FormatSentinel = "advanced"
)

func (e FormatSentinel) ToPointer() *FormatSentinel {
	return &e
}
func (e *FormatSentinel) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "ndjson":
		fallthrough
	case "json_array":
		fallthrough
	case "custom":
		fallthrough
	case "advanced":
		*e = FormatSentinel(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FormatSentinel: %v", v)
	}
}

// CompressionSentinel - Codec to use to compress the persisted data.
type CompressionSentinel string

const (
	CompressionSentinelNone CompressionSentinel = "none"
	CompressionSentinelGzip CompressionSentinel = "gzip"
)

func (e CompressionSentinel) ToPointer() *CompressionSentinel {
	return &e
}
func (e *CompressionSentinel) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionSentinel(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionSentinel: %v", v)
	}
}

// QueueFullBehaviorSentinel - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorSentinel string

const (
	QueueFullBehaviorSentinelBlock QueueFullBehaviorSentinel = "block"
	QueueFullBehaviorSentinelDrop  QueueFullBehaviorSentinel = "drop"
)

func (e QueueFullBehaviorSentinel) ToPointer() *QueueFullBehaviorSentinel {
	return &e
}
func (e *QueueFullBehaviorSentinel) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorSentinel(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorSentinel: %v", v)
	}
}

// ModeSentinel - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type ModeSentinel string

const (
	ModeSentinelError        ModeSentinel = "error"
	ModeSentinelBackpressure ModeSentinel = "backpressure"
	ModeSentinelAlways       ModeSentinel = "always"
)

func (e ModeSentinel) ToPointer() *ModeSentinel {
	return &e
}
func (e *ModeSentinel) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeSentinel(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeSentinel: %v", v)
	}
}

type PqControlsSentinel struct {
}

type OutputSentinel struct {
	// Unique ID for this output
	ID   *string       `json:"id,omitempty"`
	Type *TypeSentinel `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// Disable to close the connection immediately after sending the outgoing request
	KeepAlive *bool `default:"true" json:"keepAlive"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size (KB) of the request body (defaults to the API's maximum limit of 1000 KB)
	MaxPayloadSizeKB *float64 `default:"1000" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Defaults to Yes. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max body size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events. You can also add headers dynamically on a per-event basis in the __headers field, as explained [here](https://docs.cribl.io/stream/destinations-webhook/#internal-fields).
	ExtraHTTPHeaders []ExtraHTTPHeaderSentinel `json:"extraHttpHeaders,omitempty"`
	// Enables round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeSentinel `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable).
	ResponseRetrySettings []ResponseRetrySettingSentinel `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsSentinel  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorSentinel `default:"block" json:"onBackpressure"`
	AuthType       *OutputAuthType               `json:"authType,omitempty"`
	// URL for OAuth
	LoginURL string `json:"loginUrl"`
	// Secret parameter value to pass in request body
	Secret string `json:"secret"`
	// JavaScript expression to compute the Client ID for the Azure application. Can be a constant.
	ClientID string `json:"client_id"`
	// Scope to pass in the OAuth request
	Scope                    *string                `default:"https://monitor.azure.com/.default" json:"scope"`
	EndpointURLConfiguration *EndpointConfiguration `default:"url" json:"endpointURLConfiguration"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64        `json:"totalMemoryLimitKB,omitempty"`
	Description        *string         `json:"description,omitempty"`
	Format             *FormatSentinel `json:"format,omitempty"`
	// Expression to evaluate on events to generate output. Example: `raw=${_raw}`. See [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook#custom-format) for other examples. If empty, the full event is sent as stringified JSON.
	CustomSourceExpression *string `default:"__httpOut" json:"customSourceExpression"`
	// Whether to drop events when the source expression evaluates to null
	CustomDropWhenNull *bool `default:"false" json:"customDropWhenNull"`
	// Delimiter string to insert between individual events. Defaults to newline character.
	CustomEventDelimiter *string `default:"\\n" json:"customEventDelimiter"`
	// Content type to use for request. Defaults to application/x-ndjson. Any content types set in Advanced Settings > Extra HTTP headers will override this entry.
	CustomContentType *string `default:"application/x-ndjson" json:"customContentType"`
	// Expression specifying how to format the payload for each batch. To reference the events to send, use the `${events}` variable. Example expression: `{ "items" : [${events}] }` would send the batch inside a JSON object.
	CustomPayloadExpression *string `default:"\\${events}" json:"customPayloadExpression"`
	// HTTP content-type header value
	AdvancedContentType *string `default:"application/json" json:"advancedContentType"`
	// Custom JavaScript code to format incoming event data accessible through the __e variable. The formatted content is added to (__e['__eventOut']) if available. Otherwise, the original event is serialized as JSON. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.
	FormatEventCode *string `json:"formatEventCode,omitempty"`
	// Optional JavaScript code to format the payload sent to the Destination. The payload, containing a batch of formatted events, is accessible through the __e['payload'] variable. The formatted payload is returned in the __e['__payloadOut'] variable. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.
	FormatPayloadCode *string `json:"formatPayloadCode,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *CompressionSentinel `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorSentinel `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeSentinel       `default:"error" json:"pqMode"`
	PqControls *PqControlsSentinel `json:"pqControls,omitempty"`
	// URL to send events to. Can be overwritten by an event's __url field.
	URL *string `json:"url,omitempty"`
	// Immutable ID for the Data collection rule (DCR).
	DcrID *string `json:"dcrID,omitempty"`
	// Data collection endpoint (DCE) URL. In the format: `https://<Endpoint-Name>-<Identifier>.<Region>.ingest.monitor.azure.com`.
	DceEndpoint *string `json:"dceEndpoint,omitempty"`
	// The name of the stream (Sentinel table) in which to store the events.
	StreamName *string   `json:"streamName,omitempty"`
	Status     *TFStatus `json:"status,omitempty"`
}

func (o OutputSentinel) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputSentinel) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputSentinel) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputSentinel) GetType() *TypeSentinel {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputSentinel) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputSentinel) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputSentinel) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputSentinel) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputSentinel) GetKeepAlive() *bool {
	if o == nil {
		return nil
	}
	return o.KeepAlive
}

func (o *OutputSentinel) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputSentinel) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputSentinel) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputSentinel) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputSentinel) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputSentinel) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputSentinel) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputSentinel) GetExtraHTTPHeaders() []ExtraHTTPHeaderSentinel {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputSentinel) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputSentinel) GetFailedRequestLoggingMode() *FailedRequestLoggingModeSentinel {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputSentinel) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputSentinel) GetResponseRetrySettings() []ResponseRetrySettingSentinel {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputSentinel) GetTimeoutRetrySettings() *TimeoutRetrySettingsSentinel {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputSentinel) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputSentinel) GetOnBackpressure() *BackpressureBehaviorSentinel {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputSentinel) GetAuthType() *OutputAuthType {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputSentinel) GetLoginURL() string {
	if o == nil {
		return ""
	}
	return o.LoginURL
}

func (o *OutputSentinel) GetSecret() string {
	if o == nil {
		return ""
	}
	return o.Secret
}

func (o *OutputSentinel) GetClientID() string {
	if o == nil {
		return ""
	}
	return o.ClientID
}

func (o *OutputSentinel) GetScope() *string {
	if o == nil {
		return nil
	}
	return o.Scope
}

func (o *OutputSentinel) GetEndpointURLConfiguration() *EndpointConfiguration {
	if o == nil {
		return nil
	}
	return o.EndpointURLConfiguration
}

func (o *OutputSentinel) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputSentinel) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputSentinel) GetFormat() *FormatSentinel {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputSentinel) GetCustomSourceExpression() *string {
	if o == nil {
		return nil
	}
	return o.CustomSourceExpression
}

func (o *OutputSentinel) GetCustomDropWhenNull() *bool {
	if o == nil {
		return nil
	}
	return o.CustomDropWhenNull
}

func (o *OutputSentinel) GetCustomEventDelimiter() *string {
	if o == nil {
		return nil
	}
	return o.CustomEventDelimiter
}

func (o *OutputSentinel) GetCustomContentType() *string {
	if o == nil {
		return nil
	}
	return o.CustomContentType
}

func (o *OutputSentinel) GetCustomPayloadExpression() *string {
	if o == nil {
		return nil
	}
	return o.CustomPayloadExpression
}

func (o *OutputSentinel) GetAdvancedContentType() *string {
	if o == nil {
		return nil
	}
	return o.AdvancedContentType
}

func (o *OutputSentinel) GetFormatEventCode() *string {
	if o == nil {
		return nil
	}
	return o.FormatEventCode
}

func (o *OutputSentinel) GetFormatPayloadCode() *string {
	if o == nil {
		return nil
	}
	return o.FormatPayloadCode
}

func (o *OutputSentinel) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputSentinel) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputSentinel) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputSentinel) GetPqCompress() *CompressionSentinel {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputSentinel) GetPqOnBackpressure() *QueueFullBehaviorSentinel {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputSentinel) GetPqMode() *ModeSentinel {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputSentinel) GetPqControls() *PqControlsSentinel {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputSentinel) GetURL() *string {
	if o == nil {
		return nil
	}
	return o.URL
}

func (o *OutputSentinel) GetDcrID() *string {
	if o == nil {
		return nil
	}
	return o.DcrID
}

func (o *OutputSentinel) GetDceEndpoint() *string {
	if o == nil {
		return nil
	}
	return o.DceEndpoint
}

func (o *OutputSentinel) GetStreamName() *string {
	if o == nil {
		return nil
	}
	return o.StreamName
}

func (o *OutputSentinel) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeWebhook string

const (
	TypeWebhookWebhook TypeWebhook = "webhook"
)

func (e TypeWebhook) ToPointer() *TypeWebhook {
	return &e
}
func (e *TypeWebhook) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "webhook":
		*e = TypeWebhook(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeWebhook: %v", v)
	}
}

// MethodWebhook - The method to use when sending events. Defaults to POST.
type MethodWebhook string

const (
	MethodWebhookPost  MethodWebhook = "POST"
	MethodWebhookPut   MethodWebhook = "PUT"
	MethodWebhookPatch MethodWebhook = "PATCH"
)

func (e MethodWebhook) ToPointer() *MethodWebhook {
	return &e
}
func (e *MethodWebhook) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "POST":
		fallthrough
	case "PUT":
		fallthrough
	case "PATCH":
		*e = MethodWebhook(v)
		return nil
	default:
		return fmt.Errorf("invalid value for MethodWebhook: %v", v)
	}
}

// FormatWebhook - Specifies how to format events before sending out. Defaults to NDJSON.
type FormatWebhook string

const (
	FormatWebhookNdjson    FormatWebhook = "ndjson"
	FormatWebhookJSONArray FormatWebhook = "json_array"
	FormatWebhookCustom    FormatWebhook = "custom"
	FormatWebhookAdvanced  FormatWebhook = "advanced"
)

func (e FormatWebhook) ToPointer() *FormatWebhook {
	return &e
}
func (e *FormatWebhook) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "ndjson":
		fallthrough
	case "json_array":
		fallthrough
	case "custom":
		fallthrough
	case "advanced":
		*e = FormatWebhook(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FormatWebhook: %v", v)
	}
}

type ExtraHTTPHeaderWebhook struct {
	// Field name
	Name *string `json:"name,omitempty"`
	// Field value
	Value string `json:"value"`
}

func (o *ExtraHTTPHeaderWebhook) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *ExtraHTTPHeaderWebhook) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// FailedRequestLoggingModeWebhook - Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
type FailedRequestLoggingModeWebhook string

const (
	FailedRequestLoggingModeWebhookPayload           FailedRequestLoggingModeWebhook = "payload"
	FailedRequestLoggingModeWebhookPayloadAndHeaders FailedRequestLoggingModeWebhook = "payloadAndHeaders"
	FailedRequestLoggingModeWebhookNone              FailedRequestLoggingModeWebhook = "none"
)

func (e FailedRequestLoggingModeWebhook) ToPointer() *FailedRequestLoggingModeWebhook {
	return &e
}
func (e *FailedRequestLoggingModeWebhook) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "payload":
		fallthrough
	case "payloadAndHeaders":
		fallthrough
	case "none":
		*e = FailedRequestLoggingModeWebhook(v)
		return nil
	default:
		return fmt.Errorf("invalid value for FailedRequestLoggingModeWebhook: %v", v)
	}
}

type ResponseRetrySettingWebhook struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (r ResponseRetrySettingWebhook) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(r, "", false)
}

func (r *ResponseRetrySettingWebhook) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &r, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *ResponseRetrySettingWebhook) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *ResponseRetrySettingWebhook) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *ResponseRetrySettingWebhook) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *ResponseRetrySettingWebhook) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type TimeoutRetrySettingsWebhook struct {
	// Enable to retry on request timeout
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (t TimeoutRetrySettingsWebhook) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TimeoutRetrySettingsWebhook) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TimeoutRetrySettingsWebhook) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *TimeoutRetrySettingsWebhook) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *TimeoutRetrySettingsWebhook) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *TimeoutRetrySettingsWebhook) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// BackpressureBehaviorWebhook - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type BackpressureBehaviorWebhook string

const (
	BackpressureBehaviorWebhookBlock BackpressureBehaviorWebhook = "block"
	BackpressureBehaviorWebhookDrop  BackpressureBehaviorWebhook = "drop"
	BackpressureBehaviorWebhookQueue BackpressureBehaviorWebhook = "queue"
)

func (e BackpressureBehaviorWebhook) ToPointer() *BackpressureBehaviorWebhook {
	return &e
}
func (e *BackpressureBehaviorWebhook) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = BackpressureBehaviorWebhook(v)
		return nil
	default:
		return fmt.Errorf("invalid value for BackpressureBehaviorWebhook: %v", v)
	}
}

// AuthenticationTypeWebhook - The authentication method to use for the HTTP request. Defaults to None.
type AuthenticationTypeWebhook string

const (
	AuthenticationTypeWebhookNone              AuthenticationTypeWebhook = "none"
	AuthenticationTypeWebhookBasic             AuthenticationTypeWebhook = "basic"
	AuthenticationTypeWebhookCredentialsSecret AuthenticationTypeWebhook = "credentialsSecret"
	AuthenticationTypeWebhookToken             AuthenticationTypeWebhook = "token"
	AuthenticationTypeWebhookTextSecret        AuthenticationTypeWebhook = "textSecret"
	AuthenticationTypeWebhookOauth             AuthenticationTypeWebhook = "oauth"
)

func (e AuthenticationTypeWebhook) ToPointer() *AuthenticationTypeWebhook {
	return &e
}
func (e *AuthenticationTypeWebhook) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "basic":
		fallthrough
	case "credentialsSecret":
		fallthrough
	case "token":
		fallthrough
	case "textSecret":
		fallthrough
	case "oauth":
		*e = AuthenticationTypeWebhook(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AuthenticationTypeWebhook: %v", v)
	}
}

// MinimumTLSVersionWebhook - Minimum TLS version to use when connecting
type MinimumTLSVersionWebhook string

const (
	MinimumTLSVersionWebhookTlSv1  MinimumTLSVersionWebhook = "TLSv1"
	MinimumTLSVersionWebhookTlSv11 MinimumTLSVersionWebhook = "TLSv1.1"
	MinimumTLSVersionWebhookTlSv12 MinimumTLSVersionWebhook = "TLSv1.2"
	MinimumTLSVersionWebhookTlSv13 MinimumTLSVersionWebhook = "TLSv1.3"
)

func (e MinimumTLSVersionWebhook) ToPointer() *MinimumTLSVersionWebhook {
	return &e
}
func (e *MinimumTLSVersionWebhook) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = MinimumTLSVersionWebhook(v)
		return nil
	default:
		return fmt.Errorf("invalid value for MinimumTLSVersionWebhook: %v", v)
	}
}

// MaximumTLSVersionWebhook - Maximum TLS version to use when connecting
type MaximumTLSVersionWebhook string

const (
	MaximumTLSVersionWebhookTlSv1  MaximumTLSVersionWebhook = "TLSv1"
	MaximumTLSVersionWebhookTlSv11 MaximumTLSVersionWebhook = "TLSv1.1"
	MaximumTLSVersionWebhookTlSv12 MaximumTLSVersionWebhook = "TLSv1.2"
	MaximumTLSVersionWebhookTlSv13 MaximumTLSVersionWebhook = "TLSv1.3"
)

func (e MaximumTLSVersionWebhook) ToPointer() *MaximumTLSVersionWebhook {
	return &e
}
func (e *MaximumTLSVersionWebhook) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "TLSv1":
		fallthrough
	case "TLSv1.1":
		fallthrough
	case "TLSv1.2":
		fallthrough
	case "TLSv1.3":
		*e = MaximumTLSVersionWebhook(v)
		return nil
	default:
		return fmt.Errorf("invalid value for MaximumTLSVersionWebhook: %v", v)
	}
}

type TLSSettingsClientSideWebhook struct {
	Disabled *bool `default:"true" json:"disabled"`
	// Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.
	Servername *string `json:"servername,omitempty"`
	// The name of the predefined certificate.
	CertificateName *string `json:"certificateName,omitempty"`
	// Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.
	CaPath *string `json:"caPath,omitempty"`
	// Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.
	PrivKeyPath *string `json:"privKeyPath,omitempty"`
	// Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.
	CertPath *string `json:"certPath,omitempty"`
	// Passphrase to use to decrypt private key.
	Passphrase *string `json:"passphrase,omitempty"`
	// Minimum TLS version to use when connecting
	MinVersion *MinimumTLSVersionWebhook `json:"minVersion,omitempty"`
	// Maximum TLS version to use when connecting
	MaxVersion *MaximumTLSVersionWebhook `json:"maxVersion,omitempty"`
}

func (t TLSSettingsClientSideWebhook) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(t, "", false)
}

func (t *TLSSettingsClientSideWebhook) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &t, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *TLSSettingsClientSideWebhook) GetDisabled() *bool {
	if o == nil {
		return nil
	}
	return o.Disabled
}

func (o *TLSSettingsClientSideWebhook) GetServername() *string {
	if o == nil {
		return nil
	}
	return o.Servername
}

func (o *TLSSettingsClientSideWebhook) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

func (o *TLSSettingsClientSideWebhook) GetCaPath() *string {
	if o == nil {
		return nil
	}
	return o.CaPath
}

func (o *TLSSettingsClientSideWebhook) GetPrivKeyPath() *string {
	if o == nil {
		return nil
	}
	return o.PrivKeyPath
}

func (o *TLSSettingsClientSideWebhook) GetCertPath() *string {
	if o == nil {
		return nil
	}
	return o.CertPath
}

func (o *TLSSettingsClientSideWebhook) GetPassphrase() *string {
	if o == nil {
		return nil
	}
	return o.Passphrase
}

func (o *TLSSettingsClientSideWebhook) GetMinVersion() *MinimumTLSVersionWebhook {
	if o == nil {
		return nil
	}
	return o.MinVersion
}

func (o *TLSSettingsClientSideWebhook) GetMaxVersion() *MaximumTLSVersionWebhook {
	if o == nil {
		return nil
	}
	return o.MaxVersion
}

// CompressionWebhook - Codec to use to compress the persisted data.
type CompressionWebhook string

const (
	CompressionWebhookNone CompressionWebhook = "none"
	CompressionWebhookGzip CompressionWebhook = "gzip"
)

func (e CompressionWebhook) ToPointer() *CompressionWebhook {
	return &e
}
func (e *CompressionWebhook) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = CompressionWebhook(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CompressionWebhook: %v", v)
	}
}

// QueueFullBehaviorWebhook - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type QueueFullBehaviorWebhook string

const (
	QueueFullBehaviorWebhookBlock QueueFullBehaviorWebhook = "block"
	QueueFullBehaviorWebhookDrop  QueueFullBehaviorWebhook = "drop"
)

func (e QueueFullBehaviorWebhook) ToPointer() *QueueFullBehaviorWebhook {
	return &e
}
func (e *QueueFullBehaviorWebhook) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = QueueFullBehaviorWebhook(v)
		return nil
	default:
		return fmt.Errorf("invalid value for QueueFullBehaviorWebhook: %v", v)
	}
}

// ModeWebhook - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type ModeWebhook string

const (
	ModeWebhookError        ModeWebhook = "error"
	ModeWebhookBackpressure ModeWebhook = "backpressure"
	ModeWebhookAlways       ModeWebhook = "always"
)

func (e ModeWebhook) ToPointer() *ModeWebhook {
	return &e
}
func (e *ModeWebhook) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = ModeWebhook(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ModeWebhook: %v", v)
	}
}

type PqControlsWebhook struct {
}

type OauthParamWebhook struct {
	// OAuth parameter name
	Name string `json:"name"`
	// OAuth parameter value
	Value string `json:"value"`
}

func (o *OauthParamWebhook) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OauthParamWebhook) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OauthHeaderWebhook struct {
	// OAuth header name
	Name string `json:"name"`
	// OAuth header value
	Value string `json:"value"`
}

func (o *OauthHeaderWebhook) GetName() string {
	if o == nil {
		return ""
	}
	return o.Name
}

func (o *OauthHeaderWebhook) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type URLWebhook struct {
	// URL of a webhook endpoint to send events to, such as http://localhost:10200
	URL string `json:"url"`
	// Assign a weight (>0) to each endpoint to indicate its traffic-handling capability
	Weight *float64 `default:"1" json:"weight"`
}

func (u URLWebhook) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(u, "", false)
}

func (u *URLWebhook) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &u, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *URLWebhook) GetURL() string {
	if o == nil {
		return ""
	}
	return o.URL
}

func (o *URLWebhook) GetWeight() *float64 {
	if o == nil {
		return nil
	}
	return o.Weight
}

type OutputWebhook struct {
	// Unique ID for this output
	ID   string      `json:"id"`
	Type TypeWebhook `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The method to use when sending events. Defaults to POST.
	Method *MethodWebhook `default:"POST" json:"method"`
	// Specifies how to format events before sending out. Defaults to NDJSON.
	Format *FormatWebhook `default:"ndjson" json:"format"`
	// Disable to close the connection immediately after sending the outgoing request
	KeepAlive *bool `default:"true" json:"keepAlive"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Compress the payload body before sending
	Compress *bool `default:"true" json:"compress"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Defaults to Yes. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max body size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Headers to add to all events. You can also add headers dynamically on a per-event basis in the __headers field, as explained [here](https://docs.cribl.io/stream/destinations-webhook/#internal-fields).
	ExtraHTTPHeaders []ExtraHTTPHeaderWebhook `json:"extraHttpHeaders,omitempty"`
	// Enables round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
	FailedRequestLoggingMode *FailedRequestLoggingModeWebhook `default:"none" json:"failedRequestLoggingMode"`
	// List of headers that are safe to log in plain text
	SafeHeaders []string `json:"safeHeaders,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable).
	ResponseRetrySettings []ResponseRetrySettingWebhook `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *TimeoutRetrySettingsWebhook  `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *BackpressureBehaviorWebhook `default:"block" json:"onBackpressure"`
	// The authentication method to use for the HTTP request. Defaults to None.
	AuthType *AuthenticationTypeWebhook    `default:"none" json:"authType"`
	TLS      *TLSSettingsClientSideWebhook `json:"tls,omitempty"`
	// Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
	TotalMemoryLimitKB *float64 `json:"totalMemoryLimitKB,omitempty"`
	// Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.
	LoadBalanced *bool   `default:"false" json:"loadBalanced"`
	Description  *string `json:"description,omitempty"`
	// Expression to evaluate on events to generate output. Example: `raw=${_raw}`. See [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook#custom-format) for other examples. If empty, the full event is sent as stringified JSON.
	CustomSourceExpression *string `default:"__httpOut" json:"customSourceExpression"`
	// Whether to drop events when the source expression evaluates to null
	CustomDropWhenNull *bool `default:"false" json:"customDropWhenNull"`
	// Delimiter string to insert between individual events. Defaults to newline character.
	CustomEventDelimiter *string `default:"\\n" json:"customEventDelimiter"`
	// Content type to use for request. Defaults to application/x-ndjson. Any content types set in Advanced Settings > Extra HTTP headers will override this entry.
	CustomContentType *string `default:"application/x-ndjson" json:"customContentType"`
	// Expression specifying how to format the payload for each batch. To reference the events to send, use the `${events}` variable. Example expression: `{ "items" : [${events}] }` would send the batch inside a JSON object.
	CustomPayloadExpression *string `default:"\\${events}" json:"customPayloadExpression"`
	// HTTP content-type header value
	AdvancedContentType *string `default:"application/json" json:"advancedContentType"`
	// Custom JavaScript code to format incoming event data accessible through the __e variable. The formatted content is added to (__e['__eventOut']) if available. Otherwise, the original event is serialized as JSON. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.
	FormatEventCode *string `json:"formatEventCode,omitempty"`
	// Optional JavaScript code to format the payload sent to the Destination. The payload, containing a batch of formatted events, is accessible through the __e['payload'] variable. The formatted payload is returned in the __e['__payloadOut'] variable. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.
	FormatPayloadCode *string `json:"formatPayloadCode,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *CompressionWebhook `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *QueueFullBehaviorWebhook `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *ModeWebhook       `default:"error" json:"pqMode"`
	PqControls *PqControlsWebhook `json:"pqControls,omitempty"`
	Username   *string            `json:"username,omitempty"`
	Password   *string            `json:"password,omitempty"`
	// Bearer token to include in the authorization header
	Token *string `json:"token,omitempty"`
	// Select or create a secret that references your credentials
	CredentialsSecret *string `json:"credentialsSecret,omitempty"`
	// Select or create a stored text secret
	TextSecret *string `json:"textSecret,omitempty"`
	// URL for OAuth
	LoginURL *string `json:"loginUrl,omitempty"`
	// Secret parameter name to pass in request body
	SecretParamName *string `json:"secretParamName,omitempty"`
	// Secret parameter value to pass in request body
	Secret *string `json:"secret,omitempty"`
	// Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
	TokenAttributeName *string `json:"tokenAttributeName,omitempty"`
	// JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
	AuthHeaderExpr *string `default:"Bearer \\${token}" json:"authHeaderExpr"`
	// How often the OAuth token should be refreshed.
	TokenTimeoutSecs *float64 `default:"3600" json:"tokenTimeoutSecs"`
	// Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthParams []OauthParamWebhook `json:"oauthParams,omitempty"`
	// Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
	OauthHeaders []OauthHeaderWebhook `json:"oauthHeaders,omitempty"`
	// URL of a webhook endpoint to send events to, such as http://localhost:10200
	URL *string `json:"url,omitempty"`
	// Exclude all IPs of the current host from the list of any resolved hostnames.
	ExcludeSelf *bool        `default:"false" json:"excludeSelf"`
	Urls        []URLWebhook `json:"urls,omitempty"`
	// Re-resolve any hostnames every this many seconds and pick up destinations from A records.
	DNSResolvePeriodSec *float64 `default:"600" json:"dnsResolvePeriodSec"`
	// How far back in time to keep traffic stats for load balancing purposes.
	LoadBalanceStatsPeriodSec *float64  `default:"300" json:"loadBalanceStatsPeriodSec"`
	Status                    *TFStatus `json:"status,omitempty"`
}

func (o OutputWebhook) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputWebhook) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputWebhook) GetID() string {
	if o == nil {
		return ""
	}
	return o.ID
}

func (o *OutputWebhook) GetType() TypeWebhook {
	if o == nil {
		return TypeWebhook("")
	}
	return o.Type
}

func (o *OutputWebhook) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputWebhook) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputWebhook) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputWebhook) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputWebhook) GetMethod() *MethodWebhook {
	if o == nil {
		return nil
	}
	return o.Method
}

func (o *OutputWebhook) GetFormat() *FormatWebhook {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputWebhook) GetKeepAlive() *bool {
	if o == nil {
		return nil
	}
	return o.KeepAlive
}

func (o *OutputWebhook) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputWebhook) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputWebhook) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputWebhook) GetCompress() *bool {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputWebhook) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputWebhook) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputWebhook) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputWebhook) GetExtraHTTPHeaders() []ExtraHTTPHeaderWebhook {
	if o == nil {
		return nil
	}
	return o.ExtraHTTPHeaders
}

func (o *OutputWebhook) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputWebhook) GetFailedRequestLoggingMode() *FailedRequestLoggingModeWebhook {
	if o == nil {
		return nil
	}
	return o.FailedRequestLoggingMode
}

func (o *OutputWebhook) GetSafeHeaders() []string {
	if o == nil {
		return nil
	}
	return o.SafeHeaders
}

func (o *OutputWebhook) GetResponseRetrySettings() []ResponseRetrySettingWebhook {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputWebhook) GetTimeoutRetrySettings() *TimeoutRetrySettingsWebhook {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputWebhook) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputWebhook) GetOnBackpressure() *BackpressureBehaviorWebhook {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputWebhook) GetAuthType() *AuthenticationTypeWebhook {
	if o == nil {
		return nil
	}
	return o.AuthType
}

func (o *OutputWebhook) GetTLS() *TLSSettingsClientSideWebhook {
	if o == nil {
		return nil
	}
	return o.TLS
}

func (o *OutputWebhook) GetTotalMemoryLimitKB() *float64 {
	if o == nil {
		return nil
	}
	return o.TotalMemoryLimitKB
}

func (o *OutputWebhook) GetLoadBalanced() *bool {
	if o == nil {
		return nil
	}
	return o.LoadBalanced
}

func (o *OutputWebhook) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputWebhook) GetCustomSourceExpression() *string {
	if o == nil {
		return nil
	}
	return o.CustomSourceExpression
}

func (o *OutputWebhook) GetCustomDropWhenNull() *bool {
	if o == nil {
		return nil
	}
	return o.CustomDropWhenNull
}

func (o *OutputWebhook) GetCustomEventDelimiter() *string {
	if o == nil {
		return nil
	}
	return o.CustomEventDelimiter
}

func (o *OutputWebhook) GetCustomContentType() *string {
	if o == nil {
		return nil
	}
	return o.CustomContentType
}

func (o *OutputWebhook) GetCustomPayloadExpression() *string {
	if o == nil {
		return nil
	}
	return o.CustomPayloadExpression
}

func (o *OutputWebhook) GetAdvancedContentType() *string {
	if o == nil {
		return nil
	}
	return o.AdvancedContentType
}

func (o *OutputWebhook) GetFormatEventCode() *string {
	if o == nil {
		return nil
	}
	return o.FormatEventCode
}

func (o *OutputWebhook) GetFormatPayloadCode() *string {
	if o == nil {
		return nil
	}
	return o.FormatPayloadCode
}

func (o *OutputWebhook) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputWebhook) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputWebhook) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputWebhook) GetPqCompress() *CompressionWebhook {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputWebhook) GetPqOnBackpressure() *QueueFullBehaviorWebhook {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputWebhook) GetPqMode() *ModeWebhook {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputWebhook) GetPqControls() *PqControlsWebhook {
	if o == nil {
		return nil
	}
	return o.PqControls
}

func (o *OutputWebhook) GetUsername() *string {
	if o == nil {
		return nil
	}
	return o.Username
}

func (o *OutputWebhook) GetPassword() *string {
	if o == nil {
		return nil
	}
	return o.Password
}

func (o *OutputWebhook) GetToken() *string {
	if o == nil {
		return nil
	}
	return o.Token
}

func (o *OutputWebhook) GetCredentialsSecret() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsSecret
}

func (o *OutputWebhook) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputWebhook) GetLoginURL() *string {
	if o == nil {
		return nil
	}
	return o.LoginURL
}

func (o *OutputWebhook) GetSecretParamName() *string {
	if o == nil {
		return nil
	}
	return o.SecretParamName
}

func (o *OutputWebhook) GetSecret() *string {
	if o == nil {
		return nil
	}
	return o.Secret
}

func (o *OutputWebhook) GetTokenAttributeName() *string {
	if o == nil {
		return nil
	}
	return o.TokenAttributeName
}

func (o *OutputWebhook) GetAuthHeaderExpr() *string {
	if o == nil {
		return nil
	}
	return o.AuthHeaderExpr
}

func (o *OutputWebhook) GetTokenTimeoutSecs() *float64 {
	if o == nil {
		return nil
	}
	return o.TokenTimeoutSecs
}

func (o *OutputWebhook) GetOauthParams() []OauthParamWebhook {
	if o == nil {
		return nil
	}
	return o.OauthParams
}

func (o *OutputWebhook) GetOauthHeaders() []OauthHeaderWebhook {
	if o == nil {
		return nil
	}
	return o.OauthHeaders
}

func (o *OutputWebhook) GetURL() *string {
	if o == nil {
		return nil
	}
	return o.URL
}

func (o *OutputWebhook) GetExcludeSelf() *bool {
	if o == nil {
		return nil
	}
	return o.ExcludeSelf
}

func (o *OutputWebhook) GetUrls() []URLWebhook {
	if o == nil {
		return nil
	}
	return o.Urls
}

func (o *OutputWebhook) GetDNSResolvePeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.DNSResolvePeriodSec
}

func (o *OutputWebhook) GetLoadBalanceStatsPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.LoadBalanceStatsPeriodSec
}

func (o *OutputWebhook) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type TypeDefault string

const (
	TypeDefaultDefault TypeDefault = "default"
)

func (e TypeDefault) ToPointer() *TypeDefault {
	return &e
}
func (e *TypeDefault) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "default":
		*e = TypeDefault(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TypeDefault: %v", v)
	}
}

type OutputDefault struct {
	// Unique ID for this output
	ID   *string     `json:"id,omitempty"`
	Type TypeDefault `json:"type"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// ID of the default output. This will be used whenever a nonexistent/deleted output is referenced.
	DefaultID string    `json:"defaultId"`
	Status    *TFStatus `json:"status,omitempty"`
}

func (o *OutputDefault) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputDefault) GetType() TypeDefault {
	if o == nil {
		return TypeDefault("")
	}
	return o.Type
}

func (o *OutputDefault) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputDefault) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputDefault) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputDefault) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputDefault) GetDefaultID() string {
	if o == nil {
		return ""
	}
	return o.DefaultID
}

func (o *OutputDefault) GetStatus() *TFStatus {
	if o == nil {
		return nil
	}
	return o.Status
}

type OutputType string

const (
	OutputTypeOutputDefault                OutputType = "OutputDefault"
	OutputTypeOutputWebhook                OutputType = "OutputWebhook"
	OutputTypeOutputSentinel               OutputType = "OutputSentinel"
	OutputTypeOutputDevnull                OutputType = "OutputDevnull"
	OutputTypeOutputSyslog                 OutputType = "OutputSyslog"
	OutputTypeOutputSplunk                 OutputType = "OutputSplunk"
	OutputTypeOutputSplunkLb               OutputType = "OutputSplunkLb"
	OutputTypeOutputSplunkHec              OutputType = "OutputSplunkHec"
	OutputTypeOutputTcpjson                OutputType = "OutputTcpjson"
	OutputTypeOutputWavefront              OutputType = "OutputWavefront"
	OutputTypeOutputSignalfx               OutputType = "OutputSignalfx"
	OutputTypeOutputFilesystem             OutputType = "OutputFilesystem"
	OutputTypeOutputS3                     OutputType = "OutputS3"
	OutputTypeOutputAzureBlob              OutputType = "OutputAzureBlob"
	OutputTypeOutputAzureDataExplorer      OutputType = "OutputAzureDataExplorer"
	OutputTypeOutputAzureLogs              OutputType = "OutputAzureLogs"
	OutputTypeOutputKinesis                OutputType = "OutputKinesis"
	OutputTypeOutputHoneycomb              OutputType = "OutputHoneycomb"
	OutputTypeOutputAzureEventhub          OutputType = "OutputAzureEventhub"
	OutputTypeOutputGoogleChronicle        OutputType = "OutputGoogleChronicle"
	OutputTypeOutputGoogleCloudStorage     OutputType = "OutputGoogleCloudStorage"
	OutputTypeOutputGoogleCloudLogging     OutputType = "OutputGoogleCloudLogging"
	OutputTypeOutputGooglePubsub           OutputType = "OutputGooglePubsub"
	OutputTypeOutputExabeam                OutputType = "OutputExabeam"
	OutputTypeOutputKafka                  OutputType = "OutputKafka"
	OutputTypeOutputConfluentCloud         OutputType = "OutputConfluentCloud"
	OutputTypeOutputMsk                    OutputType = "OutputMsk"
	OutputTypeOutputElastic                OutputType = "OutputElastic"
	OutputTypeOutputElasticCloud           OutputType = "OutputElasticCloud"
	OutputTypeOutputNewrelic               OutputType = "OutputNewrelic"
	OutputTypeOutputNewrelicEvents         OutputType = "OutputNewrelicEvents"
	OutputTypeOutputInfluxdb               OutputType = "OutputInfluxdb"
	OutputTypeOutputCloudwatch             OutputType = "OutputCloudwatch"
	OutputTypeOutputMinio                  OutputType = "OutputMinio"
	OutputTypeOutputStatsd                 OutputType = "OutputStatsd"
	OutputTypeOutputStatsdExt              OutputType = "OutputStatsdExt"
	OutputTypeOutputGraphite               OutputType = "OutputGraphite"
	OutputTypeOutputRouter                 OutputType = "OutputRouter"
	OutputTypeOutputSns                    OutputType = "OutputSns"
	OutputTypeOutputSqs                    OutputType = "OutputSqs"
	OutputTypeOutputSnmp                   OutputType = "OutputSnmp"
	OutputTypeOutputSumoLogic              OutputType = "OutputSumoLogic"
	OutputTypeOutputDatadog                OutputType = "OutputDatadog"
	OutputTypeOutputGrafanaCloud           OutputType = "OutputGrafanaCloud"
	OutputTypeOutputLoki                   OutputType = "OutputLoki"
	OutputTypeOutputPrometheus             OutputType = "OutputPrometheus"
	OutputTypeOutputRing                   OutputType = "OutputRing"
	OutputTypeOutputOpenTelemetry          OutputType = "OutputOpenTelemetry"
	OutputTypeOutputServiceNow             OutputType = "OutputServiceNow"
	OutputTypeOutputDataset                OutputType = "OutputDataset"
	OutputTypeOutputCriblTCP               OutputType = "OutputCriblTcp"
	OutputTypeOutputCriblHTTP              OutputType = "OutputCriblHttp"
	OutputTypeOutputHumioHec               OutputType = "OutputHumioHec"
	OutputTypeOutputCrowdstrikeNextGenSiem OutputType = "OutputCrowdstrikeNextGenSiem"
	OutputTypeOutputDlS3                   OutputType = "OutputDlS3"
	OutputTypeOutputSecurityLake           OutputType = "OutputSecurityLake"
	OutputTypeOutputCriblLake              OutputType = "OutputCriblLake"
	OutputTypeOutputDiskSpool              OutputType = "OutputDiskSpool"
	OutputTypeOutputClickHouse             OutputType = "OutputClickHouse"
	OutputTypeOutputXsiam                  OutputType = "OutputXsiam"
	OutputTypeOutputNetflow                OutputType = "OutputNetflow"
	OutputTypeOutputDynatraceHTTP          OutputType = "OutputDynatraceHttp"
	OutputTypeOutputDynatraceOtlp          OutputType = "OutputDynatraceOtlp"
)

type Output struct {
	OutputDefault                *OutputDefault                `queryParam:"inline"`
	OutputWebhook                *OutputWebhook                `queryParam:"inline"`
	OutputSentinel               *OutputSentinel               `queryParam:"inline"`
	OutputDevnull                *OutputDevnull                `queryParam:"inline"`
	OutputSyslog                 *OutputSyslog                 `queryParam:"inline"`
	OutputSplunk                 *OutputSplunk                 `queryParam:"inline"`
	OutputSplunkLb               *OutputSplunkLb               `queryParam:"inline"`
	OutputSplunkHec              *OutputSplunkHec              `queryParam:"inline"`
	OutputTcpjson                *OutputTcpjson                `queryParam:"inline"`
	OutputWavefront              *OutputWavefront              `queryParam:"inline"`
	OutputSignalfx               *OutputSignalfx               `queryParam:"inline"`
	OutputFilesystem             *OutputFilesystem             `queryParam:"inline"`
	OutputS3                     *OutputS3                     `queryParam:"inline"`
	OutputAzureBlob              *OutputAzureBlob              `queryParam:"inline"`
	OutputAzureDataExplorer      *OutputAzureDataExplorer      `queryParam:"inline"`
	OutputAzureLogs              *OutputAzureLogs              `queryParam:"inline"`
	OutputKinesis                *OutputKinesis                `queryParam:"inline"`
	OutputHoneycomb              *OutputHoneycomb              `queryParam:"inline"`
	OutputAzureEventhub          *OutputAzureEventhub          `queryParam:"inline"`
	OutputGoogleChronicle        *OutputGoogleChronicle        `queryParam:"inline"`
	OutputGoogleCloudStorage     *OutputGoogleCloudStorage     `queryParam:"inline"`
	OutputGoogleCloudLogging     *OutputGoogleCloudLogging     `queryParam:"inline"`
	OutputGooglePubsub           *OutputGooglePubsub           `queryParam:"inline"`
	OutputExabeam                *OutputExabeam                `queryParam:"inline"`
	OutputKafka                  *OutputKafka                  `queryParam:"inline"`
	OutputConfluentCloud         *OutputConfluentCloud         `queryParam:"inline"`
	OutputMsk                    *OutputMsk                    `queryParam:"inline"`
	OutputElastic                *OutputElastic                `queryParam:"inline"`
	OutputElasticCloud           *OutputElasticCloud           `queryParam:"inline"`
	OutputNewrelic               *OutputNewrelic               `queryParam:"inline"`
	OutputNewrelicEvents         *OutputNewrelicEvents         `queryParam:"inline"`
	OutputInfluxdb               *OutputInfluxdb               `queryParam:"inline"`
	OutputCloudwatch             *OutputCloudwatch             `queryParam:"inline"`
	OutputMinio                  *OutputMinio                  `queryParam:"inline"`
	OutputStatsd                 *OutputStatsd                 `queryParam:"inline"`
	OutputStatsdExt              *OutputStatsdExt              `queryParam:"inline"`
	OutputGraphite               *OutputGraphite               `queryParam:"inline"`
	OutputRouter                 *OutputRouter                 `queryParam:"inline"`
	OutputSns                    *OutputSns                    `queryParam:"inline"`
	OutputSqs                    *OutputSqs                    `queryParam:"inline"`
	OutputSnmp                   *OutputSnmp                   `queryParam:"inline"`
	OutputSumoLogic              *OutputSumoLogic              `queryParam:"inline"`
	OutputDatadog                *OutputDatadog                `queryParam:"inline"`
	OutputGrafanaCloud           *OutputGrafanaCloud           `queryParam:"inline"`
	OutputLoki                   *OutputLoki                   `queryParam:"inline"`
	OutputPrometheus             *OutputPrometheus             `queryParam:"inline"`
	OutputRing                   *OutputRing                   `queryParam:"inline"`
	OutputOpenTelemetry          *OutputOpenTelemetry          `queryParam:"inline"`
	OutputServiceNow             *OutputServiceNow             `queryParam:"inline"`
	OutputDataset                *OutputDataset                `queryParam:"inline"`
	OutputCriblTCP               *OutputCriblTCP               `queryParam:"inline"`
	OutputCriblHTTP              *OutputCriblHTTP              `queryParam:"inline"`
	OutputHumioHec               *OutputHumioHec               `queryParam:"inline"`
	OutputCrowdstrikeNextGenSiem *OutputCrowdstrikeNextGenSiem `queryParam:"inline"`
	OutputDlS3                   *OutputDlS3                   `queryParam:"inline"`
	OutputSecurityLake           *OutputSecurityLake           `queryParam:"inline"`
	OutputCriblLake              *OutputCriblLake              `queryParam:"inline"`
	OutputDiskSpool              *OutputDiskSpool              `queryParam:"inline"`
	OutputClickHouse             *OutputClickHouse             `queryParam:"inline"`
	OutputXsiam                  *OutputXsiam                  `queryParam:"inline"`
	OutputNetflow                *OutputNetflow                `queryParam:"inline"`
	OutputDynatraceHTTP          *OutputDynatraceHTTP          `queryParam:"inline"`
	OutputDynatraceOtlp          *OutputDynatraceOtlp          `queryParam:"inline"`

	Type OutputType
}

func CreateOutputOutputDefault(outputDefault OutputDefault) Output {
	typ := OutputTypeOutputDefault

	return Output{
		OutputDefault: &outputDefault,
		Type:          typ,
	}
}

func CreateOutputOutputWebhook(outputWebhook OutputWebhook) Output {
	typ := OutputTypeOutputWebhook

	return Output{
		OutputWebhook: &outputWebhook,
		Type:          typ,
	}
}

func CreateOutputOutputSentinel(outputSentinel OutputSentinel) Output {
	typ := OutputTypeOutputSentinel

	return Output{
		OutputSentinel: &outputSentinel,
		Type:           typ,
	}
}

func CreateOutputOutputDevnull(outputDevnull OutputDevnull) Output {
	typ := OutputTypeOutputDevnull

	return Output{
		OutputDevnull: &outputDevnull,
		Type:          typ,
	}
}

func CreateOutputOutputSyslog(outputSyslog OutputSyslog) Output {
	typ := OutputTypeOutputSyslog

	return Output{
		OutputSyslog: &outputSyslog,
		Type:         typ,
	}
}

func CreateOutputOutputSplunk(outputSplunk OutputSplunk) Output {
	typ := OutputTypeOutputSplunk

	return Output{
		OutputSplunk: &outputSplunk,
		Type:         typ,
	}
}

func CreateOutputOutputSplunkLb(outputSplunkLb OutputSplunkLb) Output {
	typ := OutputTypeOutputSplunkLb

	return Output{
		OutputSplunkLb: &outputSplunkLb,
		Type:           typ,
	}
}

func CreateOutputOutputSplunkHec(outputSplunkHec OutputSplunkHec) Output {
	typ := OutputTypeOutputSplunkHec

	return Output{
		OutputSplunkHec: &outputSplunkHec,
		Type:            typ,
	}
}

func CreateOutputOutputTcpjson(outputTcpjson OutputTcpjson) Output {
	typ := OutputTypeOutputTcpjson

	return Output{
		OutputTcpjson: &outputTcpjson,
		Type:          typ,
	}
}

func CreateOutputOutputWavefront(outputWavefront OutputWavefront) Output {
	typ := OutputTypeOutputWavefront

	return Output{
		OutputWavefront: &outputWavefront,
		Type:            typ,
	}
}

func CreateOutputOutputSignalfx(outputSignalfx OutputSignalfx) Output {
	typ := OutputTypeOutputSignalfx

	return Output{
		OutputSignalfx: &outputSignalfx,
		Type:           typ,
	}
}

func CreateOutputOutputFilesystem(outputFilesystem OutputFilesystem) Output {
	typ := OutputTypeOutputFilesystem

	return Output{
		OutputFilesystem: &outputFilesystem,
		Type:             typ,
	}
}

func CreateOutputOutputS3(outputS3 OutputS3) Output {
	typ := OutputTypeOutputS3

	return Output{
		OutputS3: &outputS3,
		Type:     typ,
	}
}

func CreateOutputOutputAzureBlob(outputAzureBlob OutputAzureBlob) Output {
	typ := OutputTypeOutputAzureBlob

	return Output{
		OutputAzureBlob: &outputAzureBlob,
		Type:            typ,
	}
}

func CreateOutputOutputAzureDataExplorer(outputAzureDataExplorer OutputAzureDataExplorer) Output {
	typ := OutputTypeOutputAzureDataExplorer

	return Output{
		OutputAzureDataExplorer: &outputAzureDataExplorer,
		Type:                    typ,
	}
}

func CreateOutputOutputAzureLogs(outputAzureLogs OutputAzureLogs) Output {
	typ := OutputTypeOutputAzureLogs

	return Output{
		OutputAzureLogs: &outputAzureLogs,
		Type:            typ,
	}
}

func CreateOutputOutputKinesis(outputKinesis OutputKinesis) Output {
	typ := OutputTypeOutputKinesis

	return Output{
		OutputKinesis: &outputKinesis,
		Type:          typ,
	}
}

func CreateOutputOutputHoneycomb(outputHoneycomb OutputHoneycomb) Output {
	typ := OutputTypeOutputHoneycomb

	return Output{
		OutputHoneycomb: &outputHoneycomb,
		Type:            typ,
	}
}

func CreateOutputOutputAzureEventhub(outputAzureEventhub OutputAzureEventhub) Output {
	typ := OutputTypeOutputAzureEventhub

	return Output{
		OutputAzureEventhub: &outputAzureEventhub,
		Type:                typ,
	}
}

func CreateOutputOutputGoogleChronicle(outputGoogleChronicle OutputGoogleChronicle) Output {
	typ := OutputTypeOutputGoogleChronicle

	return Output{
		OutputGoogleChronicle: &outputGoogleChronicle,
		Type:                  typ,
	}
}

func CreateOutputOutputGoogleCloudStorage(outputGoogleCloudStorage OutputGoogleCloudStorage) Output {
	typ := OutputTypeOutputGoogleCloudStorage

	return Output{
		OutputGoogleCloudStorage: &outputGoogleCloudStorage,
		Type:                     typ,
	}
}

func CreateOutputOutputGoogleCloudLogging(outputGoogleCloudLogging OutputGoogleCloudLogging) Output {
	typ := OutputTypeOutputGoogleCloudLogging

	return Output{
		OutputGoogleCloudLogging: &outputGoogleCloudLogging,
		Type:                     typ,
	}
}

func CreateOutputOutputGooglePubsub(outputGooglePubsub OutputGooglePubsub) Output {
	typ := OutputTypeOutputGooglePubsub

	return Output{
		OutputGooglePubsub: &outputGooglePubsub,
		Type:               typ,
	}
}

func CreateOutputOutputExabeam(outputExabeam OutputExabeam) Output {
	typ := OutputTypeOutputExabeam

	return Output{
		OutputExabeam: &outputExabeam,
		Type:          typ,
	}
}

func CreateOutputOutputKafka(outputKafka OutputKafka) Output {
	typ := OutputTypeOutputKafka

	return Output{
		OutputKafka: &outputKafka,
		Type:        typ,
	}
}

func CreateOutputOutputConfluentCloud(outputConfluentCloud OutputConfluentCloud) Output {
	typ := OutputTypeOutputConfluentCloud

	return Output{
		OutputConfluentCloud: &outputConfluentCloud,
		Type:                 typ,
	}
}

func CreateOutputOutputMsk(outputMsk OutputMsk) Output {
	typ := OutputTypeOutputMsk

	return Output{
		OutputMsk: &outputMsk,
		Type:      typ,
	}
}

func CreateOutputOutputElastic(outputElastic OutputElastic) Output {
	typ := OutputTypeOutputElastic

	return Output{
		OutputElastic: &outputElastic,
		Type:          typ,
	}
}

func CreateOutputOutputElasticCloud(outputElasticCloud OutputElasticCloud) Output {
	typ := OutputTypeOutputElasticCloud

	return Output{
		OutputElasticCloud: &outputElasticCloud,
		Type:               typ,
	}
}

func CreateOutputOutputNewrelic(outputNewrelic OutputNewrelic) Output {
	typ := OutputTypeOutputNewrelic

	return Output{
		OutputNewrelic: &outputNewrelic,
		Type:           typ,
	}
}

func CreateOutputOutputNewrelicEvents(outputNewrelicEvents OutputNewrelicEvents) Output {
	typ := OutputTypeOutputNewrelicEvents

	return Output{
		OutputNewrelicEvents: &outputNewrelicEvents,
		Type:                 typ,
	}
}

func CreateOutputOutputInfluxdb(outputInfluxdb OutputInfluxdb) Output {
	typ := OutputTypeOutputInfluxdb

	return Output{
		OutputInfluxdb: &outputInfluxdb,
		Type:           typ,
	}
}

func CreateOutputOutputCloudwatch(outputCloudwatch OutputCloudwatch) Output {
	typ := OutputTypeOutputCloudwatch

	return Output{
		OutputCloudwatch: &outputCloudwatch,
		Type:             typ,
	}
}

func CreateOutputOutputMinio(outputMinio OutputMinio) Output {
	typ := OutputTypeOutputMinio

	return Output{
		OutputMinio: &outputMinio,
		Type:        typ,
	}
}

func CreateOutputOutputStatsd(outputStatsd OutputStatsd) Output {
	typ := OutputTypeOutputStatsd

	return Output{
		OutputStatsd: &outputStatsd,
		Type:         typ,
	}
}

func CreateOutputOutputStatsdExt(outputStatsdExt OutputStatsdExt) Output {
	typ := OutputTypeOutputStatsdExt

	return Output{
		OutputStatsdExt: &outputStatsdExt,
		Type:            typ,
	}
}

func CreateOutputOutputGraphite(outputGraphite OutputGraphite) Output {
	typ := OutputTypeOutputGraphite

	return Output{
		OutputGraphite: &outputGraphite,
		Type:           typ,
	}
}

func CreateOutputOutputRouter(outputRouter OutputRouter) Output {
	typ := OutputTypeOutputRouter

	return Output{
		OutputRouter: &outputRouter,
		Type:         typ,
	}
}

func CreateOutputOutputSns(outputSns OutputSns) Output {
	typ := OutputTypeOutputSns

	return Output{
		OutputSns: &outputSns,
		Type:      typ,
	}
}

func CreateOutputOutputSqs(outputSqs OutputSqs) Output {
	typ := OutputTypeOutputSqs

	return Output{
		OutputSqs: &outputSqs,
		Type:      typ,
	}
}

func CreateOutputOutputSnmp(outputSnmp OutputSnmp) Output {
	typ := OutputTypeOutputSnmp

	return Output{
		OutputSnmp: &outputSnmp,
		Type:       typ,
	}
}

func CreateOutputOutputSumoLogic(outputSumoLogic OutputSumoLogic) Output {
	typ := OutputTypeOutputSumoLogic

	return Output{
		OutputSumoLogic: &outputSumoLogic,
		Type:            typ,
	}
}

func CreateOutputOutputDatadog(outputDatadog OutputDatadog) Output {
	typ := OutputTypeOutputDatadog

	return Output{
		OutputDatadog: &outputDatadog,
		Type:          typ,
	}
}

func CreateOutputOutputGrafanaCloud(outputGrafanaCloud OutputGrafanaCloud) Output {
	typ := OutputTypeOutputGrafanaCloud

	return Output{
		OutputGrafanaCloud: &outputGrafanaCloud,
		Type:               typ,
	}
}

func CreateOutputOutputLoki(outputLoki OutputLoki) Output {
	typ := OutputTypeOutputLoki

	return Output{
		OutputLoki: &outputLoki,
		Type:       typ,
	}
}

func CreateOutputOutputPrometheus(outputPrometheus OutputPrometheus) Output {
	typ := OutputTypeOutputPrometheus

	return Output{
		OutputPrometheus: &outputPrometheus,
		Type:             typ,
	}
}

func CreateOutputOutputRing(outputRing OutputRing) Output {
	typ := OutputTypeOutputRing

	return Output{
		OutputRing: &outputRing,
		Type:       typ,
	}
}

func CreateOutputOutputOpenTelemetry(outputOpenTelemetry OutputOpenTelemetry) Output {
	typ := OutputTypeOutputOpenTelemetry

	return Output{
		OutputOpenTelemetry: &outputOpenTelemetry,
		Type:                typ,
	}
}

func CreateOutputOutputServiceNow(outputServiceNow OutputServiceNow) Output {
	typ := OutputTypeOutputServiceNow

	return Output{
		OutputServiceNow: &outputServiceNow,
		Type:             typ,
	}
}

func CreateOutputOutputDataset(outputDataset OutputDataset) Output {
	typ := OutputTypeOutputDataset

	return Output{
		OutputDataset: &outputDataset,
		Type:          typ,
	}
}

func CreateOutputOutputCriblTCP(outputCriblTCP OutputCriblTCP) Output {
	typ := OutputTypeOutputCriblTCP

	return Output{
		OutputCriblTCP: &outputCriblTCP,
		Type:           typ,
	}
}

func CreateOutputOutputCriblHTTP(outputCriblHTTP OutputCriblHTTP) Output {
	typ := OutputTypeOutputCriblHTTP

	return Output{
		OutputCriblHTTP: &outputCriblHTTP,
		Type:            typ,
	}
}

func CreateOutputOutputHumioHec(outputHumioHec OutputHumioHec) Output {
	typ := OutputTypeOutputHumioHec

	return Output{
		OutputHumioHec: &outputHumioHec,
		Type:           typ,
	}
}

func CreateOutputOutputCrowdstrikeNextGenSiem(outputCrowdstrikeNextGenSiem OutputCrowdstrikeNextGenSiem) Output {
	typ := OutputTypeOutputCrowdstrikeNextGenSiem

	return Output{
		OutputCrowdstrikeNextGenSiem: &outputCrowdstrikeNextGenSiem,
		Type:                         typ,
	}
}

func CreateOutputOutputDlS3(outputDlS3 OutputDlS3) Output {
	typ := OutputTypeOutputDlS3

	return Output{
		OutputDlS3: &outputDlS3,
		Type:       typ,
	}
}

func CreateOutputOutputSecurityLake(outputSecurityLake OutputSecurityLake) Output {
	typ := OutputTypeOutputSecurityLake

	return Output{
		OutputSecurityLake: &outputSecurityLake,
		Type:               typ,
	}
}

func CreateOutputOutputCriblLake(outputCriblLake OutputCriblLake) Output {
	typ := OutputTypeOutputCriblLake

	return Output{
		OutputCriblLake: &outputCriblLake,
		Type:            typ,
	}
}

func CreateOutputOutputDiskSpool(outputDiskSpool OutputDiskSpool) Output {
	typ := OutputTypeOutputDiskSpool

	return Output{
		OutputDiskSpool: &outputDiskSpool,
		Type:            typ,
	}
}

func CreateOutputOutputClickHouse(outputClickHouse OutputClickHouse) Output {
	typ := OutputTypeOutputClickHouse

	return Output{
		OutputClickHouse: &outputClickHouse,
		Type:             typ,
	}
}

func CreateOutputOutputXsiam(outputXsiam OutputXsiam) Output {
	typ := OutputTypeOutputXsiam

	return Output{
		OutputXsiam: &outputXsiam,
		Type:        typ,
	}
}

func CreateOutputOutputNetflow(outputNetflow OutputNetflow) Output {
	typ := OutputTypeOutputNetflow

	return Output{
		OutputNetflow: &outputNetflow,
		Type:          typ,
	}
}

func CreateOutputOutputDynatraceHTTP(outputDynatraceHTTP OutputDynatraceHTTP) Output {
	typ := OutputTypeOutputDynatraceHTTP

	return Output{
		OutputDynatraceHTTP: &outputDynatraceHTTP,
		Type:                typ,
	}
}

func CreateOutputOutputDynatraceOtlp(outputDynatraceOtlp OutputDynatraceOtlp) Output {
	typ := OutputTypeOutputDynatraceOtlp

	return Output{
		OutputDynatraceOtlp: &outputDynatraceOtlp,
		Type:                typ,
	}
}

func (u *Output) UnmarshalJSON(data []byte) error {

	var outputDevnull OutputDevnull = OutputDevnull{}
	if err := utils.UnmarshalJSON(data, &outputDevnull, "", true, true); err == nil {
		u.OutputDevnull = &outputDevnull
		u.Type = OutputTypeOutputDevnull
		return nil
	}

	var outputDefault OutputDefault = OutputDefault{}
	if err := utils.UnmarshalJSON(data, &outputDefault, "", true, true); err == nil {
		u.OutputDefault = &outputDefault
		u.Type = OutputTypeOutputDefault
		return nil
	}

	var outputRouter OutputRouter = OutputRouter{}
	if err := utils.UnmarshalJSON(data, &outputRouter, "", true, true); err == nil {
		u.OutputRouter = &outputRouter
		u.Type = OutputTypeOutputRouter
		return nil
	}

	var outputNetflow OutputNetflow = OutputNetflow{}
	if err := utils.UnmarshalJSON(data, &outputNetflow, "", true, true); err == nil {
		u.OutputNetflow = &outputNetflow
		u.Type = OutputTypeOutputNetflow
		return nil
	}

	var outputSnmp OutputSnmp = OutputSnmp{}
	if err := utils.UnmarshalJSON(data, &outputSnmp, "", true, true); err == nil {
		u.OutputSnmp = &outputSnmp
		u.Type = OutputTypeOutputSnmp
		return nil
	}

	var outputDiskSpool OutputDiskSpool = OutputDiskSpool{}
	if err := utils.UnmarshalJSON(data, &outputDiskSpool, "", true, true); err == nil {
		u.OutputDiskSpool = &outputDiskSpool
		u.Type = OutputTypeOutputDiskSpool
		return nil
	}

	var outputRing OutputRing = OutputRing{}
	if err := utils.UnmarshalJSON(data, &outputRing, "", true, true); err == nil {
		u.OutputRing = &outputRing
		u.Type = OutputTypeOutputRing
		return nil
	}

	var outputGraphite OutputGraphite = OutputGraphite{}
	if err := utils.UnmarshalJSON(data, &outputGraphite, "", true, true); err == nil {
		u.OutputGraphite = &outputGraphite
		u.Type = OutputTypeOutputGraphite
		return nil
	}

	var outputStatsdExt OutputStatsdExt = OutputStatsdExt{}
	if err := utils.UnmarshalJSON(data, &outputStatsdExt, "", true, true); err == nil {
		u.OutputStatsdExt = &outputStatsdExt
		u.Type = OutputTypeOutputStatsdExt
		return nil
	}

	var outputStatsd OutputStatsd = OutputStatsd{}
	if err := utils.UnmarshalJSON(data, &outputStatsd, "", true, true); err == nil {
		u.OutputStatsd = &outputStatsd
		u.Type = OutputTypeOutputStatsd
		return nil
	}

	var outputGooglePubsub OutputGooglePubsub = OutputGooglePubsub{}
	if err := utils.UnmarshalJSON(data, &outputGooglePubsub, "", true, true); err == nil {
		u.OutputGooglePubsub = &outputGooglePubsub
		u.Type = OutputTypeOutputGooglePubsub
		return nil
	}

	var outputSplunk OutputSplunk = OutputSplunk{}
	if err := utils.UnmarshalJSON(data, &outputSplunk, "", true, true); err == nil {
		u.OutputSplunk = &outputSplunk
		u.Type = OutputTypeOutputSplunk
		return nil
	}

	var outputSns OutputSns = OutputSns{}
	if err := utils.UnmarshalJSON(data, &outputSns, "", true, true); err == nil {
		u.OutputSns = &outputSns
		u.Type = OutputTypeOutputSns
		return nil
	}

	var outputCriblTCP OutputCriblTCP = OutputCriblTCP{}
	if err := utils.UnmarshalJSON(data, &outputCriblTCP, "", true, true); err == nil {
		u.OutputCriblTCP = &outputCriblTCP
		u.Type = OutputTypeOutputCriblTCP
		return nil
	}

	var outputCloudwatch OutputCloudwatch = OutputCloudwatch{}
	if err := utils.UnmarshalJSON(data, &outputCloudwatch, "", true, true); err == nil {
		u.OutputCloudwatch = &outputCloudwatch
		u.Type = OutputTypeOutputCloudwatch
		return nil
	}

	var outputAzureEventhub OutputAzureEventhub = OutputAzureEventhub{}
	if err := utils.UnmarshalJSON(data, &outputAzureEventhub, "", true, true); err == nil {
		u.OutputAzureEventhub = &outputAzureEventhub
		u.Type = OutputTypeOutputAzureEventhub
		return nil
	}

	var outputSyslog OutputSyslog = OutputSyslog{}
	if err := utils.UnmarshalJSON(data, &outputSyslog, "", true, true); err == nil {
		u.OutputSyslog = &outputSyslog
		u.Type = OutputTypeOutputSyslog
		return nil
	}

	var outputHoneycomb OutputHoneycomb = OutputHoneycomb{}
	if err := utils.UnmarshalJSON(data, &outputHoneycomb, "", true, true); err == nil {
		u.OutputHoneycomb = &outputHoneycomb
		u.Type = OutputTypeOutputHoneycomb
		return nil
	}

	var outputSignalfx OutputSignalfx = OutputSignalfx{}
	if err := utils.UnmarshalJSON(data, &outputSignalfx, "", true, true); err == nil {
		u.OutputSignalfx = &outputSignalfx
		u.Type = OutputTypeOutputSignalfx
		return nil
	}

	var outputWavefront OutputWavefront = OutputWavefront{}
	if err := utils.UnmarshalJSON(data, &outputWavefront, "", true, true); err == nil {
		u.OutputWavefront = &outputWavefront
		u.Type = OutputTypeOutputWavefront
		return nil
	}

	var outputSumoLogic OutputSumoLogic = OutputSumoLogic{}
	if err := utils.UnmarshalJSON(data, &outputSumoLogic, "", true, true); err == nil {
		u.OutputSumoLogic = &outputSumoLogic
		u.Type = OutputTypeOutputSumoLogic
		return nil
	}

	var outputHumioHec OutputHumioHec = OutputHumioHec{}
	if err := utils.UnmarshalJSON(data, &outputHumioHec, "", true, true); err == nil {
		u.OutputHumioHec = &outputHumioHec
		u.Type = OutputTypeOutputHumioHec
		return nil
	}

	var outputTcpjson OutputTcpjson = OutputTcpjson{}
	if err := utils.UnmarshalJSON(data, &outputTcpjson, "", true, true); err == nil {
		u.OutputTcpjson = &outputTcpjson
		u.Type = OutputTypeOutputTcpjson
		return nil
	}

	var outputCrowdstrikeNextGenSiem OutputCrowdstrikeNextGenSiem = OutputCrowdstrikeNextGenSiem{}
	if err := utils.UnmarshalJSON(data, &outputCrowdstrikeNextGenSiem, "", true, true); err == nil {
		u.OutputCrowdstrikeNextGenSiem = &outputCrowdstrikeNextGenSiem
		u.Type = OutputTypeOutputCrowdstrikeNextGenSiem
		return nil
	}

	var outputElasticCloud OutputElasticCloud = OutputElasticCloud{}
	if err := utils.UnmarshalJSON(data, &outputElasticCloud, "", true, true); err == nil {
		u.OutputElasticCloud = &outputElasticCloud
		u.Type = OutputTypeOutputElasticCloud
		return nil
	}

	var outputKinesis OutputKinesis = OutputKinesis{}
	if err := utils.UnmarshalJSON(data, &outputKinesis, "", true, true); err == nil {
		u.OutputKinesis = &outputKinesis
		u.Type = OutputTypeOutputKinesis
		return nil
	}

	var outputConfluentCloud OutputConfluentCloud = OutputConfluentCloud{}
	if err := utils.UnmarshalJSON(data, &outputConfluentCloud, "", true, true); err == nil {
		u.OutputConfluentCloud = &outputConfluentCloud
		u.Type = OutputTypeOutputConfluentCloud
		return nil
	}

	var outputKafka OutputKafka = OutputKafka{}
	if err := utils.UnmarshalJSON(data, &outputKafka, "", true, true); err == nil {
		u.OutputKafka = &outputKafka
		u.Type = OutputTypeOutputKafka
		return nil
	}

	var outputExabeam OutputExabeam = OutputExabeam{}
	if err := utils.UnmarshalJSON(data, &outputExabeam, "", true, true); err == nil {
		u.OutputExabeam = &outputExabeam
		u.Type = OutputTypeOutputExabeam
		return nil
	}

	var outputNewrelicEvents OutputNewrelicEvents = OutputNewrelicEvents{}
	if err := utils.UnmarshalJSON(data, &outputNewrelicEvents, "", true, true); err == nil {
		u.OutputNewrelicEvents = &outputNewrelicEvents
		u.Type = OutputTypeOutputNewrelicEvents
		return nil
	}

	var outputAzureLogs OutputAzureLogs = OutputAzureLogs{}
	if err := utils.UnmarshalJSON(data, &outputAzureLogs, "", true, true); err == nil {
		u.OutputAzureLogs = &outputAzureLogs
		u.Type = OutputTypeOutputAzureLogs
		return nil
	}

	var outputSqs OutputSqs = OutputSqs{}
	if err := utils.UnmarshalJSON(data, &outputSqs, "", true, true); err == nil {
		u.OutputSqs = &outputSqs
		u.Type = OutputTypeOutputSqs
		return nil
	}

	var outputSplunkLb OutputSplunkLb = OutputSplunkLb{}
	if err := utils.UnmarshalJSON(data, &outputSplunkLb, "", true, true); err == nil {
		u.OutputSplunkLb = &outputSplunkLb
		u.Type = OutputTypeOutputSplunkLb
		return nil
	}

	var outputNewrelic OutputNewrelic = OutputNewrelic{}
	if err := utils.UnmarshalJSON(data, &outputNewrelic, "", true, true); err == nil {
		u.OutputNewrelic = &outputNewrelic
		u.Type = OutputTypeOutputNewrelic
		return nil
	}

	var outputXsiam OutputXsiam = OutputXsiam{}
	if err := utils.UnmarshalJSON(data, &outputXsiam, "", true, true); err == nil {
		u.OutputXsiam = &outputXsiam
		u.Type = OutputTypeOutputXsiam
		return nil
	}

	var outputCriblHTTP OutputCriblHTTP = OutputCriblHTTP{}
	if err := utils.UnmarshalJSON(data, &outputCriblHTTP, "", true, true); err == nil {
		u.OutputCriblHTTP = &outputCriblHTTP
		u.Type = OutputTypeOutputCriblHTTP
		return nil
	}

	var outputFilesystem OutputFilesystem = OutputFilesystem{}
	if err := utils.UnmarshalJSON(data, &outputFilesystem, "", true, true); err == nil {
		u.OutputFilesystem = &outputFilesystem
		u.Type = OutputTypeOutputFilesystem
		return nil
	}

	var outputDataset OutputDataset = OutputDataset{}
	if err := utils.UnmarshalJSON(data, &outputDataset, "", true, true); err == nil {
		u.OutputDataset = &outputDataset
		u.Type = OutputTypeOutputDataset
		return nil
	}

	var outputLoki OutputLoki = OutputLoki{}
	if err := utils.UnmarshalJSON(data, &outputLoki, "", true, true); err == nil {
		u.OutputLoki = &outputLoki
		u.Type = OutputTypeOutputLoki
		return nil
	}

	var outputDynatraceHTTP OutputDynatraceHTTP = OutputDynatraceHTTP{}
	if err := utils.UnmarshalJSON(data, &outputDynatraceHTTP, "", true, true); err == nil {
		u.OutputDynatraceHTTP = &outputDynatraceHTTP
		u.Type = OutputTypeOutputDynatraceHTTP
		return nil
	}

	var outputSplunkHec OutputSplunkHec = OutputSplunkHec{}
	if err := utils.UnmarshalJSON(data, &outputSplunkHec, "", true, true); err == nil {
		u.OutputSplunkHec = &outputSplunkHec
		u.Type = OutputTypeOutputSplunkHec
		return nil
	}

	var outputDynatraceOtlp OutputDynatraceOtlp = OutputDynatraceOtlp{}
	if err := utils.UnmarshalJSON(data, &outputDynatraceOtlp, "", true, true); err == nil {
		u.OutputDynatraceOtlp = &outputDynatraceOtlp
		u.Type = OutputTypeOutputDynatraceOtlp
		return nil
	}

	var outputServiceNow OutputServiceNow = OutputServiceNow{}
	if err := utils.UnmarshalJSON(data, &outputServiceNow, "", true, true); err == nil {
		u.OutputServiceNow = &outputServiceNow
		u.Type = OutputTypeOutputServiceNow
		return nil
	}

	var outputGoogleChronicle OutputGoogleChronicle = OutputGoogleChronicle{}
	if err := utils.UnmarshalJSON(data, &outputGoogleChronicle, "", true, true); err == nil {
		u.OutputGoogleChronicle = &outputGoogleChronicle
		u.Type = OutputTypeOutputGoogleChronicle
		return nil
	}

	var outputElastic OutputElastic = OutputElastic{}
	if err := utils.UnmarshalJSON(data, &outputElastic, "", true, true); err == nil {
		u.OutputElastic = &outputElastic
		u.Type = OutputTypeOutputElastic
		return nil
	}

	var outputDatadog OutputDatadog = OutputDatadog{}
	if err := utils.UnmarshalJSON(data, &outputDatadog, "", true, true); err == nil {
		u.OutputDatadog = &outputDatadog
		u.Type = OutputTypeOutputDatadog
		return nil
	}

	var outputPrometheus OutputPrometheus = OutputPrometheus{}
	if err := utils.UnmarshalJSON(data, &outputPrometheus, "", true, true); err == nil {
		u.OutputPrometheus = &outputPrometheus
		u.Type = OutputTypeOutputPrometheus
		return nil
	}

	var outputCriblLake OutputCriblLake = OutputCriblLake{}
	if err := utils.UnmarshalJSON(data, &outputCriblLake, "", true, true); err == nil {
		u.OutputCriblLake = &outputCriblLake
		u.Type = OutputTypeOutputCriblLake
		return nil
	}

	var outputMsk OutputMsk = OutputMsk{}
	if err := utils.UnmarshalJSON(data, &outputMsk, "", true, true); err == nil {
		u.OutputMsk = &outputMsk
		u.Type = OutputTypeOutputMsk
		return nil
	}

	var outputSentinel OutputSentinel = OutputSentinel{}
	if err := utils.UnmarshalJSON(data, &outputSentinel, "", true, true); err == nil {
		u.OutputSentinel = &outputSentinel
		u.Type = OutputTypeOutputSentinel
		return nil
	}

	var outputInfluxdb OutputInfluxdb = OutputInfluxdb{}
	if err := utils.UnmarshalJSON(data, &outputInfluxdb, "", true, true); err == nil {
		u.OutputInfluxdb = &outputInfluxdb
		u.Type = OutputTypeOutputInfluxdb
		return nil
	}

	var outputAzureBlob OutputAzureBlob = OutputAzureBlob{}
	if err := utils.UnmarshalJSON(data, &outputAzureBlob, "", true, true); err == nil {
		u.OutputAzureBlob = &outputAzureBlob
		u.Type = OutputTypeOutputAzureBlob
		return nil
	}

	var outputGoogleCloudStorage OutputGoogleCloudStorage = OutputGoogleCloudStorage{}
	if err := utils.UnmarshalJSON(data, &outputGoogleCloudStorage, "", true, true); err == nil {
		u.OutputGoogleCloudStorage = &outputGoogleCloudStorage
		u.Type = OutputTypeOutputGoogleCloudStorage
		return nil
	}

	var outputOpenTelemetry OutputOpenTelemetry = OutputOpenTelemetry{}
	if err := utils.UnmarshalJSON(data, &outputOpenTelemetry, "", true, true); err == nil {
		u.OutputOpenTelemetry = &outputOpenTelemetry
		u.Type = OutputTypeOutputOpenTelemetry
		return nil
	}

	var outputMinio OutputMinio = OutputMinio{}
	if err := utils.UnmarshalJSON(data, &outputMinio, "", true, true); err == nil {
		u.OutputMinio = &outputMinio
		u.Type = OutputTypeOutputMinio
		return nil
	}

	var outputClickHouse OutputClickHouse = OutputClickHouse{}
	if err := utils.UnmarshalJSON(data, &outputClickHouse, "", true, true); err == nil {
		u.OutputClickHouse = &outputClickHouse
		u.Type = OutputTypeOutputClickHouse
		return nil
	}

	var outputSecurityLake OutputSecurityLake = OutputSecurityLake{}
	if err := utils.UnmarshalJSON(data, &outputSecurityLake, "", true, true); err == nil {
		u.OutputSecurityLake = &outputSecurityLake
		u.Type = OutputTypeOutputSecurityLake
		return nil
	}

	var outputDlS3 OutputDlS3 = OutputDlS3{}
	if err := utils.UnmarshalJSON(data, &outputDlS3, "", true, true); err == nil {
		u.OutputDlS3 = &outputDlS3
		u.Type = OutputTypeOutputDlS3
		return nil
	}

	var outputS3 OutputS3 = OutputS3{}
	if err := utils.UnmarshalJSON(data, &outputS3, "", true, true); err == nil {
		u.OutputS3 = &outputS3
		u.Type = OutputTypeOutputS3
		return nil
	}

	var outputWebhook OutputWebhook = OutputWebhook{}
	if err := utils.UnmarshalJSON(data, &outputWebhook, "", true, true); err == nil {
		u.OutputWebhook = &outputWebhook
		u.Type = OutputTypeOutputWebhook
		return nil
	}

	var outputAzureDataExplorer OutputAzureDataExplorer = OutputAzureDataExplorer{}
	if err := utils.UnmarshalJSON(data, &outputAzureDataExplorer, "", true, true); err == nil {
		u.OutputAzureDataExplorer = &outputAzureDataExplorer
		u.Type = OutputTypeOutputAzureDataExplorer
		return nil
	}

	var outputGoogleCloudLogging OutputGoogleCloudLogging = OutputGoogleCloudLogging{}
	if err := utils.UnmarshalJSON(data, &outputGoogleCloudLogging, "", true, true); err == nil {
		u.OutputGoogleCloudLogging = &outputGoogleCloudLogging
		u.Type = OutputTypeOutputGoogleCloudLogging
		return nil
	}

	var outputGrafanaCloud OutputGrafanaCloud = OutputGrafanaCloud{}
	if err := utils.UnmarshalJSON(data, &outputGrafanaCloud, "", true, true); err == nil {
		u.OutputGrafanaCloud = &outputGrafanaCloud
		u.Type = OutputTypeOutputGrafanaCloud
		return nil
	}

	return fmt.Errorf("could not unmarshal `%s` into any supported union types for Output", string(data))
}

func (u Output) MarshalJSON() ([]byte, error) {
	if u.OutputDefault != nil {
		return utils.MarshalJSON(u.OutputDefault, "", true)
	}

	if u.OutputWebhook != nil {
		return utils.MarshalJSON(u.OutputWebhook, "", true)
	}

	if u.OutputSentinel != nil {
		return utils.MarshalJSON(u.OutputSentinel, "", true)
	}

	if u.OutputDevnull != nil {
		return utils.MarshalJSON(u.OutputDevnull, "", true)
	}

	if u.OutputSyslog != nil {
		return utils.MarshalJSON(u.OutputSyslog, "", true)
	}

	if u.OutputSplunk != nil {
		return utils.MarshalJSON(u.OutputSplunk, "", true)
	}

	if u.OutputSplunkLb != nil {
		return utils.MarshalJSON(u.OutputSplunkLb, "", true)
	}

	if u.OutputSplunkHec != nil {
		return utils.MarshalJSON(u.OutputSplunkHec, "", true)
	}

	if u.OutputTcpjson != nil {
		return utils.MarshalJSON(u.OutputTcpjson, "", true)
	}

	if u.OutputWavefront != nil {
		return utils.MarshalJSON(u.OutputWavefront, "", true)
	}

	if u.OutputSignalfx != nil {
		return utils.MarshalJSON(u.OutputSignalfx, "", true)
	}

	if u.OutputFilesystem != nil {
		return utils.MarshalJSON(u.OutputFilesystem, "", true)
	}

	if u.OutputS3 != nil {
		return utils.MarshalJSON(u.OutputS3, "", true)
	}

	if u.OutputAzureBlob != nil {
		return utils.MarshalJSON(u.OutputAzureBlob, "", true)
	}

	if u.OutputAzureDataExplorer != nil {
		return utils.MarshalJSON(u.OutputAzureDataExplorer, "", true)
	}

	if u.OutputAzureLogs != nil {
		return utils.MarshalJSON(u.OutputAzureLogs, "", true)
	}

	if u.OutputKinesis != nil {
		return utils.MarshalJSON(u.OutputKinesis, "", true)
	}

	if u.OutputHoneycomb != nil {
		return utils.MarshalJSON(u.OutputHoneycomb, "", true)
	}

	if u.OutputAzureEventhub != nil {
		return utils.MarshalJSON(u.OutputAzureEventhub, "", true)
	}

	if u.OutputGoogleChronicle != nil {
		return utils.MarshalJSON(u.OutputGoogleChronicle, "", true)
	}

	if u.OutputGoogleCloudStorage != nil {
		return utils.MarshalJSON(u.OutputGoogleCloudStorage, "", true)
	}

	if u.OutputGoogleCloudLogging != nil {
		return utils.MarshalJSON(u.OutputGoogleCloudLogging, "", true)
	}

	if u.OutputGooglePubsub != nil {
		return utils.MarshalJSON(u.OutputGooglePubsub, "", true)
	}

	if u.OutputExabeam != nil {
		return utils.MarshalJSON(u.OutputExabeam, "", true)
	}

	if u.OutputKafka != nil {
		return utils.MarshalJSON(u.OutputKafka, "", true)
	}

	if u.OutputConfluentCloud != nil {
		return utils.MarshalJSON(u.OutputConfluentCloud, "", true)
	}

	if u.OutputMsk != nil {
		return utils.MarshalJSON(u.OutputMsk, "", true)
	}

	if u.OutputElastic != nil {
		return utils.MarshalJSON(u.OutputElastic, "", true)
	}

	if u.OutputElasticCloud != nil {
		return utils.MarshalJSON(u.OutputElasticCloud, "", true)
	}

	if u.OutputNewrelic != nil {
		return utils.MarshalJSON(u.OutputNewrelic, "", true)
	}

	if u.OutputNewrelicEvents != nil {
		return utils.MarshalJSON(u.OutputNewrelicEvents, "", true)
	}

	if u.OutputInfluxdb != nil {
		return utils.MarshalJSON(u.OutputInfluxdb, "", true)
	}

	if u.OutputCloudwatch != nil {
		return utils.MarshalJSON(u.OutputCloudwatch, "", true)
	}

	if u.OutputMinio != nil {
		return utils.MarshalJSON(u.OutputMinio, "", true)
	}

	if u.OutputStatsd != nil {
		return utils.MarshalJSON(u.OutputStatsd, "", true)
	}

	if u.OutputStatsdExt != nil {
		return utils.MarshalJSON(u.OutputStatsdExt, "", true)
	}

	if u.OutputGraphite != nil {
		return utils.MarshalJSON(u.OutputGraphite, "", true)
	}

	if u.OutputRouter != nil {
		return utils.MarshalJSON(u.OutputRouter, "", true)
	}

	if u.OutputSns != nil {
		return utils.MarshalJSON(u.OutputSns, "", true)
	}

	if u.OutputSqs != nil {
		return utils.MarshalJSON(u.OutputSqs, "", true)
	}

	if u.OutputSnmp != nil {
		return utils.MarshalJSON(u.OutputSnmp, "", true)
	}

	if u.OutputSumoLogic != nil {
		return utils.MarshalJSON(u.OutputSumoLogic, "", true)
	}

	if u.OutputDatadog != nil {
		return utils.MarshalJSON(u.OutputDatadog, "", true)
	}

	if u.OutputGrafanaCloud != nil {
		return utils.MarshalJSON(u.OutputGrafanaCloud, "", true)
	}

	if u.OutputLoki != nil {
		return utils.MarshalJSON(u.OutputLoki, "", true)
	}

	if u.OutputPrometheus != nil {
		return utils.MarshalJSON(u.OutputPrometheus, "", true)
	}

	if u.OutputRing != nil {
		return utils.MarshalJSON(u.OutputRing, "", true)
	}

	if u.OutputOpenTelemetry != nil {
		return utils.MarshalJSON(u.OutputOpenTelemetry, "", true)
	}

	if u.OutputServiceNow != nil {
		return utils.MarshalJSON(u.OutputServiceNow, "", true)
	}

	if u.OutputDataset != nil {
		return utils.MarshalJSON(u.OutputDataset, "", true)
	}

	if u.OutputCriblTCP != nil {
		return utils.MarshalJSON(u.OutputCriblTCP, "", true)
	}

	if u.OutputCriblHTTP != nil {
		return utils.MarshalJSON(u.OutputCriblHTTP, "", true)
	}

	if u.OutputHumioHec != nil {
		return utils.MarshalJSON(u.OutputHumioHec, "", true)
	}

	if u.OutputCrowdstrikeNextGenSiem != nil {
		return utils.MarshalJSON(u.OutputCrowdstrikeNextGenSiem, "", true)
	}

	if u.OutputDlS3 != nil {
		return utils.MarshalJSON(u.OutputDlS3, "", true)
	}

	if u.OutputSecurityLake != nil {
		return utils.MarshalJSON(u.OutputSecurityLake, "", true)
	}

	if u.OutputCriblLake != nil {
		return utils.MarshalJSON(u.OutputCriblLake, "", true)
	}

	if u.OutputDiskSpool != nil {
		return utils.MarshalJSON(u.OutputDiskSpool, "", true)
	}

	if u.OutputClickHouse != nil {
		return utils.MarshalJSON(u.OutputClickHouse, "", true)
	}

	if u.OutputXsiam != nil {
		return utils.MarshalJSON(u.OutputXsiam, "", true)
	}

	if u.OutputNetflow != nil {
		return utils.MarshalJSON(u.OutputNetflow, "", true)
	}

	if u.OutputDynatraceHTTP != nil {
		return utils.MarshalJSON(u.OutputDynatraceHTTP, "", true)
	}

	if u.OutputDynatraceOtlp != nil {
		return utils.MarshalJSON(u.OutputDynatraceOtlp, "", true)
	}

	return nil, errors.New("could not marshal union type Output: all fields are null")
}
